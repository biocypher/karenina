# Manual Interface

The **manual interface** lets you provide pre-recorded LLM responses to the verification pipeline instead of generating answers in real time. The pipeline evaluates these traces exactly as it would live responses — parsing, template verification, and rubric evaluation all work the same way.

## Why Use Pre-Recorded Traces?

| Scenario | Benefit |
|----------|---------|
| **Reproducibility** | Re-evaluate identical answers under different configurations without re-running models |
| **Cost reduction** | Skip answer generation (the most expensive step) when iterating on templates or rubrics |
| **External outputs** | Evaluate responses generated by systems outside Karenina |
| **Controlled testing** | Test templates and rubrics with known answers before running full benchmarks |
| **Model comparison** | Compare how the same traces score under different parsing models |

## How It Works

When you use the manual interface, the pipeline bypasses the answer generation stage and uses your pre-recorded trace directly:

```
Normal flow:     Question ──► Answering LLM ──► Trace ──► Parsing ──► Verify
Manual flow:     Question ──► Pre-recorded Trace ────────► Parsing ──► Verify
```

The trace is looked up by **question hash** — an MD5 hash of the question text that uniquely identifies each question. The pipeline computes this hash automatically during verification, so you just need to match traces to questions.

## Trace Format

Traces can be provided in three formats:

| Format | When to Use |
|--------|-------------|
| **String** | Simple text answers with no tool call history |
| **Port Message list** | Native architecture format (`karenina.ports.messages.Message`) |
| **LangChain message list** | Backward-compatible format (`AIMessage`, `ToolMessage`, etc.) |

Message lists are automatically converted to string traces. During conversion, agent metrics (tool call counts, failures, iterations) are extracted and preserved in the verification result.

### String Traces

The simplest format — a plain text string representing the model's response:

```python
"The answer is 4. I computed this by adding 2 and 2."
```

### JSON File Format

When loading traces from a file, the expected format is a dictionary mapping question MD5 hashes to trace strings:

```json
{
    "936dbc8755f623c951d96ea2b03e13bc": "The answer is 4.",
    "8f2e2b1e4d5c6a7b8c9d0e1f2a3b4c5d": "The answer is 6."
}
```

The hash for any question can be computed as:

```python
import hashlib
question_hash = hashlib.md5("What is 2+2?".encode("utf-8")).hexdigest()
```

## Loading Traces

Use the `ManualTraces` class to register traces for a benchmark:

```python
from karenina.benchmark import Benchmark
from karenina.adapters.manual import ManualTraces
from karenina.schemas import ModelConfig, VerificationConfig

# Load benchmark
benchmark = Benchmark.load("checkpoint.jsonld")

# Create ManualTraces linked to the benchmark
manual_traces = ManualTraces(benchmark)

# Register by question text (map_to_id=True converts text to hash)
manual_traces.register_trace(
    "What is 2+2?",
    "The answer is 4.",
    map_to_id=True
)

# Or register by MD5 hash directly
manual_traces.register_trace(
    "936dbc8755f623c951d96ea2b03e13bc",
    "The answer is 4."
)

# Or batch register
manual_traces.register_traces({
    "What is 2+2?": "The answer is 4.",
    "What is 3+3?": "The answer is 6.",
}, map_to_id=True)
```

## Configuration

Create a `ModelConfig` with `interface="manual"` and pass the traces:

```python
manual_config = ModelConfig(
    interface="manual",
    manual_traces=manual_traces
)

judge_config = ModelConfig(
    id="gpt-4.1-mini",
    model_provider="openai",
    model_name="gpt-4.1-mini",
    interface="langchain"
)

config = VerificationConfig(
    answering_models=[manual_config],
    parsing_models=[judge_config]
)
```

Key configuration points:

- `id` and `model_name` are auto-set to `"manual"` for the manual interface
- `manual_traces` is excluded from serialization (not saved in presets)
- MCP tools cannot be used with the manual interface
- A separate **parsing model** is still required to parse traces into template schemas

## Limitations

- **No MCP support**: The manual interface does not support MCP tool servers
- **No live LLM calls**: The manual adapter raises errors if the pipeline attempts direct LLM invocations (parsing is handled by the separate parsing model)
- **Preset re-population**: When loading a preset with a manual config, traces must be re-registered since they are excluded from serialization
- **Exact text matching**: When using `map_to_id=True`, question text must match the benchmark exactly (case-sensitive, including whitespace)

## Next Steps

- [Manual Interface Workflow](../06-running-verification/manual-interface.md) — Step-by-step workflow for running verification with manual traces
- [Adapters Overview](adapters.md) — How the manual interface fits into the adapter system
- [CLI Verification](../06-running-verification/cli.md#manual-traces) — Using `--interface manual` from the command line
- [Running Verification](../06-running-verification/index.md) — Overview of all verification methods
