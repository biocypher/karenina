{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mock-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:18.146557Z",
     "iopub.status.busy": "2026-01-04T09:33:18.146470Z",
     "iopub.status.idle": "2026-01-04T09:33:20.297511Z",
     "shell.execute_reply": "2026-01-04T09:33:20.297102Z"
    },
    "jupyter": {
     "source_hidden": false
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mock setup complete\n",
      "✓ Temp directory: /var/folders/34/129m5tdd04vf10ptyj12w6f80000gp/T/karenina_docs_opgi7rlv\n",
      "✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\n"
     ]
    }
   ],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"BCL2\"):\n",
    "        self.content = content\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output for template parsing.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        self.subunits = kwargs.get(\"subunits\", 4)\n",
    "        self.diseases = kwargs.get(\"diseases\", [\"asthma\", \"bronchitis\", \"pneumonia\"])\n",
    "        self.mentions_bcl2_protein = kwargs.get(\"mentions_bcl2_protein\", True)\n",
    "        self.mentions_apoptosis_regulation = kwargs.get(\"mentions_apoptosis_regulation\", False)\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model(default_response: str = \"BCL2\"):\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(default_response)\n",
    "    mock.ainvoke.return_value = MockLLMResponse(default_response)\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "def create_mock_benchmark():\n",
    "    \"\"\"Create a mock Benchmark object for demonstrations.\"\"\"\n",
    "    from karenina import Benchmark\n",
    "\n",
    "    benchmark = Benchmark.create(\n",
    "        name=\"Demo Benchmark\", description=\"Mock benchmark for documentation\", version=\"1.0.0\", creator=\"Documentation\"\n",
    "    )\n",
    "    return benchmark\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "\n",
    "# Helper to replace file paths in examples\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Get a temporary file path for documentation examples.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup on kernel shutdown\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Answer Templates\n",
    "\n",
    "Templates define how to evaluate LLM responses programmatically. This guide covers what templates are, why they're useful, and how to create them automatically or manually.\n",
    "\n",
    "**Quick Navigation:**\n",
    "\n",
    "- [What Are Templates?](#what-are-templates) - Core concepts and structure\n",
    "- [Why Use Templates?](#why-use-templates) - Benefits and use cases\n",
    "- [Automatic Template Generation](#automatic-template-generation-recommended) - Recommended LLM-based approach\n",
    "- [Manual Template Creation](#manual-template-creation-advanced) - Advanced custom templates\n",
    "- [When to Use Which Approach](#when-to-use-which-approach) - Decision guide for automatic vs manual\n",
    "- [Complete Example](#complete-example) - End-to-end workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-are-templates",
   "metadata": {},
   "source": [
    "## What Are Templates?\n",
    "\n",
    "**Answer templates** are Pydantic classes that specify:\n",
    "\n",
    "- **What information to extract** from free-text LLM responses\n",
    "- **How to verify correctness** by comparing extracted data against expected answers\n",
    "- **The structure of expected answers** (e.g., a drug name, a number, a list of items)\n",
    "\n",
    "Templates enable **LLM-as-a-judge evaluation**: The answering model generates free text, and the judge model extracts structured data from that text using the template schema. The template then programmatically verifies correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-use-templates",
   "metadata": {},
   "source": [
    "## Why Use Templates?\n",
    "\n",
    "Templates provide several key benefits:\n",
    "\n",
    "1. **Flexible Input**: Answering models can respond naturally without strict formatting constraints\n",
    "2. **Structured Evaluation**: Judge models extract specific fields, making evaluation deterministic\n",
    "3. **Programmatic Verification**: The `verify()` method implements custom logic for checking correctness\n",
    "4. **Reusable Patterns**: Templates can be generated automatically for common question types\n",
    "5. **Transparent Logic**: Evaluation criteria are explicit and inspectable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-generation-header",
   "metadata": {},
   "source": [
    "## Automatic Template Generation (Recommended)\n",
    "\n",
    "**The recommended approach** is to let Karenina automatically generate templates using an LLM. This is fast, consistent, and works well for most question types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-generation-header",
   "metadata": {},
   "source": [
    "### Basic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "basic-generation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.299120Z",
     "iopub.status.busy": "2026-01-04T09:33:20.298965Z",
     "iopub.status.idle": "2026-01-04T09:33:20.313186Z",
     "shell.execute_reply": "2026-01-04T09:33:20.312820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating templates...\n",
      "✓ Generated 3 templates successfully\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import ModelConfig\n",
    "\n",
    "# Create benchmark and add questions\n",
    "benchmark = Benchmark.create(name=\"Genomics Knowledge Benchmark\")\n",
    "\n",
    "benchmark.add_question(question=\"How many chromosomes are in a human somatic cell?\", raw_answer=\"46\")\n",
    "\n",
    "benchmark.add_question(question=\"What is the approved drug target of Venetoclax?\", raw_answer=\"BCL2\")\n",
    "\n",
    "benchmark.add_question(question=\"How many protein subunits does hemoglobin A have?\", raw_answer=\"4\")\n",
    "\n",
    "# Generate templates for all questions\n",
    "# Note: Using individual parameters instead of model_config\n",
    "print(\"Generating templates...\")\n",
    "results = benchmark.generate_all_templates(\n",
    "    model=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.1, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated {len(results)} templates successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-happens",
   "metadata": {},
   "source": [
    "**What happens:**\n",
    "\n",
    "1. Karenina sends each question + answer to the LLM\n",
    "2. The LLM generates a Pydantic class tailored to that specific question\n",
    "3. The template is automatically validated and associated with the question\n",
    "4. Questions are marked as \"finished\" and ready for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generated-template-example-header",
   "metadata": {},
   "source": [
    "### Generated Template Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generated-template-example",
   "metadata": {},
   "source": [
    "For the question \"What is the approved drug target of Venetoclax?\" with answer \"BCL2\", the LLM might generate:\n",
    "\n",
    "```python\n",
    "class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCL2\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.target.strip().upper() == self.correct[\"target\"].upper()\n",
    "```\n",
    "\n",
    "This template:\n",
    "\n",
    "- Extracts the `target` field from free-text responses\n",
    "- Compares it case-insensitively against \"BCL2\"\n",
    "- Returns `True` if they match, `False` otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-automatic-works-header",
   "metadata": {},
   "source": [
    "### How Automatic Template Generation Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-automatic-works",
   "metadata": {},
   "source": [
    "Understanding the template generation process helps you troubleshoot issues and make informed decisions about when to use automatic vs manual templates.\n",
    "\n",
    "**High-Level Process:**\n",
    "\n",
    "When you call `generate_all_templates()`, Karenina performs a **three-phase structured generation** for each question:\n",
    "\n",
    "**Phase 1: Ground Truth Extraction**\n",
    "\n",
    "The LLM analyzes the question-answer pair and generates a JSON specification defining the attributes needed for verification.\n",
    "\n",
    "Example for \"What is the approved drug target of Venetoclax?\" (answer: \"BCL2\"):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"attributes\": [\n",
    "    {\n",
    "      \"name\": \"mentions_bcl2_protein\",\n",
    "      \"type\": \"bool\",\n",
    "      \"ground_truth\": true\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"mentions_apoptosis_regulation\",\n",
    "      \"type\": \"bool\",\n",
    "      \"ground_truth\": false\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Important design principle:** The system strongly **suggests boolean-based evaluation** rather than free-text string matching. Text-based assessment is typically converted to boolean checks for concept presence.\n",
    "\n",
    "**Phase 2: Field Description Generation**\n",
    "\n",
    "Using the ground truth specification, the LLM generates clear instructions for each attribute that will guide the judge model during response parsing.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"field_descriptions\": {\n",
    "    \"mentions_bcl2_protein\": \"Answer with true if the response mentions BCL2 or semantically related terms; otherwise answer false.\",\n",
    "    \"mentions_apoptosis_regulation\": \"Answer with true if the response discusses apoptosis regulation mechanisms; otherwise answer false.\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "These descriptions emphasize **semantic equivalence** over exact string matching.\n",
    "\n",
    "**Phase 3: Code Generation**\n",
    "\n",
    "Karenina programmatically builds the Pydantic class using the structured outputs from Phases 1 and 2. The generated code includes:\n",
    "\n",
    "- Field definitions with judge instructions from Phase 2\n",
    "- The `model_post_init()` method with ground truth values from Phase 1\n",
    "- The `verify()` method with type-appropriate comparison logic\n",
    "- The `verify_granular()` method for partial credit (multi-attribute templates only)\n",
    "\n",
    "**Validation and Storage**\n",
    "\n",
    "After generation, Karenina validates the Python code and stores it with the question. If validation fails, the system retries with error context (up to 3 attempts total)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-makes-effective",
   "metadata": {},
   "source": [
    "**What Makes This Approach Effective:**\n",
    "\n",
    "- **Structured Outputs**: JSON schema validation ensures consistent, parseable results from the LLM\n",
    "- **Semantic Evaluation**: Boolean attributes capture concept presence, making verification robust to paraphrasing\n",
    "- **Type Safety**: Enforced constraints prevent ambiguous evaluation strategies\n",
    "- **Retry Logic**: Failed validations trigger automatic regeneration with error context\n",
    "- **Partial Credit**: Multi-attribute templates support granular scoring automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-boolean",
   "metadata": {},
   "source": [
    "**Why Boolean Attributes?**\n",
    "\n",
    "The system strongly prefers boolean attributes over string extraction because:\n",
    "\n",
    "- **Flexibility**: Judges check if concepts are present, not exact phrases\n",
    "- **Deterministic**: `true`/`false` comparisons are unambiguous\n",
    "- **Robust**: Handles paraphrasing, synonyms, and variations naturally\n",
    "- **Avoids pitfalls**: No need for case normalization, fuzzy matching, or string similarity thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trade-off",
   "metadata": {},
   "source": [
    "**Trade-off: Speed vs. Rigor**\n",
    "\n",
    "The current approach **may expose ground truth to the judge model** through field descriptions. For example, asking \"Answer with true if the response mentions BCL2\" reveals that BCL2 is the expected answer. The judge becomes aware of what's \"correct\" rather than acting as a pure information extractor.\n",
    "\n",
    "**Alternative approach (more rigorous but requires manual curation):**\n",
    "\n",
    "- Have the judge extract information **without knowing the correct answer**\n",
    "- Field descriptions would be neutral (e.g., \"Extract the protein target mentioned\")\n",
    "- All verification logic stays in the `verify()` method\n",
    "- Judge models act as pure parsers, not evaluators\n",
    "\n",
    "**Current approach (faster but less rigorous):**\n",
    "\n",
    "- Field descriptions include hints about correctness\n",
    "- Allows automated template generation with minimal manual curation\n",
    "- Judge models perform some evaluation during extraction\n",
    "- Faster to deploy at scale\n",
    "\n",
    "This is a **design trade-off**: a more rigorous benchmark requires more manual template curation, while the current automated approach prioritizes speed and scalability at the cost of some methodological purity.\n",
    "\n",
    "If you need the more rigorous approach, see [Manual Template Creation](#manual-template-creation-advanced) for how to write templates with neutral field descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-generation-fails",
   "metadata": {},
   "source": [
    "**When Generation Might Fail:**\n",
    "\n",
    "Template generation works well for most questions, but may struggle with:\n",
    "\n",
    "- **Highly ambiguous questions** where even the ground truth is unclear\n",
    "- **Complex compositional logic** requiring interdependent attribute checks\n",
    "- **Domain-specific tolerance requirements** (e.g., \"within 10% is acceptable\")\n",
    "- **Unusual answer formats** that don't fit the structured attribute model\n",
    "\n",
    "In these cases, you can fall back to [manual template creation](#manual-template-creation-advanced)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-creation-header",
   "metadata": {},
   "source": [
    "## Manual Template Creation (Advanced)\n",
    "\n",
    "For full control over evaluation logic, you can write templates manually. This is useful for complex verification requirements or custom validation rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-structure-header",
   "metadata": {},
   "source": [
    "### Basic Template Structure\n",
    "\n",
    "Templates inherit from `BaseAnswer` and must include these **three required components**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "field-definitions",
   "metadata": {},
   "source": [
    "**1. Field Definitions**\n",
    "\n",
    "Fields specify what data to extract. Each field should have a clear description that guides the judge LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "field-definitions-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.314937Z",
     "iopub.status.busy": "2026-01-04T09:33:20.314843Z",
     "iopub.status.idle": "2026-01-04T09:33:20.317038Z",
     "shell.execute_reply": "2026-01-04T09:33:20.316645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field type examples shown above\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "# String fields\n",
    "target: str = Field(description=\"The protein target mentioned in the response\")\n",
    "\n",
    "# Integer/Float fields\n",
    "count: int = Field(description=\"The number of items mentioned\")\n",
    "score: float = Field(description=\"Accuracy score 0.0-1.0\", ge=0.0, le=1.0)\n",
    "\n",
    "# Boolean fields (recommended for rigorous evaluation)\n",
    "mentions_bcl2: bool = Field(description=\"Extract whether BCL2 protein is mentioned\")\n",
    "\n",
    "# List fields\n",
    "proteins: list[str] = Field(description=\"List of proteins mentioned\")\n",
    "\n",
    "print(\"Field type examples shown above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-post-init",
   "metadata": {},
   "source": [
    "**2. `model_post_init(self, __context)` Method** (required)\n",
    "\n",
    "- **Purpose**: Initialize the ground truth values after Pydantic constructs the model\n",
    "- **Returns**: `None` (no return value)\n",
    "- **Usage**: Set `self.correct` dictionary with expected values\n",
    "\n",
    "```python\n",
    "def model_post_init(self, __context):\n",
    "    self.correct = {\"count\": 46}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-method",
   "metadata": {},
   "source": [
    "**3. `verify(self) -> bool` Method** (required)\n",
    "\n",
    "- **Purpose**: Determine if the extracted answer is correct\n",
    "- **Returns**: `bool` - `True` if correct, `False` if incorrect\n",
    "- **Usage**: Compare extracted field values against `self.correct`\n",
    "\n",
    "```python\n",
    "def verify(self) -> bool:\n",
    "    return self.count == self.correct[\"count\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-template-example",
   "metadata": {},
   "source": [
    "**Complete Example:**\n",
    "\n",
    "```python\n",
    "class Answer(BaseAnswer):\n",
    "    count: int = Field(description=\"The number of chromosomes mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"count\": 46}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.count == self.correct[\"count\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "granular-scoring-header",
   "metadata": {},
   "source": [
    "### Optional Method: Granular Scoring\n",
    "\n",
    "**`verify_granular(self) -> float`** (optional)\n",
    "\n",
    "- **Purpose**: Calculate partial credit for multi-attribute templates\n",
    "- **Returns**: `float` between 0.0 and 1.0 representing the fraction of correct attributes\n",
    "- **Usage**: Count matching attributes and return the percentage\n",
    "- **Note**: Automatically generated for multi-attribute templates; rarely needed for manual templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "granular-scoring-example",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.318175Z",
     "iopub.status.busy": "2026-01-04T09:33:20.318101Z",
     "iopub.status.idle": "2026-01-04T09:33:20.320084Z",
     "shell.execute_reply": "2026-01-04T09:33:20.319620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify_granular example shown above\n"
     ]
    }
   ],
   "source": [
    "# Example verify_granular implementation\n",
    "def verify_granular(self) -> float:\n",
    "    correct_count = 0\n",
    "    total_count = 2\n",
    "\n",
    "    if self.field1 == self.correct[\"field1\"]:\n",
    "        correct_count += 1\n",
    "    if self.field2 == self.correct[\"field2\"]:\n",
    "        correct_count += 1\n",
    "\n",
    "    return correct_count / total_count\n",
    "\n",
    "\n",
    "print(\"verify_granular example shown above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adding-manual-templates-header",
   "metadata": {},
   "source": [
    "### Adding Manual Templates to Questions\n",
    "\n",
    "You can add templates in three ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "option1-header",
   "metadata": {},
   "source": [
    "\"**Option 1: Pass template as a string (recommended for notebooks)**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "option1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.321084Z",
     "iopub.status.busy": "2026-01-04T09:33:20.321020Z",
     "iopub.status.idle": "2026-01-04T09:33:20.322875Z",
     "shell.execute_reply": "2026-01-04T09:33:20.322518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question added with manual template (string-based for notebook compatibility)\n"
     ]
    }
   ],
   "source": [
    "# Note: In notebooks, use string-based templates instead of class definitions\n",
    "# because inspect.getsource() cannot extract source from notebook cells\n",
    "# For file-based code, you can pass Answer classes directly\n",
    "\n",
    "template_code = \"\"\"class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target mentioned\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCL2\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.target.strip().upper() == self.correct[\"target\"].upper()\n",
    "\"\"\"\n",
    "\n",
    "benchmark2 = Benchmark.create(name=\"Manual Template Example\")\n",
    "benchmark2.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\",\n",
    "    raw_answer=\"BCL2\",\n",
    "    answer_template=template_code,\n",
    "    finished=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Question added with manual template (string-based for notebook compatibility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-automatic-extraction",
   "metadata": {},
   "source": [
    "\"**Note about notebooks:**\\n\\nIn Jupyter notebooks, classes defined in cells cannot have their source code automatically extracted by `inspect.getsource()`. For notebook development, use string-based templates (as shown in Option 1 above). For production code in `.py` files, you can pass Answer classes directly.\\n\\n**For file-based code:**\\n\\n- Classes defined in `.py` files: `inspect.getsource()` captures the source code automatically\\n- For exec-created classes: Set `YourClassName._source_code` manually\\n\\n**For notebooks:**\\n\\n- Always use string-based templates with `answer_template=template_code`\\n- The system will validate and store the template code directly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "option2-header",
   "metadata": {},
   "source": [
    "**Option 2: Pass template code as a string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "option2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.323913Z",
     "iopub.status.busy": "2026-01-04T09:33:20.323848Z",
     "iopub.status.idle": "2026-01-04T09:33:20.325610Z",
     "shell.execute_reply": "2026-01-04T09:33:20.325248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question added with string template\n"
     ]
    }
   ],
   "source": [
    "template_code = \"\"\"class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target mentioned\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCL2\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.target.strip().upper() == self.correct[\"target\"].upper()\n",
    "\"\"\"\n",
    "\n",
    "benchmark3 = Benchmark.create(name=\"String Template Example\")\n",
    "benchmark3.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\",\n",
    "    raw_answer=\"BCL2\",\n",
    "    answer_template=template_code,\n",
    "    finished=True,  # Mark as ready for verification\n",
    ")\n",
    "\n",
    "print(\"✓ Question added with string template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "option3-header",
   "metadata": {},
   "source": [
    "**Option 3: Add template to existing question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "option3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.326601Z",
     "iopub.status.busy": "2026-01-04T09:33:20.326529Z",
     "iopub.status.idle": "2026-01-04T09:33:20.328992Z",
     "shell.execute_reply": "2026-01-04T09:33:20.328595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Template added to existing question\n"
     ]
    }
   ],
   "source": [
    "benchmark4 = Benchmark.create(name=\"Add Template Later Example\")\n",
    "question_id = benchmark4.add_question(question=\"How many protein subunits does hemoglobin A have?\", raw_answer=\"4\")\n",
    "\n",
    "# Later, add the template using add_answer_template\n",
    "template_code = \"\"\"class Answer(BaseAnswer):\n",
    "    count: int = Field(description=\"The number of subunits mentioned\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"count\": 4}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.count == self.correct[\"count\"]\n",
    "\"\"\"\n",
    "\n",
    "benchmark4.add_answer_template(question_id, template_code)\n",
    "\n",
    "print(\"✓ Template added to existing question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-template-header",
   "metadata": {},
   "source": [
    "### Complex Template Example\n",
    "\n",
    "For more sophisticated evaluation, you can include multiple fields and custom logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complex-template",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.329873Z",
     "iopub.status.busy": "2026-01-04T09:33:20.329809Z",
     "iopub.status.idle": "2026-01-04T09:33:20.331645Z",
     "shell.execute_reply": "2026-01-04T09:33:20.331285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complex template example created\n"
     ]
    }
   ],
   "source": [
    "template_code = \"\"\"class Answer(BaseAnswer):\n",
    "    diseases: list[str] = Field(description=\"List of diseases mentioned in the response\")\n",
    "    inflammatory_count: int = Field(description=\"Number of inflammatory diseases identified\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\n",
    "            \"inflammatory_diseases\": [\"asthma\", \"bronchitis\", \"pneumonia\"],\n",
    "            \"non_inflammatory\": [\"emphysema\", \"pulmonary fibrosis\"]\n",
    "        }\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        # Check if the correct inflammatory diseases are identified\n",
    "        identified = [d.lower().strip() for d in self.diseases]\n",
    "        correct_identified = sum(1 for d in self.correct[\"inflammatory_diseases\"]\n",
    "                                if d in identified)\n",
    "\n",
    "        # At least 2 out of 3 correct inflammatory diseases\n",
    "        return correct_identified >= 2\n",
    "\"\"\"\n",
    "\n",
    "benchmark5 = Benchmark.create(name=\"Complex Template Example\")\n",
    "benchmark5.add_question(\n",
    "    question=\"Which of the following are inflammatory lung diseases: asthma, bronchitis, pneumonia, emphysema, pulmonary fibrosis?\",\n",
    "    raw_answer=\"asthma, bronchitis, pneumonia\",\n",
    "    answer_template=template_code,\n",
    "    finished=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Complex template example created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-use-header",
   "metadata": {},
   "source": [
    "## When to Use Which Approach\n",
    "\n",
    "### Use Automatic Generation When:\n",
    "\n",
    "- You have many questions to process\n",
    "- Questions follow standard patterns (factual recall, numerical answers, multiple choice)\n",
    "- You're prototyping or testing quickly\n",
    "\n",
    "### Use Manual Creation When:\n",
    "\n",
    "- You need very specific verification logic\n",
    "- You want to implement tolerance ranges or fuzzy matching\n",
    "- You're creating reusable template libraries\n",
    "- Automatic generation doesn't produce the desired structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-example-header",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "\n",
    "Here's a complete workflow showing automatic template generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complete-example",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.332652Z",
     "iopub.status.busy": "2026-01-04T09:33:20.332565Z",
     "iopub.status.idle": "2026-01-04T09:33:20.347938Z",
     "shell.execute_reply": "2026-01-04T09:33:20.347555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added 3 questions to benchmark\n",
      "Generating templates...\n",
      "✓ Generated 3 templates\n",
      "Configuration ready for verification\n",
      "  Answering models: ['gpt-4.1-mini']\n",
      "  Parsing models: ['gpt-4.1-mini']\n",
      "✓ Benchmark saved to genomics_benchmark.jsonld\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import VerificationConfig\n",
    "\n",
    "# 1. Create benchmark and add questions\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\", description=\"Testing LLM knowledge of genomics\", version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add questions\n",
    "questions = [\n",
    "    (\"How many chromosomes are in a human somatic cell?\", \"46\"),\n",
    "    (\"What is the approved drug target of Venetoclax?\", \"BCL2\"),\n",
    "    (\"How many protein subunits does hemoglobin A have?\", \"4\"),\n",
    "]\n",
    "\n",
    "for q, a in questions:\n",
    "    benchmark.add_question(question=q, raw_answer=a, author={\"name\": \"Bio Curator\"})\n",
    "\n",
    "print(f\"✓ Added {len(questions)} questions to benchmark\")\n",
    "\n",
    "# 2. Generate templates automatically\n",
    "print(\"Generating templates...\")\n",
    "results = benchmark.generate_all_templates(\n",
    "    model=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.1, interface=\"langchain\"\n",
    ")\n",
    "print(f\"✓ Generated {len(results)} templates\")\n",
    "\n",
    "# 3. Templates are now ready - proceed to verification\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.1,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.1,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Note: In actual usage, this would call real LLM APIs\n",
    "# For documentation purposes, we show the pattern\n",
    "print(\"Configuration ready for verification\")\n",
    "print(f\"  Answering models: {[m.id for m in config.answering_models]}\")\n",
    "print(f\"  Parsing models: {[m.id for m in config.parsing_models]}\")\n",
    "\n",
    "# 4. Save benchmark\n",
    "save_path = temp_path(\"genomics_benchmark.jsonld\")\n",
    "benchmark.save(save_path)\n",
    "print(f\"✓ Benchmark saved to {save_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once you have templates set up for your questions, you can:\n",
    "\n",
    "- [Create rubrics](rubrics.md) for qualitative assessment criteria\n",
    "- [Run verification](verification.md) to evaluate LLM responses\n",
    "- [Analyze results](verification.md#accessing-verification-results) to assess model performance\n",
    "- [Save your benchmark](saving-loading.md) using checkpoints or database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-docs",
   "metadata": {},
   "source": [
    "## Related Documentation\n",
    "\n",
    "- [Adding Questions](adding-questions.md) - Populate your benchmark with questions\n",
    "- [Rubrics](rubrics.md) - Assess qualitative aspects beyond factual correctness\n",
    "- [Verification](verification.md) - Run evaluations with multiple models\n",
    "- [Quick Start](../quickstart.md) - End-to-end workflow example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
