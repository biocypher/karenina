{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full GEPA Optimization Workflow\n",
    "\n",
    "This notebook demonstrates a complete end-to-end GEPA optimization workflow using the AIME 2025 benchmark. We'll cover:\n",
    "\n",
    "1. Loading and preparing the benchmark\n",
    "2. Configuring optimization\n",
    "3. Splitting data for train/val/test\n",
    "4. Running baseline verification\n",
    "5. Setting up the GEPA adapter\n",
    "6. Simulating optimization (with manual prompt iterations)\n",
    "7. Tracking results\n",
    "8. Exporting optimized prompts\n",
    "9. Final evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "# Core Karenina imports\n",
    "from karenina import Benchmark\n",
    "\n",
    "# GEPA integration imports\n",
    "from karenina.integrations.gepa import (\n",
    "    GEPA_AVAILABLE,\n",
    "    OptimizationConfig,\n",
    "    OptimizationRun,\n",
    "    OptimizationTarget,\n",
    "    OptimizationTracker,\n",
    "    compute_improvement,\n",
    "    compute_single_score,\n",
    "    export_prompts_json,\n",
    "    export_to_preset,\n",
    "    split_benchmark,\n",
    ")\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Create temp directory for outputs\n",
    "OUTPUT_DIR = Path(tempfile.mkdtemp(prefix=\"gepa_workflow_\"))\n",
    "\n",
    "print(f\"GEPA available: {GEPA_AVAILABLE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load and Explore Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AIME 2025 benchmark\n",
    "benchmark_path = Path.home() / \"Projects/karenina-monorepo/local_data/data/checkpoints/aime_2025.jsonld\"\n",
    "benchmark = Benchmark.load(benchmark_path)\n",
    "\n",
    "print(f\"Benchmark: {benchmark.name}\")\n",
    "print(f\"Description: {benchmark.description}\")\n",
    "print(f\"Total questions: {len(benchmark.get_question_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a sample question\n",
    "question_ids = benchmark.get_question_ids()\n",
    "sample_q = benchmark.get_question(question_ids[0])\n",
    "\n",
    "print(\"Sample AIME problem:\")\n",
    "print(f\"  Question: {sample_q['question'][:100]}...\")\n",
    "print(f\"  Answer: {sample_q['raw_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Configure Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seed prompt (starting point for optimization)\n",
    "SEED_PROMPT = \"\"\"You are a helpful math assistant. \n",
    "Solve the problem and provide the final answer.\"\"\"\n",
    "\n",
    "# Create optimization config\n",
    "opt_config = OptimizationConfig(\n",
    "    # What to optimize\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    # Seed prompt\n",
    "    seed_answering_prompt=SEED_PROMPT,\n",
    "    # Scoring: correctness only for AIME\n",
    "    template_weight=1.0,\n",
    "    rubric_weight=0.0,\n",
    "    # Data splitting\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.1,\n",
    "    split_seed=42,\n",
    "    # GEPA parameters\n",
    "    reflection_model=\"anthropic/claude-haiku-4-5\",\n",
    "    max_metric_calls=100,\n",
    ")\n",
    "\n",
    "print(\"Optimization Configuration:\")\n",
    "print(f\"  Targets: {[t.value for t in opt_config.targets]}\")\n",
    "print(f\"  Scoring: template={opt_config.template_weight}, rubric={opt_config.rubric_weight}\")\n",
    "print(f\"  Split: train={opt_config.train_ratio}, val={opt_config.val_ratio}, test={opt_config.test_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the benchmark\n",
    "split = split_benchmark(\n",
    "    benchmark,\n",
    "    train_ratio=opt_config.train_ratio,\n",
    "    val_ratio=opt_config.val_ratio,\n",
    "    test_ratio=opt_config.test_ratio,\n",
    "    seed=opt_config.split_seed,\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"  {split.summary()}\")\n",
    "print(f\"\\nTrain set: {len(split.train)} questions (for optimization)\")\n",
    "print(f\"Val set: {len(split.val)} questions (for candidate selection)\")\n",
    "print(f\"Test set: {len(split.test)} questions (held out for final eval)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Create Verification Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_verification_config(system_prompt: str) -> VerificationConfig:\n",
    "    \"\"\"Create a verification config with the given system prompt.\"\"\"\n",
    "    return VerificationConfig(\n",
    "        answering_models=[\n",
    "            ModelConfig(\n",
    "                id=\"claude-haiku\",\n",
    "                model_provider=\"anthropic\",\n",
    "                model_name=\"claude-haiku-4-5\",\n",
    "                temperature=0.0,\n",
    "                interface=\"langchain\",\n",
    "                system_prompt=system_prompt,\n",
    "            )\n",
    "        ],\n",
    "        parsing_models=[\n",
    "            ModelConfig(\n",
    "                id=\"parser\",\n",
    "                model_provider=\"anthropic\",\n",
    "                model_name=\"claude-haiku-4-5\",\n",
    "                temperature=0.0,\n",
    "                interface=\"langchain\",\n",
    "            )\n",
    "        ],\n",
    "        evaluation_mode=\"template_only\",\n",
    "        replicate_count=1,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Verification config factory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Run Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(prompt: str, question_ids: list, description: str = \"\") -> float:\n",
    "    \"\"\"Evaluate a prompt on given questions and return the score.\"\"\"\n",
    "    config = create_verification_config(prompt)\n",
    "    results = benchmark.run_verification(config, question_ids=question_ids)\n",
    "\n",
    "    scores = [compute_single_score(r, template_weight=1.0, rubric_weight=0.0) for r in results.results]\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "    passed = sum(1 for s in scores if s > 0)\n",
    "    print(f\"{description}: {avg_score:.2%} ({passed}/{len(scores)} passed)\")\n",
    "\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline on training set (subset for speed)\n",
    "train_sample_ids = split.train_ids[:5]\n",
    "val_sample_ids = split.val_ids[:3]\n",
    "\n",
    "print(\"Evaluating baseline prompt...\")\n",
    "print(f\"  Train sample: {len(train_sample_ids)} questions\")\n",
    "print(f\"  Val sample: {len(val_sample_ids)} questions\")\n",
    "print()\n",
    "\n",
    "baseline_train = evaluate_prompt(SEED_PROMPT, train_sample_ids, \"Baseline (train)\")\n",
    "baseline_val = evaluate_prompt(SEED_PROMPT, val_sample_ids, \"Baseline (val)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Optimization Loop (Simulated)\n",
    "\n",
    "In a real GEPA run, the reflection LLM would automatically generate improved prompts. Here we simulate the optimization by manually iterating through progressively better prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated prompt evolution (what GEPA would generate)\n",
    "PROMPT_GENERATIONS = [\n",
    "    # Generation 0: Baseline\n",
    "    \"\"\"You are a helpful math assistant. \n",
    "Solve the problem and provide the final answer.\"\"\",\n",
    "    # Generation 1: Add AIME context\n",
    "    \"\"\"You are a math assistant solving AIME problems.\n",
    "AIME answers are integers from 0 to 999.\n",
    "Solve step by step and give the final integer answer.\"\"\",\n",
    "    # Generation 2: Add structure\n",
    "    \"\"\"You are an expert mathematician solving AIME competition problems.\n",
    "\n",
    "Important:\n",
    "- AIME answers are ALWAYS integers from 0 to 999\n",
    "- Show your complete reasoning\n",
    "- State your final answer clearly\n",
    "\n",
    "Solve the problem:\"\"\",\n",
    "    # Generation 3: Add verification step\n",
    "    \"\"\"You are an expert competition mathematician specializing in AIME problems.\n",
    "\n",
    "CRITICAL GUIDELINES:\n",
    "1. AIME answers are ALWAYS integers from 0 to 999\n",
    "2. Show complete step-by-step reasoning\n",
    "3. Verify your answer by checking edge cases\n",
    "4. State your final answer as: \"The answer is [N]\"\n",
    "\n",
    "Solve systematically:\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization loop\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMIZATION LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "generation_results = []\n",
    "\n",
    "for gen, prompt in enumerate(PROMPT_GENERATIONS):\n",
    "    print(f\"\\n--- Generation {gen} ---\")\n",
    "    print(f\"Prompt: {prompt[:80]}...\")\n",
    "\n",
    "    train_score = evaluate_prompt(prompt, train_sample_ids, f\"  Gen {gen} (train)\")\n",
    "    val_score = evaluate_prompt(prompt, val_sample_ids, f\"  Gen {gen} (val)\")\n",
    "\n",
    "    improvement = compute_improvement(baseline_val, val_score)\n",
    "    print(f\"  Improvement vs baseline: {improvement:+.2%}\")\n",
    "\n",
    "    generation_results.append(\n",
    "        {\n",
    "            \"generation\": gen,\n",
    "            \"prompt\": prompt,\n",
    "            \"train_score\": train_score,\n",
    "            \"val_score\": val_score,\n",
    "            \"improvement\": improvement,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best generation\n",
    "best_result = max(generation_results, key=lambda x: x[\"val_score\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest generation: {best_result['generation']}\")\n",
    "print(f\"Best val score: {best_result['val_score']:.2%}\")\n",
    "print(f\"Improvement: {best_result['improvement']:+.2%}\")\n",
    "print(\"\\nBest prompt:\")\n",
    "print(best_result[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out test set\n",
    "print(\"Evaluating on held-out test set...\")\n",
    "print(f\"  Test questions: {len(split.test_ids)}\")\n",
    "print()\n",
    "\n",
    "# Baseline on test\n",
    "baseline_test = evaluate_prompt(SEED_PROMPT, split.test_ids, \"Baseline (test)\")\n",
    "\n",
    "# Best prompt on test\n",
    "best_test = evaluate_prompt(best_result[\"prompt\"], split.test_ids, \"Optimized (test)\")\n",
    "\n",
    "test_improvement = compute_improvement(baseline_test, best_test)\n",
    "print(f\"\\nTest improvement: {test_improvement:+.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Track Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tracker and log the run\n",
    "tracker = OptimizationTracker(OUTPUT_DIR / \"optimization_history.db\")\n",
    "\n",
    "run = OptimizationRun(\n",
    "    benchmark_name=benchmark.name,\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT.value],\n",
    "    seed_prompts={\"answering_system_prompt\": SEED_PROMPT},\n",
    "    optimized_prompts={\"answering_system_prompt\": best_result[\"prompt\"]},\n",
    "    train_score=best_result[\"train_score\"],\n",
    "    val_score=best_result[\"val_score\"],\n",
    "    test_score=best_test,\n",
    "    improvement=best_result[\"improvement\"],\n",
    "    reflection_model=opt_config.reflection_model,\n",
    "    metric_calls=len(PROMPT_GENERATIONS) * (len(train_sample_ids) + len(val_sample_ids)),\n",
    "    best_generation=best_result[\"generation\"],\n",
    "    total_generations=len(PROMPT_GENERATIONS),\n",
    ")\n",
    "\n",
    "run_id = tracker.log_run(run)\n",
    "\n",
    "print(f\"Logged optimization run: {run_id}\")\n",
    "print(f\"  Train: {run.train_score:.2%}\")\n",
    "print(f\"  Val: {run.val_score:.2%}\")\n",
    "print(f\"  Test: {run.test_score:.2%}\")\n",
    "print(f\"  Improvement: {run.improvement:+.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as verification preset\n",
    "optimized_prompts = {\"answering_system_prompt\": best_result[\"prompt\"]}\n",
    "base_config = create_verification_config(SEED_PROMPT)\n",
    "\n",
    "preset_path = export_to_preset(\n",
    "    optimized_prompts=optimized_prompts,\n",
    "    base_config=base_config,\n",
    "    output_path=OUTPUT_DIR / \"aime_optimized_preset.json\",\n",
    ")\n",
    "\n",
    "print(f\"Exported preset: {preset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as lightweight prompts file\n",
    "prompts_path = export_prompts_json(\n",
    "    optimized_prompts=optimized_prompts,\n",
    "    metadata={\n",
    "        \"benchmark\": benchmark.name,\n",
    "        \"train_score\": run.train_score,\n",
    "        \"val_score\": run.val_score,\n",
    "        \"test_score\": run.test_score,\n",
    "        \"improvement\": run.improvement,\n",
    "        \"best_generation\": run.best_generation,\n",
    "        \"total_generations\": run.total_generations,\n",
    "    },\n",
    "    output_path=OUTPUT_DIR / \"optimized_prompts.json\",\n",
    ")\n",
    "\n",
    "print(f\"Exported prompts: {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Benchmark: {benchmark.name}\n",
    "Questions: {len(benchmark.get_question_ids())}\n",
    "\n",
    "Data Split:\n",
    "  Train: {len(split.train)} questions\n",
    "  Val: {len(split.val)} questions  \n",
    "  Test: {len(split.test)} questions\n",
    "\n",
    "Optimization:\n",
    "  Generations: {len(PROMPT_GENERATIONS)}\n",
    "  Best generation: {best_result[\"generation\"]}\n",
    "  \n",
    "Results:\n",
    "  Baseline (val): {baseline_val:.2%}\n",
    "  Optimized (val): {best_result[\"val_score\"]:.2%}\n",
    "  Improvement: {best_result[\"improvement\"]:+.2%}\n",
    "  \n",
    "Test Set (held out):\n",
    "  Baseline: {baseline_test:.2%}\n",
    "  Optimized: {best_test:.2%}\n",
    "  Improvement: {test_improvement:+.2%}\n",
    "\n",
    "Outputs:\n",
    "  Preset: {preset_path.name}\n",
    "  Prompts: {prompts_path.name}\n",
    "  History: optimization_history.db\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print(\"Output files:\")\n",
    "for f in OUTPUT_DIR.iterdir():\n",
    "    print(f\"  {f.name}: {f.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Production usage**: Use the exported preset with `karenina verify --preset`\n",
    "2. **Iterate further**: Run more optimization with different targets\n",
    "3. **Multi-model**: Add more answering models for Pareto optimization\n",
    "4. **Scale up**: Use the full benchmark (all 30 questions)\n",
    "\n",
    "### Using the Preset\n",
    "\n",
    "```bash\n",
    "karenina verify aime_2025.jsonld --preset aime_optimized_preset.json --output results.json\n",
    "```\n",
    "\n",
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up (uncomment to delete temp files)\n",
    "# import shutil\n",
    "# shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "# print(f\"Cleaned up: {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nOutput directory preserved at: {OUTPUT_DIR}\")\n",
    "print(\"Run the cleanup cell above to delete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
