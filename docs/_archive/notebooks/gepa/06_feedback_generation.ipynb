{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Feedback Generation\n",
    "\n",
    "This notebook covers the `LLMFeedbackGenerator` class, which provides rich diagnostic feedback for GEPA's reflective optimization. Learn how to:\n",
    "\n",
    "1. Initialize the feedback generator\n",
    "2. Generate single trajectory feedback\n",
    "3. Perform differential analysis (success vs failure)\n",
    "4. Generate rubric-specific feedback\n",
    "5. Combine all feedback types\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.integrations.gepa import (\n",
    "    KareninaDataInst,\n",
    "    KareninaTrajectory,\n",
    "    LLMFeedbackGenerator,\n",
    ")\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Load benchmark\n",
    "benchmark_path = Path.home() / \"Projects/karenina-monorepo/local_data/data/checkpoints/aime_2025.jsonld\"\n",
    "benchmark = Benchmark.load(benchmark_path)\n",
    "print(f\"Loaded: {benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Initializing LLMFeedbackGenerator\n",
    "\n",
    "The generator uses an LLM to analyze verification failures and provide actionable feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ModelConfig for the feedback LLM\n",
    "feedback_model_config = ModelConfig(\n",
    "    id=\"feedback-haiku\",\n",
    "    model_provider=\"anthropic\",\n",
    "    model_name=\"claude-haiku-4-5\",\n",
    "    temperature=0.7,  # Some creativity for suggestions\n",
    "    interface=\"langchain\",\n",
    ")\n",
    "\n",
    "# Initialize the generator\n",
    "generator = LLMFeedbackGenerator(feedback_model_config)\n",
    "\n",
    "print(\"LLMFeedbackGenerator initialized\")\n",
    "print(f\"  Model: {feedback_model_config.model_name}\")\n",
    "print(f\"  Temperature: {feedback_model_config.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating Sample Trajectories\n",
    "\n",
    "To demonstrate feedback generation, we'll create sample trajectories simulating verification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a question from the benchmark\n",
    "question_ids = benchmark.get_question_ids()\n",
    "question = benchmark.get_question(question_ids[0])\n",
    "template_code = benchmark.get_template(question_ids[0])\n",
    "\n",
    "print(f\"Sample question: {question['question'][:80]}...\")\n",
    "print(f\"Expected answer: {question['raw_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data instance\n",
    "data_inst = KareninaDataInst(\n",
    "    question_id=question_ids[0],\n",
    "    question_text=question[\"question\"],\n",
    "    raw_answer=question[\"raw_answer\"],\n",
    "    template_code=template_code or \"\",\n",
    ")\n",
    "\n",
    "print(f\"Created KareninaDataInst for: {data_inst.question_id[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run actual verification to get real trajectories\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are a math expert. Provide only the final integer answer.\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "print(\"Running verification on 3 questions...\")\n",
    "results = benchmark.run_verification(config, question_ids=question_ids[:3])\n",
    "print(f\"Got {len(results.results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to trajectories\n",
    "from karenina.integrations.gepa import compute_single_score, extract_failed_fields, questions_to_data_insts\n",
    "\n",
    "data_insts = questions_to_data_insts(benchmark, question_ids[:3])\n",
    "\n",
    "trajectories = []\n",
    "for inst, result in zip(data_insts, results.results, strict=False):\n",
    "    traj = KareninaTrajectory(\n",
    "        data_inst=inst,\n",
    "        model_name=result.metadata.answering_model or \"claude-haiku-4-5\",\n",
    "        model_config=config.answering_models[0],\n",
    "        optimized_components={\"answering_system_prompt\": config.answering_models[0].system_prompt},\n",
    "        verification_result=result,\n",
    "        score=compute_single_score(result, template_weight=1.0, rubric_weight=0.0),\n",
    "        raw_llm_response=result.template.raw_llm_response if result.template else None,\n",
    "        parsing_error=result.metadata.error,\n",
    "        failed_fields=extract_failed_fields(result),\n",
    "        rubric_scores=None,\n",
    "    )\n",
    "    trajectories.append(traj)\n",
    "\n",
    "print(f\"Created {len(trajectories)} trajectories:\")\n",
    "for t in trajectories:\n",
    "    status = \"PASS\" if t.passed() else \"FAIL\"\n",
    "    print(f\"  {status}: Score {t.score:.2f} | {t.data_inst.question_text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## generate_single_feedback(): Analyzing One Failure\n",
    "\n",
    "Analyze a single failed trajectory to understand why it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a failed trajectory\n",
    "failed_trajs = [t for t in trajectories if not t.passed()]\n",
    "passed_trajs = [t for t in trajectories if t.passed()]\n",
    "\n",
    "print(f\"Failed trajectories: {len(failed_trajs)}\")\n",
    "print(f\"Passed trajectories: {len(passed_trajs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if failed_trajs:\n",
    "    failed_traj = failed_trajs[0]\n",
    "\n",
    "    print(\"Analyzing failed trajectory...\")\n",
    "    print(f\"  Question: {failed_traj.data_inst.question_text[:60]}...\")\n",
    "    print(f\"  Expected: {failed_traj.data_inst.raw_answer}\")\n",
    "    print(f\"  Response: {failed_traj.raw_llm_response[:100] if failed_traj.raw_llm_response else 'None'}...\")\n",
    "\n",
    "    # Generate single feedback\n",
    "    feedback = generator.generate_single_feedback(failed_traj)\n",
    "\n",
    "    print(\"\\n=== LLM Feedback ===\")\n",
    "    print(feedback)\n",
    "else:\n",
    "    print(\"No failed trajectories to analyze.\")\n",
    "    print(\"Using a passed trajectory for demonstration...\")\n",
    "\n",
    "    if passed_trajs:\n",
    "        demo_traj = passed_trajs[0]\n",
    "        print(f\"  Question: {demo_traj.data_inst.question_text[:60]}...\")\n",
    "        print(f\"  Answer: {demo_traj.data_inst.raw_answer}\")\n",
    "        print(f\"  Response: {demo_traj.raw_llm_response[:100] if demo_traj.raw_llm_response else 'None'}...\")\n",
    "        print(f\"  Score: {demo_traj.score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## generate_differential_feedback(): Comparing Success vs Failure\n",
    "\n",
    "Compare a failed trajectory against successful ones to identify what works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if failed_trajs and passed_trajs:\n",
    "    failed_traj = failed_trajs[0]\n",
    "\n",
    "    print(\"Performing differential analysis...\")\n",
    "    print(f\"  Failed: {failed_traj.model_name}\")\n",
    "    print(f\"  Comparing against {len(passed_trajs)} successful trajectories\")\n",
    "\n",
    "    # Generate differential feedback\n",
    "    diff_feedback = generator.generate_differential_feedback(\n",
    "        failed_trajectory=failed_traj,\n",
    "        successful_trajectories=passed_trajs,\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Differential Feedback ===\")\n",
    "    print(diff_feedback)\n",
    "else:\n",
    "    print(\"Need both failed and passed trajectories for differential analysis.\")\n",
    "    print(\"\\nDifferential feedback compares:\")\n",
    "    print(\"  - What successful models did differently\")\n",
    "    print(\"  - The specific failure mode\")\n",
    "    print(\"  - Concrete prompt improvements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## generate_rubric_feedback(): Rubric-Specific Analysis\n",
    "\n",
    "Analyze why specific rubric traits failed or scored low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate rubric scores\n",
    "sample_rubric_scores = {\n",
    "    \"Conciseness\": 3,  # 3/5 - moderate\n",
    "    \"ShowsWork\": False,  # Failed - didn't show reasoning\n",
    "    \"CorrectFormat\": True,  # Passed - answer format was correct\n",
    "    \"Accuracy\": 0.4,  # Low score\n",
    "}\n",
    "\n",
    "if trajectories:\n",
    "    traj = trajectories[0]\n",
    "\n",
    "    print(\"Generating rubric feedback...\")\n",
    "    print(f\"  Rubric scores: {sample_rubric_scores}\")\n",
    "\n",
    "    rubric_feedback = generator.generate_rubric_feedback(\n",
    "        trajectory=traj,\n",
    "        rubric_scores=sample_rubric_scores,\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Rubric Feedback ===\")\n",
    "    print(rubric_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## generate_complete_feedback(): Combined Analysis\n",
    "\n",
    "The main entry point that combines template verification and rubric feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trajectories:\n",
    "    traj = trajectories[0]\n",
    "\n",
    "    print(\"Generating complete feedback...\")\n",
    "\n",
    "    complete_feedback = generator.generate_complete_feedback(\n",
    "        failed_trajectory=traj,\n",
    "        successful_trajectories=passed_trajs if passed_trajs else None,\n",
    "        rubric_scores=sample_rubric_scores,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"COMPLETE FEEDBACK\")\n",
    "    print(\"=\" * 50)\n",
    "    print(complete_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feedback System Prompts\n",
    "\n",
    "The generator uses specialized system prompts for each feedback type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.integrations.gepa.feedback import (\n",
    "    DIFFERENTIAL_FEEDBACK_SYSTEM_PROMPT,\n",
    "    RUBRIC_FEEDBACK_SYSTEM_PROMPT,\n",
    "    SINGLE_FEEDBACK_SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"=== Single Feedback System Prompt ===\")\n",
    "print(SINGLE_FEEDBACK_SYSTEM_PROMPT)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Differential Feedback System Prompt ===\")\n",
    "print(DIFFERENTIAL_FEEDBACK_SYSTEM_PROMPT)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Rubric Feedback System Prompt ===\")\n",
    "print(RUBRIC_FEEDBACK_SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Integration with KareninaAdapter\n",
    "\n",
    "The feedback generator integrates with the adapter via `make_reflective_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual integration\n",
    "print(\"\"\"\n",
    "# Integration with KareninaAdapter\n",
    "\n",
    "from karenina.integrations.gepa import KareninaAdapter, OptimizationTarget\n",
    "\n",
    "# Create adapter with feedback generator\n",
    "adapter = KareninaAdapter(\n",
    "    benchmark=benchmark,\n",
    "    base_config=verification_config,\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    feedback_model_config=ModelConfig(\n",
    "        model_provider=\"anthropic\",\n",
    "        model_name=\"claude-haiku-4-5\",\n",
    "        temperature=0.7,\n",
    "        interface=\"langchain\",\n",
    "    ),\n",
    "    enable_differential_analysis=True,\n",
    ")\n",
    "\n",
    "# Evaluate candidate\n",
    "eval_batch = adapter.evaluate(batch, candidate, capture_traces=True)\n",
    "\n",
    "# Build reflective dataset (uses LLMFeedbackGenerator internally)\n",
    "reflective_data = adapter.make_reflective_dataset(\n",
    "    candidate=candidate,\n",
    "    eval_batch=eval_batch,\n",
    "    components_to_update=[\"answering_system_prompt\"],\n",
    ")\n",
    "\n",
    "# The reflective_data now contains LLM-generated feedback\n",
    "# for GEPA's reflection LLM to use\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Method | Purpose | When to Use |\n",
    "|--------|---------|-------------|\n",
    "| `generate_single_feedback()` | Analyze one failure | Single model fails |\n",
    "| `generate_differential_feedback()` | Compare success vs failure | Multiple models, some pass |\n",
    "| `generate_rubric_feedback()` | Analyze rubric scores | Quality traits enabled |\n",
    "| `generate_complete_feedback()` | Combined analysis | Full optimization |\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "1. **Rich diagnostics**: LLM provides human-readable explanations\n",
    "2. **Differential analysis**: Learn from successful models\n",
    "3. **Rubric integration**: Understand quality trait failures\n",
    "4. **Actionable suggestions**: Concrete prompt improvements\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [07_tracking_runs.ipynb](07_tracking_runs.ipynb) - Optimization run tracking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
