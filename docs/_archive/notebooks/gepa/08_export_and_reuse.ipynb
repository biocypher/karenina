{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export and Reuse Optimized Prompts\n",
    "\n",
    "This notebook covers exporting optimized prompts for reuse in production. Learn how to:\n",
    "\n",
    "1. Export as Karenina verification presets\n",
    "2. Export as lightweight JSON files\n",
    "3. Load saved prompts\n",
    "4. Generate comparison reports\n",
    "5. Use optimized prompts in verification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.integrations.gepa import (\n",
    "    OptimizationTarget,\n",
    "    export_comparison_report,\n",
    "    export_prompts_json,\n",
    "    export_to_preset,\n",
    "    load_prompts_json,\n",
    ")\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Create temp directory for exports\n",
    "temp_dir = Path(tempfile.mkdtemp(prefix=\"gepa_export_\"))\n",
    "print(f\"Using temp directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark\n",
    "benchmark_path = Path.home() / \"Projects/karenina-monorepo/local_data/data/checkpoints/aime_2025.jsonld\"\n",
    "benchmark = Benchmark.load(benchmark_path)\n",
    "print(f\"Loaded: {benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sample Optimized Prompts\n",
    "\n",
    "Let's create sample optimized prompts (as would result from GEPA optimization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated optimization result\n",
    "optimized_prompts = {\n",
    "    \"answering_system_prompt\": \"\"\"You are an expert competition mathematician specializing in AIME problems.\n",
    "\n",
    "IMPORTANT GUIDELINES:\n",
    "1. AIME answers are ALWAYS integers from 0 to 999\n",
    "2. Show complete step-by-step reasoning\n",
    "3. Verify your answer by checking edge cases\n",
    "4. State your final answer clearly: \"The answer is [N]\"\n",
    "\n",
    "Solve the following problem systematically:\"\"\",\n",
    "    \"parsing_instructions\": \"\"\"Extract the final integer answer from the response.\n",
    "\n",
    "Look for patterns like:\n",
    "- \"The answer is [N]\"\n",
    "- \"Therefore, [N]\"\n",
    "- Boxed answers: \\\\boxed{N}\n",
    "- Final integer mentioned at the end\n",
    "\n",
    "Return only the integer value (0-999).\"\"\",\n",
    "}\n",
    "\n",
    "print(\"Optimized prompts:\")\n",
    "for key, value in optimized_prompts.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  {value[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## export_to_preset(): Full Verification Preset\n",
    "\n",
    "Export optimized prompts as a complete Karenina verification preset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base verification config\n",
    "base_config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"PLACEHOLDER\",  # Will be replaced\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "print(\"Base config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as preset\n",
    "preset_path = export_to_preset(\n",
    "    optimized_prompts=optimized_prompts,\n",
    "    base_config=base_config,\n",
    "    output_path=temp_dir / \"aime_optimized.json\",\n",
    "    targets=[\n",
    "        OptimizationTarget.ANSWERING_SYSTEM_PROMPT,\n",
    "        OptimizationTarget.PARSING_INSTRUCTIONS,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Exported preset to: {preset_path}\")\n",
    "print(f\"File size: {preset_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the preset content\n",
    "with open(preset_path) as f:\n",
    "    preset_data = json.load(f)\n",
    "\n",
    "print(\"Preset structure:\")\n",
    "print(f\"  Keys: {list(preset_data.keys())}\")\n",
    "\n",
    "# Check the injected system prompt\n",
    "answering_models = preset_data.get(\"answering_models\", [])\n",
    "if answering_models:\n",
    "    system_prompt = answering_models[0].get(\"system_prompt\", \"\")\n",
    "    print(\"\\nInjected system prompt:\")\n",
    "    print(f\"  {system_prompt[:100]}...\")\n",
    "\n",
    "# Check GEPA metadata\n",
    "gepa_meta = preset_data.get(\"_gepa_optimization\", {})\n",
    "print(\"\\nGEPA metadata:\")\n",
    "print(f\"  Exported at: {gepa_meta.get('exported_at')}\")\n",
    "print(f\"  Targets: {gepa_meta.get('targets')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Preset with CLI\n",
    "\n",
    "The exported preset can be used with the Karenina CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "# Use the preset with karenina CLI:\n",
    "\n",
    "karenina verify aime_2025.jsonld --preset aime_optimized.json\n",
    "\n",
    "# Or with additional options:\n",
    "karenina verify aime_2025.jsonld \\\\\n",
    "    --preset aime_optimized.json \\\\\n",
    "    --output results.json \\\\\n",
    "    --format json\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## export_prompts_json(): Lightweight Export\n",
    "\n",
    "Export just the prompts with metadata (smaller, more portable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as lightweight JSON\n",
    "metadata = {\n",
    "    \"benchmark\": \"AIME 2025\",\n",
    "    \"train_score\": 0.80,\n",
    "    \"val_score\": 0.75,\n",
    "    \"test_score\": 0.72,\n",
    "    \"improvement\": 0.25,\n",
    "    \"reflection_model\": \"anthropic/claude-haiku-4-5\",\n",
    "    \"total_generations\": 15,\n",
    "}\n",
    "\n",
    "prompts_path = export_prompts_json(\n",
    "    optimized_prompts=optimized_prompts,\n",
    "    metadata=metadata,\n",
    "    output_path=temp_dir / \"optimized_prompts.json\",\n",
    ")\n",
    "\n",
    "print(f\"Exported prompts to: {prompts_path}\")\n",
    "print(f\"File size: {prompts_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the prompts file\n",
    "with open(prompts_path) as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "print(\"Prompts file structure:\")\n",
    "print(json.dumps(prompts_data, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## load_prompts_json(): Loading Saved Prompts\n",
    "\n",
    "Load prompts from a saved file for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompts\n",
    "loaded_prompts, loaded_metadata = load_prompts_json(prompts_path)\n",
    "\n",
    "print(\"Loaded prompts:\")\n",
    "for key in loaded_prompts:\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "print(\"\\nLoaded metadata:\")\n",
    "for key, value in loaded_metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use loaded prompts in a new verification config\n",
    "new_config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=loaded_prompts[\"answering_system_prompt\"],  # Use loaded prompt\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "print(\"Created new config with loaded prompts\")\n",
    "print(f\"  System prompt: {new_config.answering_models[0].system_prompt[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## export_comparison_report(): Multi-Run Reports\n",
    "\n",
    "Generate comparison reports across multiple optimization runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple runs\n",
    "runs_data = [\n",
    "    {\n",
    "        \"run_id\": \"run_001\",\n",
    "        \"benchmark_name\": \"AIME 2025\",\n",
    "        \"targets\": [\"answering_system_prompt\"],\n",
    "        \"train_score\": 0.70,\n",
    "        \"val_score\": 0.65,\n",
    "        \"improvement\": 0.08,\n",
    "        \"reflection_model\": \"claude-haiku\",\n",
    "        \"metric_calls\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"run_id\": \"run_002\",\n",
    "        \"benchmark_name\": \"AIME 2025\",\n",
    "        \"targets\": [\"answering_system_prompt\"],\n",
    "        \"train_score\": 0.75,\n",
    "        \"val_score\": 0.70,\n",
    "        \"improvement\": 0.17,\n",
    "        \"reflection_model\": \"claude-haiku\",\n",
    "        \"metric_calls\": 75,\n",
    "    },\n",
    "    {\n",
    "        \"run_id\": \"run_003\",\n",
    "        \"benchmark_name\": \"AIME 2025\",\n",
    "        \"targets\": [\"answering_system_prompt\", \"parsing_instructions\"],\n",
    "        \"train_score\": 0.80,\n",
    "        \"val_score\": 0.75,\n",
    "        \"improvement\": 0.25,\n",
    "        \"reflection_model\": \"claude-sonnet\",\n",
    "        \"metric_calls\": 100,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Export comparison report\n",
    "report_path = export_comparison_report(\n",
    "    runs=runs_data,\n",
    "    output_path=temp_dir / \"comparison_report.json\",\n",
    ")\n",
    "\n",
    "print(f\"Exported comparison report to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the report\n",
    "with open(report_path) as f:\n",
    "    report = json.load(f)\n",
    "\n",
    "print(\"Comparison Report:\")\n",
    "print(f\"  Generated at: {report['generated_at']}\")\n",
    "print(f\"  Number of runs: {report['num_runs']}\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for key, value in report[\"summary\"].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using Optimized Prompts in Verification\n",
    "\n",
    "Complete example of loading and using optimized prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimized prompts\n",
    "prompts, meta = load_prompts_json(prompts_path)\n",
    "\n",
    "print(f\"Loaded prompts from run with {meta['improvement']:.2%} improvement\")\n",
    "\n",
    "# Create verification config\n",
    "optimized_config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=prompts[\"answering_system_prompt\"],\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "# Run verification\n",
    "print(\"\\nRunning verification with optimized prompts...\")\n",
    "question_ids = benchmark.get_question_ids()[:3]\n",
    "results = benchmark.run_verification(optimized_config, question_ids=question_ids)\n",
    "\n",
    "# Show results\n",
    "passed = sum(1 for r in results.results if r.template and r.template.verify_result)\n",
    "print(f\"\\nResults: {passed}/{len(results.results)} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List exported files\n",
    "print(\"Exported files:\")\n",
    "for f in temp_dir.iterdir():\n",
    "    print(f\"  {f.name}: {f.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "print(f\"Cleaned up: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Function | Output | Use Case |\n",
    "|----------|--------|----------|\n",
    "| `export_to_preset()` | Full VerificationConfig JSON | CLI usage with `--preset` |\n",
    "| `export_prompts_json()` | Prompts + metadata JSON | Portable, lightweight sharing |\n",
    "| `load_prompts_json()` | (prompts, metadata) tuple | Load for programmatic use |\n",
    "| `export_comparison_report()` | Multi-run analysis JSON | Compare optimization strategies |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [09_full_optimization_workflow.ipynb](09_full_optimization_workflow.ipynb) - Complete end-to-end example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
