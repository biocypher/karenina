# Manual Interface Workflow

This page walks through the step-by-step workflow for running verification with pre-recorded traces using the manual interface. For conceptual background on how the manual interface works, see [Manual Interface Concepts](../core_concepts/manual-interface.md).

## When to Use This Workflow

| Scenario | Example |
|----------|---------|
| **Iterate on templates/rubrics** | You already have model answers and want to refine evaluation criteria without re-generating responses |
| **Reproduce evaluations** | Re-run the same traces under different configurations (models, features, settings) |
| **External outputs** | Evaluate responses generated by systems outside Karenina (other frameworks, production logs, human-written answers) |
| **Cost-sensitive testing** | Skip the most expensive step (answer generation) when debugging parsing or rubric logic |
| **Controlled experiments** | Test known-good and known-bad answers to validate template correctness |

## Step-by-Step Workflow

```
1. Load benchmark    ──► 2. Prepare traces    ──► 3. Register traces
                                                        │
4. Configure         ◄──────────────────────────────────┘
        │
5. Run verification  ──► 6. Inspect results
```

### Step 1: Load or Create a Benchmark

Start with an existing benchmark or create one from scratch:

```python
from karenina.benchmark import Benchmark

# Load an existing benchmark
benchmark = Benchmark.load("checkpoint.jsonld")

# Or create a new one
benchmark = Benchmark("my_experiment")
benchmark.add_question(
    question="What is the capital of France?",
    raw_answer="Paris",
    answer_template="""
from pydantic import Field
from karenina.schemas.entities import BaseAnswer

class Answer(BaseAnswer):
    capital: str = Field(description="The capital city mentioned in the response")

    def verify(self) -> bool:
        return self.capital.strip().lower() == "paris"
"""
)
```

### Step 2: Prepare Traces

Traces are the pre-recorded model responses you want to evaluate. The simplest format is a plain text string:

```python
traces = {
    "What is the capital of France?": "The capital of France is Paris. It has been the capital since the 10th century.",
    "What is 6 times 7?": "6 times 7 equals 42.",
}
```

#### Trace JSON File Format

For file-based loading (especially useful with the CLI), create a JSON file mapping question MD5 hashes to trace strings:

```json
{
    "d41833c30b820e3a3de0ad93e7a7244f": "The capital of France is Paris. It has been the capital since the 10th century.",
    "4a68b75506e58a5fd93e5de1fdd2e8f0": "6 times 7 equals 42."
}
```

#### Computing Question Hashes

Each question is identified by the MD5 hash of its text. You can compute hashes manually:

```python
import hashlib

question_hash = hashlib.md5(
    "What is the capital of France?".encode("utf-8")
).hexdigest()
# "d41833c30b820e3a3de0ad93e7a7244f"
```

Or extract hashes from a loaded benchmark:

```python
benchmark = Benchmark.load("checkpoint.jsonld")

for qid in benchmark.get_question_ids():
    q = benchmark.get_question(qid)
    question_hash = hashlib.md5(q["question"].encode("utf-8")).hexdigest()
    print(f"{question_hash}  {q['question'][:60]}")
```

### Step 3: Register Traces

Use the `ManualTraces` class to register traces for a benchmark:

```python
from karenina.adapters.manual import ManualTraces

manual_traces = ManualTraces(benchmark)
```

**Register by question text** (recommended — avoids manual hash computation):

```python
manual_traces.register_trace(
    "What is the capital of France?",
    "The capital of France is Paris.",
    map_to_id=True
)
```

**Batch register** (most convenient for multiple traces):

```python
manual_traces.register_traces({
    "What is the capital of France?": "The capital of France is Paris.",
    "What is 6 times 7?": "6 times 7 equals 42.",
}, map_to_id=True)
```

**Register by hash directly** (useful when working with exported hash mappings):

```python
manual_traces.register_trace(
    "d41833c30b820e3a3de0ad93e7a7244f",
    "The capital of France is Paris."
)
```

**Load from a JSON file**:

```python
from karenina.adapters.manual import load_manual_traces_from_file

manual_traces = load_manual_traces_from_file(
    "traces/my_traces.json",
    benchmark
)
```

!!! note "Exact text matching"
    When using `map_to_id=True`, the question text must match the benchmark exactly — case-sensitive, including whitespace. If a question is not found, a `ValueError` is raised with the computed hash and the number of indexed questions.

### Step 4: Configure Verification

Create a `ModelConfig` with `interface="manual"` for the answering model, and a separate parsing model for the judge:

```python
from karenina.schemas import ModelConfig, VerificationConfig

# Answering model uses manual interface
manual_config = ModelConfig(
    interface="manual",
    manual_traces=manual_traces
)

# Parsing model is a live LLM (the judge)
judge_config = ModelConfig(
    id="claude-haiku-4-5",
    model_provider="anthropic",
    model_name="claude-haiku-4-5",
    interface="langchain"
)

config = VerificationConfig(
    answering_models=[manual_config],
    parsing_models=[judge_config]
)
```

Key configuration points:

- `id` and `model_name` are **auto-set to `"manual"`** — you don't need to provide them
- A **separate parsing model** is always required to parse traces into template schemas
- `manual_traces` is **excluded from serialization** (not saved in presets)
- **MCP tools are not supported** with the manual interface

### Step 5: Run Verification

Run verification exactly as you would with live models:

```python
results = benchmark.run_verification(config)
```

The pipeline bypasses answer generation and feeds the pre-recorded traces directly into the parsing stage. Everything after parsing (template verification, rubric evaluation, deep judgment) works identically to live verification.

### Step 6: Inspect Results

Results have the same structure as live verification results:

```python
for result in results:
    q = result.metadata.question_text[:50]
    status = "PASS" if result.template and result.template.verify_result else "FAIL"
    print(f"{q}... → {status}")
```

The `metadata.answering` field will show `interface: manual` and `model_name: manual` to identify these as manual trace results.

## CLI Workflow

For file-based traces, use the CLI directly:

```bash
karenina verify checkpoint.jsonld \
  --interface manual \
  --manual-traces traces/my_traces.json \
  --parsing-model claude-haiku-4-5 \
  --parsing-provider anthropic
```

The `--manual-traces` flag accepts a JSON file path with the `{question_hash: trace_string}` format described above.

See [CLI Verification](cli.md#manual-traces) for more CLI options.

## Common Patterns

### Iterating on Templates

The most common use case — refine templates without re-generating answers:

```python
# 1. Run initial verification with live models
config = VerificationConfig(
    answering_models=[live_model],
    parsing_models=[judge_model]
)
results = benchmark.run_verification(config)

# 2. Identify failing questions, fix templates

# 3. Re-run with manual traces (reusing the same answers)
manual_traces = ManualTraces(benchmark)
# ... register traces from step 1 ...

manual_config = ModelConfig(interface="manual", manual_traces=manual_traces)
config = VerificationConfig(
    answering_models=[manual_config],
    parsing_models=[judge_model]
)
results = benchmark.run_verification(config)
```

### Comparing Parsing Models

Evaluate how different judges interpret the same traces:

```python
manual_config = ModelConfig(interface="manual", manual_traces=manual_traces)

for judge_name, judge_provider in [("claude-haiku-4-5", "anthropic"), ("claude-sonnet-4-5-20250929", "anthropic")]:
    judge = ModelConfig(
        id=judge_name,
        model_provider=judge_provider,
        model_name=judge_name,
        interface="langchain"
    )
    config = VerificationConfig(
        answering_models=[manual_config],
        parsing_models=[judge]
    )
    results = benchmark.run_verification(config)
    summary = results.get_summary()
    print(f"{judge_name}: {summary.get('template_pass_overall', 'N/A')}")
```

### Populating Traces After Config Creation

The `ManualTraces` object is passed by reference, so you can populate traces after creating the config:

```python
# Create config structure first
manual_traces = ManualTraces(benchmark)
manual_config = ModelConfig(interface="manual", manual_traces=manual_traces)
config = VerificationConfig(
    answering_models=[manual_config],
    parsing_models=[judge_config]
)

# Later, populate traces (e.g., from file, database, API)
for question_text, trace in load_traces_from_source():
    manual_traces.register_trace(question_text, trace, map_to_id=True)

# Run verification with populated traces
results = benchmark.run_verification(config)
```

### Preset Workflow

Since `manual_traces` is excluded from serialization, you need to re-register traces when loading a preset:

```python
# Load preset (manual_traces will be None)
config = VerificationConfig.from_preset("my_manual_preset.json")

# Re-populate traces
manual_traces = ManualTraces(benchmark)
manual_traces.register_traces(traces_dict, map_to_id=True)

# Update config with traces
config.answering_models[0].manual_traces = manual_traces

# Run verification
results = benchmark.run_verification(config)
```

## Validating Before Running

Check that traces are registered correctly before running verification:

```python
from karenina.adapters.manual import has_manual_trace, get_manual_trace_count
import hashlib

# Check total trace count
print(f"Registered traces: {get_manual_trace_count()}")
print(f"Benchmark questions: {benchmark.question_count}")

# Verify individual traces
for qid in benchmark.get_question_ids():
    q = benchmark.get_question(qid)
    qhash = hashlib.md5(q["question"].encode("utf-8")).hexdigest()
    if not has_manual_trace(qhash):
        print(f"Missing trace for: {q['question'][:60]}...")
```

## Troubleshooting

**"No manual trace found for question hash"**

The trace was not registered for that question. Check:

- Was the trace registered with the correct question text or hash?
- Did traces expire? (session timeout is 1 hour of inactivity)
- Use `has_manual_trace(hash)` to verify registration

**"Question not found in benchmark"**

When using `map_to_id=True`, the question text must match exactly. Check:

- Case sensitivity (the match is case-sensitive)
- Whitespace (leading/trailing spaces matter)
- Use the hash directly with `map_to_id=False` as a fallback

**"manual_traces is required when interface='manual'"**

The `ModelConfig` was created with `interface="manual"` but without a `manual_traces` argument. Pass the `ManualTraces` object to the constructor.

## Next Steps

- [Manual Interface Concepts](../core_concepts/manual-interface.md) — Conceptual background on trace formats and the adapter architecture
- [Verification Config](verification-config.md) — Configure evaluation modes, features, and models
- [Python API](python-api.md) — Complete verification workflow reference
- [CLI Verification](cli.md#manual-traces) — Using `--interface manual` from the command line
- [Analyzing Results](../07-analyzing-results/index.md) — Explore and export verification results
