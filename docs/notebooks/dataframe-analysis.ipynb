{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929ee8a8",
   "metadata": {},
   "source": [
    "# DataFrame Analysis\n",
    "\n",
    "Karenina provides a DataFrame-first approach for analyzing verification results.\n",
    "By converting results to pandas DataFrames, you can use the full power of pandas\n",
    "for filtering, grouping, aggregation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4024575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.121295Z",
     "iopub.status.busy": "2026-02-06T07:04:44.121063Z",
     "iopub.status.idle": "2026-02-06T07:04:44.451269Z",
     "shell.execute_reply": "2026-02-06T07:04:44.450995Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell: creates mock VerificationResult objects for documentation examples.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "# Build mock results representing a verification run with template + rubric evaluation\n",
    "_answering_gpt4o = ModelIdentity(model_name=\"gpt-4o\", interface=\"langchain\")\n",
    "_answering_claude = ModelIdentity(model_name=\"claude-sonnet-4-20250514\", interface=\"claude_agent_sdk\")\n",
    "_parsing = ModelIdentity(model_name=\"gpt-4o-mini\", interface=\"langchain\")\n",
    "_ts = datetime.datetime.now(tz=datetime.UTC).isoformat()\n",
    "\n",
    "\n",
    "def _make_result(\n",
    "    qid, question_text, answering, verified, response,\n",
    "    rubric_scores=None, regex_scores=None, callable_scores=None,\n",
    "    parsed_gt=None, parsed_llm=None, replicate=None,\n",
    "):\n",
    "    rid = VerificationResultMetadata.compute_result_id(\n",
    "        qid, answering, _parsing, _ts, replicate\n",
    "    )\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=response,\n",
    "        verify_result=verified,\n",
    "        template_verification_performed=True,\n",
    "        parsed_gt_response=parsed_gt or {\"answer\": response},\n",
    "        parsed_llm_response=parsed_llm or {\"answer\": response},\n",
    "    )\n",
    "    rubric = None\n",
    "    if rubric_scores or regex_scores or callable_scores:\n",
    "        rubric = VerificationResultRubric(\n",
    "            rubric_evaluation_performed=True,\n",
    "            llm_trait_scores=rubric_scores,\n",
    "            regex_trait_scores=regex_scores,\n",
    "            callable_trait_scores=callable_scores,\n",
    "        )\n",
    "    return VerificationResult(\n",
    "        metadata=VerificationResultMetadata(\n",
    "            question_id=qid,\n",
    "            template_id=\"tmpl_\" + qid[:8],\n",
    "            completed_without_errors=True,\n",
    "            question_text=question_text,\n",
    "            answering=answering,\n",
    "            parsing=_parsing,\n",
    "            execution_time=1.5,\n",
    "            timestamp=_ts,\n",
    "            result_id=rid,\n",
    "            replicate=replicate,\n",
    "        ),\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create results for two models across 3 questions\n",
    "_mock_results = [\n",
    "    # GPT-4o results\n",
    "    _make_result(\"q1\", \"What is the capital of France?\", _answering_gpt4o, True, \"Paris\",\n",
    "                 rubric_scores={\"clarity\": 4, \"conciseness\": True},\n",
    "                 regex_scores={\"no_hedging\": True},\n",
    "                 parsed_gt={\"capital\": \"Paris\"}, parsed_llm={\"capital\": \"Paris\"}),\n",
    "    _make_result(\"q2\", \"What is 6 multiplied by 7?\", _answering_gpt4o, True, \"42\",\n",
    "                 rubric_scores={\"clarity\": 5, \"conciseness\": True},\n",
    "                 parsed_gt={\"result\": \"42\"}, parsed_llm={\"result\": \"42\"}),\n",
    "    _make_result(\"q3\", \"What element has atomic number 8?\", _answering_gpt4o, False, \"Nitrogen\",\n",
    "                 rubric_scores={\"clarity\": 3, \"conciseness\": False},\n",
    "                 parsed_gt={\"element\": \"Oxygen\"}, parsed_llm={\"element\": \"Nitrogen\"}),\n",
    "    # Claude results\n",
    "    _make_result(\"q1\", \"What is the capital of France?\", _answering_claude, True, \"Paris\",\n",
    "                 rubric_scores={\"clarity\": 5, \"conciseness\": True},\n",
    "                 regex_scores={\"no_hedging\": True},\n",
    "                 parsed_gt={\"capital\": \"Paris\"}, parsed_llm={\"capital\": \"Paris\"}),\n",
    "    _make_result(\"q2\", \"What is 6 multiplied by 7?\", _answering_claude, True, \"42\",\n",
    "                 rubric_scores={\"clarity\": 5, \"conciseness\": True},\n",
    "                 parsed_gt={\"result\": \"42\"}, parsed_llm={\"result\": \"42\"}),\n",
    "    _make_result(\"q3\", \"What element has atomic number 8?\", _answering_claude, True, \"Oxygen\",\n",
    "                 rubric_scores={\"clarity\": 4, \"conciseness\": True},\n",
    "                 parsed_gt={\"element\": \"Oxygen\"}, parsed_llm={\"element\": \"Oxygen\"}),\n",
    "]\n",
    "\n",
    "results = VerificationResultSet(results=_mock_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f3058",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "After running verification, you receive a `VerificationResultSet` containing all\n",
    "results. The result set provides three specialized accessors that convert results\n",
    "to pandas DataFrames:\n",
    "\n",
    "| Accessor | Returns | Rows Represent |\n",
    "|----------|---------|----------------|\n",
    "| `get_template_results()` | `TemplateResults` | One row per **parsed field** comparison |\n",
    "| `get_rubrics_results()` | `RubricResults` | One row per **rubric trait** evaluated |\n",
    "| `get_judgment_results()` | `JudgmentResults` | One row per **(attribute x excerpt)** pair |\n",
    "\n",
    "Each accessor returns a wrapper object with a `.to_dataframe()` method plus\n",
    "filtering, grouping, and aggregation helpers.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The basic workflow is: extract a result type, convert to DataFrame, analyze\n",
    "with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d98a5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.452581Z",
     "iopub.status.busy": "2026-02-06T07:04:44.452495Z",
     "iopub.status.idle": "2026-02-06T07:04:44.454872Z",
     "shell.execute_reply": "2026-02-06T07:04:44.454683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (6, 34)\n",
      "Columns: ['completed_without_errors', 'error', 'recursion_limit_reached', 'question_id', 'template_id', 'question_text', 'keywords', 'replicate']...\n"
     ]
    }
   ],
   "source": [
    "# Extract template results and convert to DataFrame\n",
    "template_results = results.get_template_results()\n",
    "df = template_results.to_dataframe()\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns[:8])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e129b",
   "metadata": {},
   "source": [
    "## Template DataFrames\n",
    "\n",
    "`TemplateResults` provides three DataFrame methods:\n",
    "\n",
    "| Method | Exploded By | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `to_dataframe()` | Parsed fields | Field-level pass/fail analysis |\n",
    "| `to_regex_dataframe()` | Regex patterns | Format compliance analysis |\n",
    "| `to_usage_dataframe()` | Token usage stages | Cost analysis |\n",
    "\n",
    "### Field-Level Analysis\n",
    "\n",
    "The main template DataFrame creates one row per parsed field, enabling\n",
    "field-level comparison between ground truth and LLM-extracted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d63210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.455853Z",
     "iopub.status.busy": "2026-02-06T07:04:44.455799Z",
     "iopub.status.idle": "2026-02-06T07:04:44.458994Z",
     "shell.execute_reply": "2026-02-06T07:04:44.458809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field comparison columns:\n",
      "question_id                           answering_model field_name gt_value llm_value  field_match\n",
      "         q1                          langchain:gpt-4o    capital    Paris     Paris         True\n",
      "         q2                          langchain:gpt-4o     result       42        42         True\n",
      "         q3                          langchain:gpt-4o    element   Oxygen  Nitrogen        False\n",
      "         q1 claude_agent_sdk:claude-sonnet-4-20250514    capital    Paris     Paris         True\n",
      "         q2 claude_agent_sdk:claude-sonnet-4-20250514     result       42        42         True\n",
      "         q3 claude_agent_sdk:claude-sonnet-4-20250514    element   Oxygen    Oxygen         True\n"
     ]
    }
   ],
   "source": [
    "template_results = results.get_template_results()\n",
    "df = template_results.to_dataframe()\n",
    "\n",
    "# Key columns for field-level analysis\n",
    "print(\"Field comparison columns:\")\n",
    "print(df[[\"question_id\", \"answering_model\", \"field_name\", \"gt_value\", \"llm_value\", \"field_match\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db50235",
   "metadata": {},
   "source": [
    "### Pass Rate by Model\n",
    "\n",
    "A common analysis pattern: calculate template verification pass rates grouped\n",
    "by answering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a668f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.459897Z",
     "iopub.status.busy": "2026-02-06T07:04:44.459827Z",
     "iopub.status.idle": "2026-02-06T07:04:44.462283Z",
     "shell.execute_reply": "2026-02-06T07:04:44.462105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rate by model:\n",
      "  claude_agent_sdk:claude-sonnet-4-20250514: 100%\n",
      "  langchain:gpt-4o: 67%\n"
     ]
    }
   ],
   "source": [
    "# Use the built-in aggregation helper\n",
    "pass_rates = template_results.aggregate_pass_rate(by=\"answering_model\")\n",
    "print(\"Pass rate by model:\")\n",
    "for model, rate in pass_rates.items():\n",
    "    print(f\"  {model}: {rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3012e",
   "metadata": {},
   "source": [
    "### Pass Rate by Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee3eb2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.463222Z",
     "iopub.status.busy": "2026-02-06T07:04:44.463154Z",
     "iopub.status.idle": "2026-02-06T07:04:44.465449Z",
     "shell.execute_reply": "2026-02-06T07:04:44.465267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rate by question:\n",
      "  q1: 100%\n",
      "  q2: 100%\n",
      "  q3: 50%\n"
     ]
    }
   ],
   "source": [
    "pass_rates_by_q = template_results.aggregate_pass_rate(by=\"question_id\")\n",
    "print(\"Pass rate by question:\")\n",
    "for qid, rate in pass_rates_by_q.items():\n",
    "    print(f\"  {qid}: {rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c021d",
   "metadata": {},
   "source": [
    "### Filtering Results\n",
    "\n",
    "`TemplateResults` supports filtering before DataFrame conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c981f7a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.466365Z",
     "iopub.status.busy": "2026-02-06T07:04:44.466302Z",
     "iopub.status.idle": "2026-02-06T07:04:44.468237Z",
     "shell.execute_reply": "2026-02-06T07:04:44.468052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed results: 1 (fields in DataFrame: 1)\n",
      "GPT-4o results: 3\n"
     ]
    }
   ],
   "source": [
    "# Filter to only failed results\n",
    "failed = template_results.filter(failed_only=True)\n",
    "df_failed = failed.to_dataframe()\n",
    "print(f\"Failed results: {len(failed)} (fields in DataFrame: {len(df_failed)})\")\n",
    "\n",
    "# Filter by model (use the full display string: \"interface:model_name\")\n",
    "gpt_results = template_results.filter(answering_models=[\"langchain:gpt-4o\"])\n",
    "print(f\"GPT-4o results: {len(gpt_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ca76f",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f4a748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.469161Z",
     "iopub.status.busy": "2026-02-06T07:04:44.469108Z",
     "iopub.status.idle": "2026-02-06T07:04:44.470709Z",
     "shell.execute_reply": "2026-02-06T07:04:44.470551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 6\n",
      "Passed: 5, Failed: 1\n",
      "Pass rate: 83%\n",
      "Unique questions: 3\n"
     ]
    }
   ],
   "source": [
    "summary = template_results.get_template_summary()\n",
    "print(f\"Total results: {summary['num_results']}\")\n",
    "print(f\"Passed: {summary['num_passed']}, Failed: {summary['num_failed']}\")\n",
    "print(f\"Pass rate: {summary['pass_rate']:.0%}\")\n",
    "print(f\"Unique questions: {summary['num_questions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d927d",
   "metadata": {},
   "source": [
    "## Rubric DataFrames\n",
    "\n",
    "`RubricResults` converts rubric evaluation scores to DataFrames, with one row\n",
    "per trait evaluated. It supports filtering by trait type.\n",
    "\n",
    "### Trait Type Filtering\n",
    "\n",
    "The `to_dataframe()` method accepts a `trait_type` parameter:\n",
    "\n",
    "| Value | Includes |\n",
    "|-------|----------|\n",
    "| `\"all\"` | All trait types (default) |\n",
    "| `\"llm\"` | All LLM traits (score + binary + literal) |\n",
    "| `\"llm_score\"` | LLM score traits only (1-5 scale) |\n",
    "| `\"llm_binary\"` | LLM binary traits only (True/False) |\n",
    "| `\"llm_literal\"` | LLM literal traits only (categorical) |\n",
    "| `\"regex\"` | Regex traits |\n",
    "| `\"callable\"` | Callable traits |\n",
    "| `\"metric\"` | Metric traits (exploded by metric name) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ecf7675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.471612Z",
     "iopub.status.busy": "2026-02-06T07:04:44.471562Z",
     "iopub.status.idle": "2026-02-06T07:04:44.474159Z",
     "shell.execute_reply": "2026-02-06T07:04:44.473950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rubric traits:\n",
      "question_id                           answering_model  trait_name trait_score trait_type\n",
      "         q1                          langchain:gpt-4o     clarity           4  llm_score\n",
      "         q1                          langchain:gpt-4o conciseness        True llm_binary\n",
      "         q1                          langchain:gpt-4o  no_hedging        True      regex\n",
      "         q2                          langchain:gpt-4o     clarity           5  llm_score\n",
      "         q2                          langchain:gpt-4o conciseness        True llm_binary\n",
      "         q3                          langchain:gpt-4o     clarity           3  llm_score\n",
      "         q3                          langchain:gpt-4o conciseness       False llm_binary\n",
      "         q1 claude_agent_sdk:claude-sonnet-4-20250514     clarity           5  llm_score\n",
      "         q1 claude_agent_sdk:claude-sonnet-4-20250514 conciseness        True llm_binary\n",
      "         q1 claude_agent_sdk:claude-sonnet-4-20250514  no_hedging        True      regex\n",
      "         q2 claude_agent_sdk:claude-sonnet-4-20250514     clarity           5  llm_score\n",
      "         q2 claude_agent_sdk:claude-sonnet-4-20250514 conciseness        True llm_binary\n",
      "         q3 claude_agent_sdk:claude-sonnet-4-20250514     clarity           4  llm_score\n",
      "         q3 claude_agent_sdk:claude-sonnet-4-20250514 conciseness        True llm_binary\n"
     ]
    }
   ],
   "source": [
    "rubric_results = results.get_rubrics_results()\n",
    "df_all = rubric_results.to_dataframe()\n",
    "\n",
    "print(\"All rubric traits:\")\n",
    "print(df_all[[\"question_id\", \"answering_model\", \"trait_name\", \"trait_score\", \"trait_type\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dead1c",
   "metadata": {},
   "source": [
    "### Filtering by Trait Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad7ef48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.475080Z",
     "iopub.status.busy": "2026-02-06T07:04:44.475030Z",
     "iopub.status.idle": "2026-02-06T07:04:44.477571Z",
     "shell.execute_reply": "2026-02-06T07:04:44.477374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM score traits: 6 rows\n",
      "question_id                           answering_model trait_name  trait_score\n",
      "         q1                          langchain:gpt-4o    clarity            4\n",
      "         q2                          langchain:gpt-4o    clarity            5\n",
      "         q3                          langchain:gpt-4o    clarity            3\n",
      "         q1 claude_agent_sdk:claude-sonnet-4-20250514    clarity            5\n",
      "         q2 claude_agent_sdk:claude-sonnet-4-20250514    clarity            5\n",
      "         q3 claude_agent_sdk:claude-sonnet-4-20250514    clarity            4\n"
     ]
    }
   ],
   "source": [
    "# Get only LLM score traits (numeric 1-5 scale)\n",
    "df_scores = rubric_results.to_dataframe(trait_type=\"llm_score\")\n",
    "print(f\"\\nLLM score traits: {len(df_scores)} rows\")\n",
    "if len(df_scores) > 0:\n",
    "    print(df_scores[[\"question_id\", \"answering_model\", \"trait_name\", \"trait_score\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc7a03",
   "metadata": {},
   "source": [
    "### Aggregating Trait Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1819764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.478494Z",
     "iopub.status.busy": "2026-02-06T07:04:44.478445Z",
     "iopub.status.idle": "2026-02-06T07:04:44.481024Z",
     "shell.execute_reply": "2026-02-06T07:04:44.480840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average LLM trait scores by model:\n",
      "  claude_agent_sdk:claude-sonnet-4-20250514:\n",
      "    clarity: 4.7\n",
      "    conciseness: 1.0\n",
      "  langchain:gpt-4o:\n",
      "    clarity: 4.0\n",
      "    conciseness: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Average LLM trait scores by model\n",
    "avg_by_model = rubric_results.aggregate_llm_traits(\n",
    "    strategy=\"mean\", by=\"answering_model\"\n",
    ")\n",
    "print(\"Average LLM trait scores by model:\")\n",
    "for model, traits in avg_by_model.items():\n",
    "    print(f\"  {model}:\")\n",
    "    for trait, score in traits.items():\n",
    "        print(f\"    {trait}: {score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc37b26",
   "metadata": {},
   "source": [
    "### Trait Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be47b6a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.481899Z",
     "iopub.status.busy": "2026-02-06T07:04:44.481844Z",
     "iopub.status.idle": "2026-02-06T07:04:44.483163Z",
     "shell.execute_reply": "2026-02-06T07:04:44.482997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with rubric data: 6\n",
      "LLM traits: ['clarity', 'conciseness']\n",
      "Regex traits: ['no_hedging']\n",
      "Callable traits: []\n"
     ]
    }
   ],
   "source": [
    "trait_summary = rubric_results.get_trait_summary()\n",
    "print(f\"Results with rubric data: {trait_summary['num_results']}\")\n",
    "print(f\"LLM traits: {trait_summary['llm_traits']}\")\n",
    "print(f\"Regex traits: {trait_summary['regex_traits']}\")\n",
    "print(f\"Callable traits: {trait_summary['callable_traits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9582c7f",
   "metadata": {},
   "source": [
    "## Deep Judgment DataFrames\n",
    "\n",
    "`JudgmentResults` handles deep judgment data, creating one row per\n",
    "(attribute x excerpt) pair. This is the most granular DataFrame — use it\n",
    "when deep judgment is enabled in your verification configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1de119d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.483979Z",
     "iopub.status.busy": "2026-02-06T07:04:44.483931Z",
     "iopub.status.idle": "2026-02-06T07:04:44.485403Z",
     "shell.execute_reply": "2026-02-06T07:04:44.485226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with deep judgment: 0\n"
     ]
    }
   ],
   "source": [
    "# Access judgment results (empty if deep judgment was not enabled)\n",
    "judgment_results = results.get_judgment_results()\n",
    "print(f\"Results with deep judgment: {len(judgment_results.get_results_with_judgment())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe039a",
   "metadata": {},
   "source": [
    "When deep judgment is enabled, the DataFrame provides columns for excerpt text,\n",
    "confidence scores, similarity scores, hallucination risk, and reasoning traces.\n",
    "\n",
    "### Including Deep Judgment in Rubric DataFrames\n",
    "\n",
    "You can also include deep judgment columns in rubric DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e849a8bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.486274Z",
     "iopub.status.busy": "2026-02-06T07:04:44.486216Z",
     "iopub.status.idle": "2026-02-06T07:04:44.488124Z",
     "shell.execute_reply": "2026-02-06T07:04:44.487948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rubric DataFrame columns: 21\n"
     ]
    }
   ],
   "source": [
    "# Include trait reasoning and excerpts in rubric DataFrame\n",
    "rubric_with_dj = results.get_rubrics_results(include_deep_judgment=True)\n",
    "df = rubric_with_dj.to_dataframe()\n",
    "# When deep judgment is enabled, additional columns appear:\n",
    "# trait_reasoning, trait_excerpts, trait_hallucination_risk\n",
    "print(f\"Rubric DataFrame columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95bd25",
   "metadata": {},
   "source": [
    "## Common Analysis Patterns\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "Compare template pass rates and rubric scores across models using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec13ea11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.488949Z",
     "iopub.status.busy": "2026-02-06T07:04:44.488900Z",
     "iopub.status.idle": "2026-02-06T07:04:44.491257Z",
     "shell.execute_reply": "2026-02-06T07:04:44.491094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template pass rate by model:\n",
      "answering_model\n",
      "claude_agent_sdk:claude-sonnet-4-20250514    1.000000\n",
      "langchain:gpt-4o                             0.666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Template pass rates by model\n",
    "template_df = results.get_template_results().to_dataframe()\n",
    "model_pass = (\n",
    "    template_df.drop_duplicates(subset=[\"result_index\"])\n",
    "    .groupby(\"answering_model\")[\"verify_result\"]\n",
    "    .mean()\n",
    ")\n",
    "print(\"Template pass rate by model:\")\n",
    "print(model_pass.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2470514a",
   "metadata": {},
   "source": [
    "### Question Difficulty\n",
    "\n",
    "Identify which questions are hardest by looking at pass rates across all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01145504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.492126Z",
     "iopub.status.busy": "2026-02-06T07:04:44.492078Z",
     "iopub.status.idle": "2026-02-06T07:04:44.494539Z",
     "shell.execute_reply": "2026-02-06T07:04:44.494362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question difficulty (sorted by pass rate):\n",
      "             pass_rate  num_runs\n",
      "question_id                     \n",
      "q3                 0.5         2\n",
      "q1                 1.0         2\n",
      "q2                 1.0         2\n"
     ]
    }
   ],
   "source": [
    "question_pass = (\n",
    "    template_df.drop_duplicates(subset=[\"result_index\"])\n",
    "    .groupby(\"question_id\")[\"verify_result\"]\n",
    "    .agg([\"mean\", \"count\"])\n",
    "    .rename(columns={\"mean\": \"pass_rate\", \"count\": \"num_runs\"})\n",
    "    .sort_values(\"pass_rate\")\n",
    ")\n",
    "print(\"\\nQuestion difficulty (sorted by pass rate):\")\n",
    "print(question_pass.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fdaf71",
   "metadata": {},
   "source": [
    "### Exporting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e3bce3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:04:44.495362Z",
     "iopub.status.busy": "2026-02-06T07:04:44.495313Z",
     "iopub.status.idle": "2026-02-06T07:04:44.498350Z",
     "shell.execute_reply": "2026-02-06T07:04:44.498203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 6 rows to CSV\n"
     ]
    }
   ],
   "source": [
    "# Export template results to CSV\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False, mode=\"w\") as f:\n",
    "    template_df.to_csv(f.name, index=False)\n",
    "    print(f\"Exported {len(template_df)} rows to CSV\")\n",
    "    os.unlink(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a79223",
   "metadata": {},
   "source": [
    "## Result Access Methods Summary\n",
    "\n",
    "All three result types share a consistent interface:\n",
    "\n",
    "| Method | TemplateResults | RubricResults | JudgmentResults |\n",
    "|--------|:---:|:---:|:---:|\n",
    "| `to_dataframe()` | field-level | trait-level | attribute x excerpt |\n",
    "| `filter()` | by model, question, pass/fail | by model, question | by model, question, search |\n",
    "| `group_by_question()` | dict of TemplateResults | dict of RubricResults | dict of JudgmentResults |\n",
    "| `group_by_model()` | dict of TemplateResults | dict of RubricResults | dict of JudgmentResults |\n",
    "| `get_*_summary()` | template stats | trait inventory | judgment stats |\n",
    "\n",
    "The `VerificationResultSet` itself provides higher-level operations:\n",
    "\n",
    "- `filter()` — filter by question IDs, models, completion status, etc.\n",
    "- `group_by_question()` / `group_by_model()` / `group_by_replicate()` — group results\n",
    "- `get_summary()` — comprehensive statistics including pass rates, token usage, and tool usage\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [VerificationResult Structure](verification-result.md) — understand the complete result hierarchy\n",
    "- [Exporting Results](exporting.md) — save results to JSON, CSV, or files\n",
    "- [Iterating on Benchmarks](iterating.md) — use analysis to improve templates and rubrics\n",
    "- [Running Verification](../06-running-verification/python-api.md) — how to generate results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
