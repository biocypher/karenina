{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28acfb21",
   "metadata": {},
   "source": [
    "# Running Verification with the Python API\n",
    "\n",
    "The Python API is the most flexible way to run verification. It gives you full\n",
    "control over configuration, lets you filter which questions to verify, and\n",
    "provides rich result objects for analysis.\n",
    "\n",
    "This page walks through the complete workflow: load a benchmark, configure\n",
    "verification, run it, and work with results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83034eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.583849Z",
     "iopub.status.busy": "2026-02-06T01:02:44.583657Z",
     "iopub.status.idle": "2026-02-06T01:02:44.911929Z",
     "shell.execute_reply": "2026-02-06T01:02:44.911575Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(self)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mock cell: patches run_verification so examples execute without live API keys.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationConfig, VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "# Change to notebooks directory so test_checkpoint.jsonld is found\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "\n",
    "def _mock_run_verification(self, config, question_ids=None, **kwargs):\n",
    "    \"\"\"Return realistic mock results for documentation examples.\"\"\"\n",
    "    qids = question_ids or self.get_question_ids()\n",
    "    mock_results = []\n",
    "    answers = {\n",
    "        \"capital of France\": (\"Paris\", True),\n",
    "        \"6 multiplied by 7\": (\"42\", True),\n",
    "        \"atomic number 8\": (\"Oxygen (O)\", True),\n",
    "        \"17 a prime\": (\"True\", True),\n",
    "        \"machine learning\": (\"Machine learning is a subset of AI\", None),\n",
    "    }\n",
    "    for qid in qids:\n",
    "        q = self.get_question(qid)\n",
    "        question_text = q[\"question\"]\n",
    "        response, verified = (\"Mock response\", True)\n",
    "        for key, (resp, ver) in answers.items():\n",
    "            if key in question_text.lower():\n",
    "                response, verified = resp, ver\n",
    "                break\n",
    "        answering = ModelIdentity(model_name=\"gpt-4o\", interface=\"langchain\")\n",
    "        parsing = ModelIdentity(model_name=\"gpt-4o\", interface=\"langchain\")\n",
    "        ts = datetime.datetime.now(tz=datetime.UTC).isoformat()\n",
    "        result_id = VerificationResultMetadata.compute_result_id(qid, answering, parsing, ts)\n",
    "        template_result = None\n",
    "        if verified is not None:\n",
    "            template_result = VerificationResultTemplate(\n",
    "                raw_llm_response=response,\n",
    "                verify_result=verified,\n",
    "                template_verification_performed=True,\n",
    "            )\n",
    "        rubric_result = None\n",
    "        if \"capital\" in question_text.lower():\n",
    "            rubric_result = VerificationResultRubric(\n",
    "                rubric_evaluation_performed=True,\n",
    "                llm_trait_scores={\"Is the response concise?\": True},\n",
    "            )\n",
    "        result = VerificationResult(\n",
    "            metadata=VerificationResultMetadata(\n",
    "                question_id=qid,\n",
    "                template_id=\"mock_template\" if verified is not None else \"no_template\",\n",
    "                completed_without_errors=True,\n",
    "                question_text=question_text,\n",
    "                raw_answer=q.get(\"raw_answer\"),\n",
    "                answering=answering,\n",
    "                parsing=parsing,\n",
    "                execution_time=1.2,\n",
    "                timestamp=ts,\n",
    "                result_id=result_id,\n",
    "            ),\n",
    "            template=template_result,\n",
    "            rubric=rubric_result,\n",
    "        )\n",
    "        mock_results.append(result)\n",
    "    return VerificationResultSet(results=mock_results)\n",
    "\n",
    "\n",
    "_patcher_run = patch(\n",
    "    \"karenina.benchmark.benchmark.Benchmark.run_verification\",\n",
    "    _mock_run_verification,\n",
    ")\n",
    "_patcher_validate = patch.object(VerificationConfig, \"_validate_config\", lambda self: None)\n",
    "_patcher_run.start()\n",
    "_patcher_validate.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3f752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Example\n",
    "\n",
    "Here is a minimal end-to-end verification in four lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cad65f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.913189Z",
     "iopub.status.busy": "2026-02-06T01:02:44.913082Z",
     "iopub.status.idle": "2026-02-06T01:02:44.915727Z",
     "shell.execute_reply": "2026-02-06T01:02:44.915527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified 5 results across 5 questions\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas.config import ModelConfig\n",
    "from karenina.schemas.verification import VerificationConfig\n",
    "\n",
    "# 1. Load benchmark\n",
    "benchmark = Benchmark.load(\"test_checkpoint.jsonld\")\n",
    "\n",
    "# 2. Configure\n",
    "config = VerificationConfig(\n",
    "    answering_models=[ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\")],\n",
    "    parsing_models=[ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\")],\n",
    ")\n",
    "\n",
    "# 3. Run\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "# 4. Inspect\n",
    "print(f\"Verified {len(results)} results across {benchmark.question_count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fee06",
   "metadata": {},
   "source": [
    "The rest of this page explains each step in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d4c104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.916725Z",
     "iopub.status.busy": "2026-02-06T01:02:44.916651Z",
     "iopub.status.idle": "2026-02-06T01:02:44.918582Z",
     "shell.execute_reply": "2026-02-06T01:02:44.918403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: Documentation Test Benchmark\n",
      "Questions: 5\n",
      "Complete:  False\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.load(\"test_checkpoint.jsonld\")\n",
    "print(f\"Benchmark: {benchmark.name}\")\n",
    "print(f\"Questions: {benchmark.question_count}\")\n",
    "print(f\"Complete:  {benchmark.is_complete}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd0700",
   "metadata": {},
   "source": [
    "`Benchmark.load()` reads a JSON-LD checkpoint file and returns a `Benchmark`\n",
    "object. See [Loading a Benchmark](loading-benchmark.md) for details on\n",
    "inspecting questions, templates, and rubrics before running verification.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Configure Verification\n",
    "\n",
    "Configuration controls which models to use, what evaluation mode to apply,\n",
    "and which optional features to enable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f821f9cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.919503Z",
     "iopub.status.busy": "2026-02-06T01:02:44.919448Z",
     "iopub.status.idle": "2026-02-06T01:02:44.921252Z",
     "shell.execute_reply": "2026-02-06T01:02:44.921065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VerificationConfig(\n",
      "  === MODELS ===\n",
      "  Answering (1):\n",
      "    - gpt-4o (none) [temp=0.1, interface=langchain]\n",
      "  Parsing (1):\n",
      "    - gpt-4o (none) [temp=0.1, interface=langchain]\n",
      "\n",
      "  === EXECUTION ===\n",
      "  Replicates: 1\n",
      "  Async: True\n",
      "    └─ workers: 2\n",
      "  Evaluation Mode: template_and_rubric\n",
      "  Rubric Evaluation Strategy: batch\n",
      "\n",
      "  === FEATURES ===\n",
      "  Rubric: enabled\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas.config import ModelConfig\n",
    "from karenina.schemas.verification import VerificationConfig\n",
    "\n",
    "config = VerificationConfig(\n",
    "    # Models\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\"),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\"),\n",
    "    ],\n",
    "    # Evaluation mode\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    rubric_enabled=True,\n",
    "    # Optional features\n",
    "    abstention_check_enabled=True,\n",
    "    embedding_check_enabled=False,\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42684a85",
   "metadata": {},
   "source": [
    "See [VerificationConfig](verification-config.md) for a full tutorial on all\n",
    "configuration options, including deep judgment, async execution, and MCP\n",
    "settings.\n",
    "\n",
    "### Quick Configuration with `from_overrides`\n",
    "\n",
    "For simple setups, `from_overrides()` creates a config with sensible defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a069d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.922118Z",
     "iopub.status.busy": "2026-02-06T01:02:44.922070Z",
     "iopub.status.idle": "2026-02-06T01:02:44.923607Z",
     "shell.execute_reply": "2026-02-06T01:02:44.923404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation mode: template_only\n"
     ]
    }
   ],
   "source": [
    "config = VerificationConfig.from_overrides(\n",
    "    answering_id=\"gpt-4o\",\n",
    "    answering_model=\"gpt-4o\",\n",
    "    parsing_id=\"gpt-4o\",\n",
    "    parsing_model=\"gpt-4o\",\n",
    ")\n",
    "print(f\"Evaluation mode: {config.evaluation_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac31878",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Run Verification\n",
    "\n",
    "### Basic Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88064634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.924500Z",
     "iopub.status.busy": "2026-02-06T01:02:44.924448Z",
     "iopub.status.idle": "2026-02-06T01:02:44.925950Z",
     "shell.execute_reply": "2026-02-06T01:02:44.925759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 5 verifications\n"
     ]
    }
   ],
   "source": [
    "results = benchmark.run_verification(config)\n",
    "print(f\"Completed: {len(results)} verifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb793ad",
   "metadata": {},
   "source": [
    "### Verifying Specific Questions\n",
    "\n",
    "Pass `question_ids` to verify only a subset of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d39f4de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.926792Z",
     "iopub.status.busy": "2026-02-06T01:02:44.926728Z",
     "iopub.status.idle": "2026-02-06T01:02:44.928266Z",
     "shell.execute_reply": "2026-02-06T01:02:44.928088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified 2 of 5 questions\n"
     ]
    }
   ],
   "source": [
    "# Verify only the first two questions\n",
    "question_ids = benchmark.get_question_ids()[:2]\n",
    "partial_results = benchmark.run_verification(config, question_ids=question_ids)\n",
    "print(f\"Verified {len(partial_results)} of {benchmark.question_count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe14bb8",
   "metadata": {},
   "source": [
    "### Method Signature\n",
    "\n",
    "`run_verification()` accepts the following parameters:\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `config` | `VerificationConfig` | *(required)* | Full verification configuration |\n",
    "| `question_ids` | `list[str] \\| None` | `None` | Specific questions to verify (all if `None`) |\n",
    "| `run_name` | `str \\| None` | `None` | Optional name for this run |\n",
    "| `async_enabled` | `bool \\| None` | `None` | Override async execution (uses config default) |\n",
    "| `progress_callback` | `Callable[[float, str], None] \\| None` | `None` | Progress callback `(percentage, message)` |\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Inspect Results\n",
    "\n",
    "`run_verification()` returns a `VerificationResultSet` — a container with\n",
    "filtering, grouping, and analysis methods.\n",
    "\n",
    "### Iterating Over Results\n",
    "\n",
    "Each result is a `VerificationResult` with nested sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570944d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.929097Z",
     "iopub.status.busy": "2026-02-06T01:02:44.929051Z",
     "iopub.status.idle": "2026-02-06T01:02:44.930740Z",
     "shell.execute_reply": "2026-02-06T01:02:44.930571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [PASS] What is the capital of France? | rubric traits: 1\n",
      "  [PASS] What is 6 multiplied by 7?\n",
      "  [PASS] What element has the atomic number 8? Provide both\n",
      "  [PASS] Is 17 a prime number?\n",
      "  [N/A] Explain the concept of machine learning in simple \n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    meta = result.metadata\n",
    "    q_text = meta.question_text[:50]\n",
    "\n",
    "    # Template result (correctness)\n",
    "    if result.template and result.template.verify_result is not None:\n",
    "        status = \"PASS\" if result.template.verify_result else \"FAIL\"\n",
    "    else:\n",
    "        status = \"N/A\"\n",
    "\n",
    "    # Rubric result (quality)\n",
    "    rubric_info = \"\"\n",
    "    if result.rubric and result.rubric.rubric_evaluation_performed:\n",
    "        scores = result.rubric.llm_trait_scores or {}\n",
    "        rubric_info = f\" | rubric traits: {len(scores)}\"\n",
    "\n",
    "    print(f\"  [{status}] {q_text}{rubric_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50154c6a",
   "metadata": {},
   "source": [
    "### Result Structure\n",
    "\n",
    "Each `VerificationResult` contains up to four nested sections:\n",
    "\n",
    "| Section | Field | Contains |\n",
    "|---------|-------|----------|\n",
    "| **Metadata** | `result.metadata` | Question ID, models, timing, completion status |\n",
    "| **Template** | `result.template` | Pass/fail, raw LLM response, embedding similarity |\n",
    "| **Rubric** | `result.rubric` | LLM trait scores, regex scores, callable scores |\n",
    "| **Deep Judgment** | `result.deep_judgment` | Extracted excerpts, reasoning, hallucination risk |\n",
    "\n",
    "Access fields through the nested structure:\n",
    "\n",
    "    result.metadata.question_id      # Which question\n",
    "    result.metadata.answering_model  # Which model answered\n",
    "    result.metadata.execution_time   # How long it took\n",
    "    result.template.verify_result    # True/False/None\n",
    "    result.rubric.llm_trait_scores   # {\"trait_name\": True/False or int}\n",
    "\n",
    "See [VerificationResult Structure](../07-analyzing-results/verification-result.md)\n",
    "for complete field documentation.\n",
    "\n",
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412bc708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.931590Z",
     "iopub.status.busy": "2026-02-06T01:02:44.931543Z",
     "iopub.status.idle": "2026-02-06T01:02:44.933110Z",
     "shell.execute_reply": "2026-02-06T01:02:44.932945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  5\n",
      "Completed:      5\n",
      "With template:  4\n",
      "With rubric:    1\n",
      "Unique models:  1\n"
     ]
    }
   ],
   "source": [
    "summary = results.get_summary()\n",
    "print(f\"Total results:  {summary['num_results']}\")\n",
    "print(f\"Completed:      {summary['num_completed']}\")\n",
    "print(f\"With template:  {summary['num_with_template']}\")\n",
    "print(f\"With rubric:    {summary['num_with_rubric']}\")\n",
    "print(f\"Unique models:  {summary['num_models']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da1050",
   "metadata": {},
   "source": [
    "### Filtering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4682fe8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.934008Z",
     "iopub.status.busy": "2026-02-06T01:02:44.933948Z",
     "iopub.status.idle": "2026-02-06T01:02:44.935268Z",
     "shell.execute_reply": "2026-02-06T01:02:44.935082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 4 results with template verification\n"
     ]
    }
   ],
   "source": [
    "# Filter to only completed results that have template verification\n",
    "filtered = results.filter(completed_only=True, has_template=True)\n",
    "print(f\"Filtered: {len(filtered)} results with template verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f926a",
   "metadata": {},
   "source": [
    "### Grouping Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60f4d217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.936088Z",
     "iopub.status.busy": "2026-02-06T01:02:44.936037Z",
     "iopub.status.idle": "2026-02-06T01:02:44.937581Z",
     "shell.execute_reply": "2026-02-06T01:02:44.937421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  What is the capital of France?: 1 result(s)\n",
      "  What is 6 multiplied by 7?: 1 result(s)\n",
      "  What element has the atomic number 8? Pr: 1 result(s)\n",
      "  Is 17 a prime number?: 1 result(s)\n",
      "  Explain the concept of machine learning : 1 result(s)\n"
     ]
    }
   ],
   "source": [
    "# Group by question to see per-question outcomes\n",
    "by_question = results.group_by_question()\n",
    "for qid, group in by_question.items():\n",
    "    first = group.results[0]\n",
    "    q_text = first.metadata.question_text[:40]\n",
    "    print(f\"  {q_text}: {len(group)} result(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab657190",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Error Handling\n",
    "\n",
    "Karenina uses a structured exception hierarchy rooted at `KareninaError`.\n",
    "Errors are caught per-question during verification — a single question\n",
    "failure does not abort the entire run.\n",
    "\n",
    "### Checking for Errors\n",
    "\n",
    "Results that encountered errors have `completed_without_errors=False`:\n",
    "\n",
    "    for result in results:\n",
    "        if not result.metadata.completed_without_errors:\n",
    "            print(f\"Error on: {result.metadata.question_text[:50]}\")\n",
    "\n",
    "### Exception Hierarchy\n",
    "\n",
    "When running verification programmatically, you can catch specific error types:\n",
    "\n",
    "    from karenina.exceptions import KareninaError\n",
    "    from karenina.ports import PortError, ParseError, AdapterUnavailableError\n",
    "\n",
    "    try:\n",
    "        results = benchmark.run_verification(config)\n",
    "    except KareninaError as e:\n",
    "        print(f\"Verification failed: {e}\")\n",
    "\n",
    "Key exception types:\n",
    "\n",
    "| Exception | When It Occurs |\n",
    "|-----------|---------------|\n",
    "| `KareninaError` | Base for all karenina errors |\n",
    "| `PortError` | Adapter/port layer failure |\n",
    "| `AdapterUnavailableError` | Requested backend not available |\n",
    "| `ParseError` | Judge LLM couldn't parse response into template |\n",
    "| `AgentExecutionError` | Agent runtime failure |\n",
    "| `AgentTimeoutError` | Agent hit turn/time limit |\n",
    "| `McpError` | MCP server communication failure |\n",
    "\n",
    "Most errors during verification are caught internally and recorded in the\n",
    "result metadata. Exceptions that escape to your code typically indicate\n",
    "configuration problems (wrong model name, missing API key) rather than\n",
    "per-question failures.\n",
    "\n",
    "---\n",
    "\n",
    "## Async Execution\n",
    "\n",
    "For large benchmarks, enable async execution to verify multiple questions\n",
    "in parallel:\n",
    "\n",
    "    config = VerificationConfig(\n",
    "        answering_models=[ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\")],\n",
    "        parsing_models=[ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\")],\n",
    "        async_enabled=True,\n",
    "        async_max_workers=4,\n",
    "    )\n",
    "    results = benchmark.run_verification(config)\n",
    "\n",
    "You can also override the async setting per-call:\n",
    "\n",
    "    # Force async even if config says sync\n",
    "    results = benchmark.run_verification(config, async_enabled=True)\n",
    "\n",
    "Async execution is controlled by two settings:\n",
    "\n",
    "| Setting | Default | Description |\n",
    "|---------|---------|-------------|\n",
    "| `async_enabled` | `False` | Enable parallel question verification |\n",
    "| `async_max_workers` | `4` | Maximum concurrent verifications |\n",
    "\n",
    "Both can also be set via environment variables (`KARENINA_ASYNC_ENABLED`,\n",
    "`KARENINA_ASYNC_MAX_WORKERS`).\n",
    "\n",
    "---\n",
    "\n",
    "## Progress Tracking\n",
    "\n",
    "For long-running verifications, pass a callback to monitor progress:\n",
    "\n",
    "    def on_progress(percentage: float, message: str):\n",
    "        print(f\"[{percentage:.0f}%] {message}\")\n",
    "\n",
    "    results = benchmark.run_verification(config, progress_callback=on_progress)\n",
    "\n",
    "The callback receives a percentage (0.0–100.0) and a human-readable status\n",
    "message at each step.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79684d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:02:44.938395Z",
     "iopub.status.busy": "2026-02-06T01:02:44.938347Z",
     "iopub.status.idle": "2026-02-06T01:02:44.939618Z",
     "shell.execute_reply": "2026-02-06T01:02:44.939459Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up the mocks\n",
    "_ = _patcher_run.stop()\n",
    "_ = _patcher_validate.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765bcb6",
   "metadata": {},
   "source": [
    "- [VerificationConfig](verification-config.md) — Full configuration tutorial\n",
    "- [Analyzing Results](../07-analyzing-results/index.md) — Filtering, grouping, and DataFrame analysis\n",
    "- [VerificationResult Structure](../07-analyzing-results/verification-result.md) — Complete field reference\n",
    "- [CLI Verification](cli.md) — Running verification from the command line\n",
    "- [Multi-Model Evaluation](multi-model.md) — Comparing multiple LLMs on the same benchmark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
