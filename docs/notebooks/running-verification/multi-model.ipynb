{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e221e1",
   "metadata": {},
   "source": [
    "# Multi-Model Evaluation\n",
    "\n",
    "Karenina supports running the same benchmark across multiple answering and parsing\n",
    "models in a single call. This lets you compare model performance, measure\n",
    "inter-judge agreement, and study result variance — all while using **answer\n",
    "caching** to avoid redundant LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b20b0167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.337711Z",
     "iopub.status.busy": "2026-02-06T07:22:31.337644Z",
     "iopub.status.idle": "2026-02-06T07:22:31.681434Z",
     "shell.execute_reply": "2026-02-06T07:22:31.681206Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: patches run_verification so examples execute without live API keys.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationConfig, VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "\n",
    "def _mock_run_verification(self, config, question_ids=None, **kwargs):\n",
    "    \"\"\"Return realistic mock results for multi-model documentation.\"\"\"\n",
    "    qids = question_ids or self.get_question_ids()\n",
    "    mock_results = []\n",
    "\n",
    "    # Model performance profiles — different models have different strengths\n",
    "    model_profiles = {\n",
    "        \"gpt-4o\": {\"capital of France\": True, \"6 multiplied by 7\": True, \"atomic number 8\": True, \"prime number\": True},\n",
    "        \"claude-sonnet-4-5-20250514\": {\n",
    "            \"capital of France\": True,\n",
    "            \"6 multiplied by 7\": True,\n",
    "            \"atomic number 8\": True,\n",
    "            \"prime number\": True,\n",
    "        },\n",
    "        \"gemini-2.0-flash\": {\n",
    "            \"capital of France\": True,\n",
    "            \"6 multiplied by 7\": True,\n",
    "            \"atomic number 8\": False,\n",
    "            \"prime number\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for qid in qids:\n",
    "        q = self.get_question(qid)\n",
    "        q_text = q.get(\"question\", \"\")\n",
    "        has_template = self.has_template(qid)\n",
    "\n",
    "        for ans_model in config.answering_models:\n",
    "            model_name = ans_model.model_name\n",
    "            profile = model_profiles.get(model_name, {})\n",
    "\n",
    "            for parse_model in config.parsing_models:\n",
    "                # Determine pass/fail from the profile\n",
    "                passed = any(key in q_text and profile.get(key, False) for key in profile)\n",
    "                if not has_template:\n",
    "                    passed = None\n",
    "\n",
    "                answering = ModelIdentity(\n",
    "                    interface=ans_model.interface,\n",
    "                    model_name=ans_model.model_name,\n",
    "                )\n",
    "                parsing = ModelIdentity(\n",
    "                    interface=parse_model.interface,\n",
    "                    model_name=parse_model.model_name,\n",
    "                )\n",
    "\n",
    "                template_result = None\n",
    "                if has_template:\n",
    "                    template_result = VerificationResultTemplate(\n",
    "                        raw_llm_response=f\"Mock answer from {model_name}\",\n",
    "                        verify_result=passed,\n",
    "                        template_verification_performed=True,\n",
    "                    )\n",
    "\n",
    "                metadata = VerificationResultMetadata(\n",
    "                    question_id=qid,\n",
    "                    template_id=self.get_template(qid)[:10] + \"...\" if has_template else \"no_template\",\n",
    "                    completed_without_errors=True,\n",
    "                    question_text=q_text,\n",
    "                    answering=answering,\n",
    "                    parsing=parsing,\n",
    "                    execution_time=1.2,\n",
    "                    timestamp=datetime.datetime.now(datetime.UTC).isoformat(),\n",
    "                    result_id=f\"mock-{qid[:8]}-{model_name[:4]}-{parse_model.model_name[:4]}\",\n",
    "                    run_name=kwargs.get(\"run_name\"),\n",
    "                )\n",
    "\n",
    "                mock_results.append(VerificationResult(metadata=metadata, template=template_result))\n",
    "\n",
    "    return VerificationResultSet(results=mock_results)\n",
    "\n",
    "\n",
    "_patcher1 = patch(\n",
    "    \"karenina.benchmark.benchmark.Benchmark.run_verification\",\n",
    "    _mock_run_verification,\n",
    ")\n",
    "_patcher2 = patch(\n",
    "    \"karenina.schemas.verification.config.VerificationConfig._validate_config\",\n",
    "    lambda self: None,\n",
    ")\n",
    "_ = _patcher1.start()\n",
    "_ = _patcher2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f0feb",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "When you provide multiple answering or parsing models, Karenina creates a\n",
    "**combinatorial task queue**:\n",
    "\n",
    "```\n",
    "Questions × Answering Models × Parsing Models × Replicates = Total Tasks\n",
    "```\n",
    "\n",
    "For example, 5 questions × 2 answering models × 1 parsing model × 1 replicate\n",
    "= 10 verification tasks. Each task runs through the full pipeline independently,\n",
    "and all results are returned in a single `VerificationResultSet`.\n",
    "\n",
    "## Configuring Multiple Models\n",
    "\n",
    "Provide lists for `answering_models` and `parsing_models`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983cbb4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.682825Z",
     "iopub.status.busy": "2026-02-06T07:22:31.682726Z",
     "iopub.status.idle": "2026-02-06T07:22:31.685552Z",
     "shell.execute_reply": "2026-02-06T07:22:31.685337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 15\n",
      "Questions: 5\n",
      "Answering models: 3\n"
     ]
    }
   ],
   "source": [
    "from karenina.benchmark import Benchmark\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "benchmark = Benchmark.load(\"test_checkpoint.jsonld\")\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt4o\",\n",
    "            model_name=\"gpt-4o\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"claude-sonnet\",\n",
    "            model_name=\"claude-sonnet-4-5-20250514\",\n",
    "            model_provider=\"anthropic\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"gemini-flash\",\n",
    "            model_name=\"gemini-2.0-flash\",\n",
    "            model_provider=\"google_genai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Total results: {len(results.results)}\")\n",
    "print(f\"Questions: {len(set(r.metadata.question_id for r in results.results))}\")\n",
    "print(f\"Answering models: {len(config.answering_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629029b2",
   "metadata": {},
   "source": [
    "This runs each of the 5 questions against all 3 answering models, parsed by 1\n",
    "judge — 15 total verification tasks.\n",
    "\n",
    "## Comparing Models\n",
    "\n",
    "### Group by Answering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899d6090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.686525Z",
     "iopub.status.busy": "2026-02-06T07:22:31.686463Z",
     "iopub.status.idle": "2026-02-06T07:22:31.688159Z",
     "shell.execute_reply": "2026-02-06T07:22:31.687972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain:gpt-4o: 4/4 passed (100%)\n",
      "langchain:claude-sonnet-4-5-20250514: 4/4 passed (100%)\n",
      "langchain:gemini-2.0-flash: 3/4 passed (75%)\n"
     ]
    }
   ],
   "source": [
    "by_model = results.group_by_model(by=\"answering\")\n",
    "\n",
    "for model_name, model_results in by_model.items():\n",
    "    summary = model_results.get_summary()\n",
    "    pass_info = summary.get(\"template_pass_overall\", {})\n",
    "    passed = pass_info.get(\"passed\", 0)\n",
    "    total = pass_info.get(\"total\", 0)\n",
    "    pct = pass_info.get(\"pass_pct\", 0)\n",
    "    print(f\"{model_name}: {passed}/{total} passed ({pct:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9d55f",
   "metadata": {},
   "source": [
    "### Filter to a Specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a39a37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.689075Z",
     "iopub.status.busy": "2026-02-06T07:22:31.689019Z",
     "iopub.status.idle": "2026-02-06T07:22:31.690498Z",
     "shell.execute_reply": "2026-02-06T07:22:31.690316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o results: 5\n"
     ]
    }
   ],
   "source": [
    "gpt4_results = results.filter(answering_models=[\"langchain:gpt-4o\"])\n",
    "print(f\"GPT-4o results: {len(gpt4_results.results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925d81a",
   "metadata": {},
   "source": [
    "!!! note\n",
    "    Model filter values use the `interface:model_name` format (e.g.,\n",
    "    `\"langchain:gpt-4o\"`). This is the display string from `ModelIdentity`.\n",
    "\n",
    "### Group by Question\n",
    "\n",
    "Compare how each model performed on the same question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a9b583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.691473Z",
     "iopub.status.busy": "2026-02-06T07:22:31.691403Z",
     "iopub.status.idle": "2026-02-06T07:22:31.693031Z",
     "shell.execute_reply": "2026-02-06T07:22:31.692831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: urn:uuid:question-what-is-the-...\n",
      "  gpt-4o: True\n",
      "  claude-sonnet-4-5-20250514: True\n",
      "  gemini-2.0-flash: True\n",
      "\n",
      "Question: urn:uuid:question-what-is-6-mu...\n",
      "  gpt-4o: True\n",
      "  claude-sonnet-4-5-20250514: True\n",
      "  gemini-2.0-flash: True\n"
     ]
    }
   ],
   "source": [
    "by_question = results.group_by_question()\n",
    "\n",
    "for qid, q_results in list(by_question.items())[:2]:\n",
    "    print(f\"\\nQuestion: {qid[:30]}...\")\n",
    "    for r in q_results.results:\n",
    "        model = r.metadata.answering.model_name\n",
    "        passed = r.template.verify_result if r.template else \"N/A\"\n",
    "        print(f\"  {model}: {passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00ab21",
   "metadata": {},
   "source": [
    "## Result Summary\n",
    "\n",
    "The `get_summary()` method provides a comprehensive breakdown including token\n",
    "usage and pass rates by model combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6e187e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.693964Z",
     "iopub.status.busy": "2026-02-06T07:22:31.693910Z",
     "iopub.status.idle": "2026-02-06T07:22:31.695530Z",
     "shell.execute_reply": "2026-02-06T07:22:31.695324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 15\n",
      "Completed: 15\n",
      "Models: 3\n",
      "Overall pass rate: 92%\n"
     ]
    }
   ],
   "source": [
    "summary = results.get_summary()\n",
    "print(f\"Total results: {summary['num_results']}\")\n",
    "print(f\"Completed: {summary['num_completed']}\")\n",
    "print(f\"Models: {summary['num_models']}\")\n",
    "print(f\"Overall pass rate: {summary.get('template_pass_overall', {}).get('pass_pct', 0):.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f3d5d",
   "metadata": {},
   "source": [
    "The summary also includes `template_pass_by_combo` which breaks down pass rates\n",
    "per model combination — useful for identifying which answering/parsing pair works\n",
    "best for your benchmark.\n",
    "\n",
    "## Answer Caching\n",
    "\n",
    "When you use multiple **parsing models** with the same answering model, Karenina\n",
    "automatically caches the answering model's response. This avoids generating the\n",
    "same answer multiple times.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Consider this configuration:\n",
    "\n",
    "    1 answering model (GPT-4o) × 3 parsing models × 5 questions = 15 tasks\n",
    "\n",
    "Without caching, the answering model would be called 15 times. With caching, it\n",
    "is called only **5 times** — once per question. The 10 remaining tasks reuse the\n",
    "cached answer and only call the parsing model.\n",
    "\n",
    "The cache key is: `{question_id}_{answering_model_id}_{replicate}`\n",
    "\n",
    "When a task finds a cached answer:\n",
    "1. It skips the answer generation stage entirely\n",
    "2. It uses the cached response (including token usage and MCP metrics)\n",
    "3. It runs only the parsing and evaluation stages\n",
    "\n",
    "### Cost Savings\n",
    "\n",
    "Answer caching is most impactful when:\n",
    "\n",
    "| Scenario | Without Cache | With Cache | Savings |\n",
    "|----------|--------------|------------|---------|\n",
    "| 1 answering × 3 judges | 15 answer calls | 5 answer calls | 67% fewer answering calls |\n",
    "| 2 answering × 2 judges | 20 answer calls | 10 answer calls | 50% fewer answering calls |\n",
    "| 1 answering × 1 judge | 5 answer calls | 5 answer calls | 0% (no sharing) |\n",
    "\n",
    "Caching is **automatic** — you do not need to configure it. It activates\n",
    "whenever the same answering model + question + replicate combination appears\n",
    "in multiple tasks (i.e., when you have multiple parsing models).\n",
    "\n",
    "### Cache Key and Scope\n",
    "\n",
    "- Answers are cached **per verification run** (not across runs)\n",
    "- The cache is **thread-safe** for parallel execution\n",
    "- If answer generation fails, the cache allows retry by subsequent tasks\n",
    "- Cache statistics are available via the executor but not exposed in results\n",
    "\n",
    "## Using `from_overrides` for Model Comparison\n",
    "\n",
    "A common pattern is to run multiple verification passes from the same base\n",
    "configuration, overriding just the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e417662f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.696403Z",
     "iopub.status.busy": "2026-02-06T07:22:31.696348Z",
     "iopub.status.idle": "2026-02-06T07:22:31.698335Z",
     "shell.execute_reply": "2026-02-06T07:22:31.698121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o: 4/4 passed\n",
      "claude-sonnet-4-5-20250514: 4/4 passed\n",
      "gemini-2.0-flash: 3/4 passed\n"
     ]
    }
   ],
   "source": [
    "models_to_compare = [\n",
    "    (\"gpt-4o\", \"openai\"),\n",
    "    (\"claude-sonnet-4-5-20250514\", \"anthropic\"),\n",
    "    (\"gemini-2.0-flash\", \"google_genai\"),\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "for model_name, provider in models_to_compare:\n",
    "    config = VerificationConfig.from_overrides(\n",
    "        answering_model=model_name,\n",
    "        answering_provider=provider,\n",
    "        answering_id=f\"ans-{model_name}\",\n",
    "        parsing_model=\"gpt-4o-mini\",\n",
    "        parsing_provider=\"openai\",\n",
    "        parsing_id=\"parser\",\n",
    "    )\n",
    "    run_results = benchmark.run_verification(config, run_name=model_name)\n",
    "    all_results[model_name] = run_results\n",
    "\n",
    "for model_name, model_results in all_results.items():\n",
    "    summary = model_results.get_summary()\n",
    "    pass_info = summary.get(\"template_pass_overall\", {})\n",
    "    print(f\"{model_name}: {pass_info.get('passed', 0)}/{pass_info.get('total', 0)} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d63cf",
   "metadata": {},
   "source": [
    "This approach gives each model a separate `VerificationResultSet`, which can be\n",
    "useful when you want to export or analyze results independently. The `run_name`\n",
    "parameter tags each run for identification.\n",
    "\n",
    "!!! tip\n",
    "    The multi-model list approach (multiple `answering_models`) is simpler and\n",
    "    enables answer caching. The `from_overrides` loop approach gives you separate\n",
    "    result sets per model. Choose based on your analysis needs.\n",
    "\n",
    "## Replicates\n",
    "\n",
    "Run each model combination multiple times to measure variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c37b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.699282Z",
     "iopub.status.busy": "2026-02-06T07:22:31.699222Z",
     "iopub.status.idle": "2026-02-06T07:22:31.700871Z",
     "shell.execute_reply": "2026-02-06T07:22:31.700676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 5 (5 questions × 3 replicates)\n"
     ]
    }
   ],
   "source": [
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt4o\",\n",
    "            model_name=\"gpt-4o\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    replicate_count=3,\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Total results: {len(results.results)} (5 questions × 3 replicates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7828f02",
   "metadata": {},
   "source": [
    "Group by replicate to analyze variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f37d58f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.701814Z",
     "iopub.status.busy": "2026-02-06T07:22:31.701749Z",
     "iopub.status.idle": "2026-02-06T07:22:31.703137Z",
     "shell.execute_reply": "2026-02-06T07:22:31.702953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicate 0: 4/4 passed\n"
     ]
    }
   ],
   "source": [
    "by_replicate = results.group_by_replicate()\n",
    "for rep_num, rep_results in sorted(by_replicate.items()):\n",
    "    summary = rep_results.get_summary()\n",
    "    pass_info = summary.get(\"template_pass_overall\", {})\n",
    "    print(f\"Replicate {rep_num}: {pass_info.get('passed', 0)}/{pass_info.get('total', 0)} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb019a6",
   "metadata": {},
   "source": [
    "## Async Execution\n",
    "\n",
    "Multi-model runs benefit from parallel execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c71d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.704005Z",
     "iopub.status.busy": "2026-02-06T07:22:31.703954Z",
     "iopub.status.idle": "2026-02-06T07:22:31.705411Z",
     "shell.execute_reply": "2026-02-06T07:22:31.705210Z"
    }
   },
   "outputs": [],
   "source": [
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt4o\",\n",
    "            model_name=\"gpt-4o\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"claude-sonnet\",\n",
    "            model_name=\"claude-sonnet-4-5-20250514\",\n",
    "            model_provider=\"anthropic\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    async_enabled=True,\n",
    "    async_max_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fdf86d",
   "metadata": {},
   "source": [
    "Parallel execution works seamlessly with answer caching — the cache uses\n",
    "thread-safe locking. When a task encounters an in-progress cache entry, it is\n",
    "requeued rather than blocked, keeping all workers busy.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Verification Result structure](../07-analyzing-results/verification-result.md) — full result hierarchy\n",
    "- [DataFrame analysis](../07-analyzing-results/dataframe-analysis.md) — convert results to pandas DataFrames\n",
    "- [Python API verification](python-api.md) — single-model workflow\n",
    "- [VerificationConfig reference](../10-configuration-reference/verification-config.md) — all configuration fields\n",
    "- [CLI verification](cli.md) — run multi-model from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9fb9358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:31.706297Z",
     "iopub.status.busy": "2026-02-06T07:22:31.706240Z",
     "iopub.status.idle": "2026-02-06T07:22:31.707487Z",
     "shell.execute_reply": "2026-02-06T07:22:31.707314Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Cleanup mocks\n",
    "_ = _patcher1.stop()\n",
    "_ = _patcher2.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
