{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fb7571",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "Get started with Karenina in minutes. This guide walks you through creating a benchmark, adding questions, writing answer templates, defining rubric traits, running verification, and inspecting results.\n",
    "\n",
    "By the end you will have a working benchmark that evaluates LLM responses for both **correctness** (via answer templates) and **quality** (via rubric traits).\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Python 3.11+**\n",
    "- **Karenina installed** (see [Installation](02-installation/index.md))\n",
    "- **API keys** for the LLM providers you plan to use:\n",
    "\n",
    "> ```bash\n",
    "> export OPENAI_API_KEY=\"sk-...\"\n",
    "> export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "> export GOOGLE_API_KEY=\"AI...\"\n",
    "> ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91217e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:54.639081Z",
     "iopub.status.busy": "2026-02-06T11:50:54.638997Z",
     "iopub.status.idle": "2026-02-06T11:50:54.983912Z",
     "shell.execute_reply": "2026-02-06T11:50:54.983631Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: patches run_verification so the quickstart executes without live API keys.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "import tempfile\n",
    "from unittest.mock import patch\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationConfig, VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "_MOCK_RESPONSES = {\n",
    "    \"chromosomes\": \"There are 46 chromosomes in a human somatic cell — 23 pairs in total.\",\n",
    "    \"venetoclax\": \"Venetoclax targets BCL2 (B-cell lymphoma 2), an anti-apoptotic protein in the BH3 family.\",\n",
    "    \"hemoglobin\": \"Hemoglobin A has 4 protein subunits: two alpha and two beta globin chains.\",\n",
    "}\n",
    "\n",
    "\n",
    "def _mock_run_verification(self, config, question_ids=None, **kwargs):\n",
    "    \"\"\"Return realistic mock results for documentation examples.\"\"\"\n",
    "    qids = question_ids or self.get_question_ids()\n",
    "    mock_results = []\n",
    "    for qid in qids:\n",
    "        q = self.get_question(qid)\n",
    "        question_text = q[\"question\"]\n",
    "        # Match question to mock response\n",
    "        response, verified = \"Mock response\", True\n",
    "        for key, resp in _MOCK_RESPONSES.items():\n",
    "            if key in question_text.lower():\n",
    "                response = resp\n",
    "                break\n",
    "        answering = ModelIdentity(model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "        parsing = ModelIdentity(model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "        ts = datetime.datetime.now(tz=datetime.UTC).isoformat()\n",
    "        result_id = VerificationResultMetadata.compute_result_id(qid, answering, parsing, ts)\n",
    "        template_result = VerificationResultTemplate(\n",
    "            raw_llm_response=response,\n",
    "            verify_result=verified,\n",
    "            template_verification_performed=True,\n",
    "        )\n",
    "        rubric_result = None\n",
    "        if config.rubric_enabled:\n",
    "            scores = {\"Conciseness\": 4}\n",
    "            regex_scores = {}\n",
    "            if \"venetoclax\" in question_text.lower():\n",
    "                regex_scores[\"Contains BCL2\"] = True\n",
    "            rubric_result = VerificationResultRubric(\n",
    "                rubric_evaluation_performed=True,\n",
    "                rubric_evaluation_strategy=\"batch\",\n",
    "                llm_trait_scores=scores,\n",
    "                regex_trait_scores=regex_scores if regex_scores else None,\n",
    "            )\n",
    "        result = VerificationResult(\n",
    "            metadata=VerificationResultMetadata(\n",
    "                question_id=qid,\n",
    "                template_id=\"mock_template\",\n",
    "                completed_without_errors=True,\n",
    "                question_text=question_text,\n",
    "                raw_answer=q.get(\"raw_answer\"),\n",
    "                answering=answering,\n",
    "                parsing=parsing,\n",
    "                execution_time=1.2,\n",
    "                timestamp=ts,\n",
    "                result_id=result_id,\n",
    "            ),\n",
    "            template=template_result,\n",
    "            rubric=rubric_result,\n",
    "        )\n",
    "        mock_results.append(result)\n",
    "    return VerificationResultSet(results=mock_results)\n",
    "\n",
    "\n",
    "_patcher_run = patch(\n",
    "    \"karenina.benchmark.benchmark.Benchmark.run_verification\",\n",
    "    _mock_run_verification,\n",
    ")\n",
    "_patcher_validate = patch.object(VerificationConfig, \"_validate_config\", lambda self: None)\n",
    "_patcher_run.start()\n",
    "_patcher_validate.start()\n",
    "\n",
    "# Temp directory for save/load examples\n",
    "_tmpdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71aeead",
   "metadata": {},
   "source": [
    "## Step 1: Create a Benchmark\n",
    "\n",
    "A benchmark is the top-level container that holds questions, answer templates, rubric traits, and verification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fddc07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:54.985156Z",
     "iopub.status.busy": "2026-02-06T11:50:54.985073Z",
     "iopub.status.idle": "2026-02-06T11:50:54.986792Z",
     "shell.execute_reply": "2026-02-06T11:50:54.986589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created benchmark: Genomics Knowledge Benchmark\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    description=\"Testing LLM knowledge of genomics and molecular biology\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Your Name\",\n",
    ")\n",
    "\n",
    "print(f\"Created benchmark: {benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17201e41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Add Questions\n",
    "\n",
    "Each question has a text prompt and a reference answer (the ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4de132e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:54.987673Z",
     "iopub.status.busy": "2026-02-06T11:50:54.987618Z",
     "iopub.status.idle": "2026-02-06T11:50:54.989264Z",
     "shell.execute_reply": "2026-02-06T11:50:54.989068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 questions\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    {\n",
    "        \"question\": \"How many chromosomes are in a human somatic cell?\",\n",
    "        \"answer\": \"46\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the approved drug target of Venetoclax?\",\n",
    "        \"answer\": \"BCL2\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many protein subunits does hemoglobin A have?\",\n",
    "        \"answer\": \"4\",\n",
    "    },\n",
    "]\n",
    "\n",
    "question_ids = []\n",
    "for q in questions:\n",
    "    qid = benchmark.add_question(\n",
    "        question=q[\"question\"],\n",
    "        raw_answer=q[\"answer\"],\n",
    "        author={\"name\": \"Bio Curator\", \"email\": \"curator@example.com\"},\n",
    "    )\n",
    "    question_ids.append(qid)\n",
    "\n",
    "print(f\"Added {len(question_ids)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89f95e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Write Answer Templates\n",
    "\n",
    "Answer templates are Pydantic models that define how a Judge LLM should parse and verify a model's response. Each template:\n",
    "\n",
    "1. Declares **attributes** the judge must extract (typed fields)\n",
    "2. Stores the **correct values** in `model_post_init`\n",
    "3. Implements a **`verify()`** method that compares extracted values to ground truth\n",
    "\n",
    "The class must always be named `Answer` and inherit from `BaseAnswer`.\n",
    "\n",
    "Here is a template for the Venetoclax question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75109e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:54.990198Z",
     "iopub.status.busy": "2026-02-06T11:50:54.990142Z",
     "iopub.status.idle": "2026-02-06T11:50:54.991488Z",
     "shell.execute_reply": "2026-02-06T11:50:54.991311Z"
    }
   },
   "outputs": [],
   "source": [
    "venetoclax_template = \"\"\"\n",
    "from pydantic import Field\n",
    "from karenina.schemas.entities import BaseAnswer\n",
    "\n",
    "class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target of the drug mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCL2\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.target.strip().upper() == self.correct[\"target\"].upper()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650d8c9",
   "metadata": {},
   "source": [
    "Add templates to each question using `update_template`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edc66f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:54.992447Z",
     "iopub.status.busy": "2026-02-06T11:50:54.992380Z",
     "iopub.status.idle": "2026-02-06T11:50:54.994737Z",
     "shell.execute_reply": "2026-02-06T11:50:54.994554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added templates to 3 questions\n"
     ]
    }
   ],
   "source": [
    "chromosomes_template = \"\"\"\n",
    "from pydantic import Field\n",
    "from karenina.schemas.entities import BaseAnswer\n",
    "\n",
    "class Answer(BaseAnswer):\n",
    "    count: int = Field(description=\"The number of chromosomes mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"count\": 46}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.count == self.correct[\"count\"]\n",
    "\"\"\"\n",
    "\n",
    "hemoglobin_template = \"\"\"\n",
    "from pydantic import Field\n",
    "from karenina.schemas.entities import BaseAnswer\n",
    "\n",
    "class Answer(BaseAnswer):\n",
    "    subunit_count: int = Field(description=\"The number of protein subunits mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"subunit_count\": 4}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.subunit_count == self.correct[\"subunit_count\"]\n",
    "\"\"\"\n",
    "\n",
    "templates = [chromosomes_template, venetoclax_template, hemoglobin_template]\n",
    "for qid, code in zip(question_ids, templates, strict=False):\n",
    "    benchmark.update_template(qid, code)\n",
    "\n",
    "print(f\"Added templates to {len(templates)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcae98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Add Rubric Traits\n",
    "\n",
    "While templates verify **correctness**, rubrics assess **quality** — properties of the raw response like conciseness, safety, or format compliance.\n",
    "\n",
    "Karenina supports four trait types: LLM, regex, callable, and metric. Here we use two.\n",
    "\n",
    "### Global Trait (evaluated for every question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bea2b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:54.995618Z",
     "iopub.status.busy": "2026-02-06T11:50:54.995553Z",
     "iopub.status.idle": "2026-02-06T11:50:54.996952Z",
     "shell.execute_reply": "2026-02-06T11:50:54.996775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added global rubric trait: Conciseness (score 1-5)\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait\n",
    "\n",
    "benchmark.add_global_rubric_trait(\n",
    "    LLMRubricTrait(\n",
    "        name=\"Conciseness\",\n",
    "        description=\"Rate how concise the answer is on a scale of 1-5, where 1 is very verbose and 5 is extremely concise.\",\n",
    "        kind=\"score\",\n",
    "    )\n",
    ")\n",
    "print(\"Added global rubric trait: Conciseness (score 1-5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace3368",
   "metadata": {},
   "source": [
    "### Question-Specific Trait (evaluated for one question)\n",
    "\n",
    "This regex trait checks that the Venetoclax answer mentions the BCL2 protein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8044dd4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:54.997805Z",
     "iopub.status.busy": "2026-02-06T11:50:54.997757Z",
     "iopub.status.idle": "2026-02-06T11:50:54.999294Z",
     "shell.execute_reply": "2026-02-06T11:50:54.999099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added regex trait 'Contains BCL2' to question urn:uuid:question-what-is-the-approved-drug-target-of-venetoclax-2a9de717\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import RegexTrait\n",
    "\n",
    "venetoclax_qid = question_ids[1]  # The Venetoclax question\n",
    "\n",
    "benchmark.add_question_rubric_trait(\n",
    "    venetoclax_qid,\n",
    "    RegexTrait(\n",
    "        name=\"Contains BCL2\",\n",
    "        description=\"The response must mention BCL2\",\n",
    "        pattern=r\"\\bBCL2\\b\",\n",
    "        case_sensitive=True,\n",
    "    ),\n",
    ")\n",
    "print(f\"Added regex trait 'Contains BCL2' to question {venetoclax_qid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6aaf7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Run Verification\n",
    "\n",
    "Configure the answering model (the model being evaluated) and the parsing model (the judge), then run verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6020c3a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:55.000152Z",
     "iopub.status.busy": "2026-02-06T11:50:55.000097Z",
     "iopub.status.idle": "2026-02-06T11:50:55.001790Z",
     "shell.execute_reply": "2026-02-06T11:50:55.001610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification complete — 3 results\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            interface=\"langchain\",\n",
    "            temperature=0.7,\n",
    "            system_prompt=\"You are a knowledgeable assistant. Answer accurately and concisely.\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            interface=\"langchain\",\n",
    "            temperature=0.0,\n",
    "        )\n",
    "    ],\n",
    "    rubric_enabled=True,\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Verification complete — {len(results.results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c648c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Inspect Results\n",
    "\n",
    "### Iterate over results\n",
    "\n",
    "Each `VerificationResult` contains metadata, template verification, and rubric evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e49ed59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:55.002588Z",
     "iopub.status.busy": "2026-02-06T11:50:55.002521Z",
     "iopub.status.idle": "2026-02-06T11:50:55.004207Z",
     "shell.execute_reply": "2026-02-06T11:50:55.004023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [PASS] How many chromosomes are in a human somatic cell?  rubric: Conciseness=4\n",
      "  [PASS] What is the approved drug target of Venetoclax?  rubric: Conciseness=4\n",
      "  [PASS] How many protein subunits does hemoglobin A have?  rubric: Conciseness=4\n"
     ]
    }
   ],
   "source": [
    "for result in results.results:\n",
    "    q_text = result.metadata.question_text[:60]\n",
    "\n",
    "    # Template verification\n",
    "    if result.template and result.template.verify_result is not None:\n",
    "        status = \"PASS\" if result.template.verify_result else \"FAIL\"\n",
    "    else:\n",
    "        status = \"N/A\"\n",
    "\n",
    "    # Rubric scores\n",
    "    rubric_info = \"\"\n",
    "    if result.rubric and result.rubric.llm_trait_scores:\n",
    "        scores = \", \".join(f\"{k}={v}\" for k, v in result.rubric.llm_trait_scores.items())\n",
    "        rubric_info = f\"  rubric: {scores}\"\n",
    "\n",
    "    print(f\"  [{status}] {q_text}{rubric_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664c305",
   "metadata": {},
   "source": [
    "### Aggregate pass rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58e5e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:55.005014Z",
     "iopub.status.busy": "2026-02-06T11:50:55.004965Z",
     "iopub.status.idle": "2026-02-06T11:50:55.006428Z",
     "shell.execute_reply": "2026-02-06T11:50:55.006227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall pass rate: 3/3 (100%)\n"
     ]
    }
   ],
   "source": [
    "total = len(results.results)\n",
    "passed = sum(1 for r in results.results if r.template and r.template.verify_result)\n",
    "print(f\"\\nOverall pass rate: {passed}/{total} ({passed / total * 100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f12bd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Save and Load\n",
    "\n",
    "Save the benchmark — including questions, templates, rubrics, and results — as a JSON-LD checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeaa38ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:55.007271Z",
     "iopub.status.busy": "2026-02-06T11:50:55.007214Z",
     "iopub.status.idle": "2026-02-06T11:50:55.009170Z",
     "shell.execute_reply": "2026-02-06T11:50:55.008991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to genomics_benchmark.jsonld\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path(_tmpdir) / \"genomics_benchmark.jsonld\"\n",
    "benchmark.save(checkpoint_path)\n",
    "print(\"Saved to genomics_benchmark.jsonld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440443c",
   "metadata": {},
   "source": [
    "Load it back later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59811f87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:55.010141Z",
     "iopub.status.busy": "2026-02-06T11:50:55.010081Z",
     "iopub.status.idle": "2026-02-06T11:50:55.011758Z",
     "shell.execute_reply": "2026-02-06T11:50:55.011580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'Genomics Knowledge Benchmark' with 3 questions\n"
     ]
    }
   ],
   "source": [
    "loaded = Benchmark.load(checkpoint_path)\n",
    "print(f\"Loaded '{loaded.name}' with {loaded.question_count} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521b8736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T11:50:55.012562Z",
     "iopub.status.busy": "2026-02-06T11:50:55.012511Z",
     "iopub.status.idle": "2026-02-06T11:50:55.014273Z",
     "shell.execute_reply": "2026-02-06T11:50:55.014038Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up mocks and temp directory\n",
    "import shutil\n",
    "\n",
    "_patcher_run.stop()\n",
    "_patcher_validate.stop()\n",
    "shutil.rmtree(_tmpdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a92e0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Going Further\n",
    "\n",
    "### Automatic Template Generation\n",
    "\n",
    "Instead of writing templates by hand, Karenina can generate them using an LLM.\n",
    "Call `benchmark.generate_all_templates()` with model parameters:\n",
    "\n",
    "> ```python\n",
    "> benchmark.generate_all_templates(\n",
    ">     model=\"gpt-4.1-mini\",\n",
    ">     model_provider=\"openai\",\n",
    ">     temperature=0.1,\n",
    "> )\n",
    "> ```\n",
    "\n",
    "See [Generating Templates](05-creating-benchmarks/generating-templates.md) for details.\n",
    "\n",
    "### Extracting Questions from Files\n",
    "\n",
    "Import questions from Excel, CSV, or TSV files using `extract_questions_from_file`:\n",
    "\n",
    "> ```python\n",
    "> from karenina.benchmark.authoring.questions import extract_questions_from_file\n",
    ">\n",
    "> questions = extract_questions_from_file(\n",
    ">     file_path=\"questions.xlsx\",\n",
    ">     question_column=\"Question\",\n",
    ">     answer_column=\"Answer\",\n",
    ">     keywords_columns=[{\"column\": \"Keywords\", \"separator\": \",\"}],\n",
    "> )\n",
    "> ```\n",
    "\n",
    "See [Adding Questions](05-creating-benchmarks/adding-questions.md) for details.\n",
    "\n",
    "### Using Different LLM Providers\n",
    "\n",
    "Karenina supports many backends. Pass different `interface` values to `ModelConfig`:\n",
    "\n",
    "> ```python\n",
    "> # Anthropic Claude (via LangChain)\n",
    "> ModelConfig(id=\"claude\", model_name=\"claude-sonnet-4-5-20250929\", interface=\"langchain\")\n",
    ">\n",
    "> # OpenRouter\n",
    "> ModelConfig(id=\"sonnet\", model_name=\"anthropic/claude-sonnet-4-5-20250929\", interface=\"openrouter\")\n",
    ">\n",
    "> # Local model (Ollama or any OpenAI-compatible endpoint)\n",
    "> ModelConfig(id=\"local\", model_name=\"llama3\", interface=\"openai_endpoint\",\n",
    ">             endpoint_base_url=\"http://localhost:11434/v1\")\n",
    "> ```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "| Topic | Link |\n",
    "|-------|------|\n",
    "| Core concepts (checkpoints, templates, rubrics) | [Core Concepts](04-core-concepts/index.md) |\n",
    "| Writing custom templates in depth | [Writing Templates](05-creating-benchmarks/writing-templates.md) |\n",
    "| All four rubric trait types | [Rubrics](04-core-concepts/rubrics/index.md) |\n",
    "| Verification configuration options | [Verification Config](06-running-verification/verification-config.md) |\n",
    "| DataFrame-based result analysis | [DataFrame Analysis](07-analyzing-results/dataframe-analysis.md) |\n",
    "| CLI verification (no Python needed) | [CLI Reference](09-cli-reference/verify.md) |"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,notebooks//ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
