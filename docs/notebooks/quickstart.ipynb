{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fb7571",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "Get started with Karenina in minutes. This guide walks you through creating a benchmark, adding questions, writing answer templates, defining rubric traits, running verification, and inspecting results.\n",
    "\n",
    "By the end you will have a working benchmark that evaluates LLM responses for both **correctness** (via answer templates) and **quality** (via rubric traits).\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Python 3.11+**\n",
    "- **Karenina installed** (see [Installation](installation.md))\n",
    "- **API keys** for the LLM providers you plan to use:\n",
    "\n",
    "> ```bash\n",
    "> export OPENAI_API_KEY=\"sk-...\"\n",
    "> export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "> export GOOGLE_API_KEY=\"AI...\"\n",
    "> ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91217e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:39.789470Z",
     "iopub.status.busy": "2026-02-10T16:25:39.789386Z",
     "iopub.status.idle": "2026-02-10T16:25:39.944446Z",
     "shell.execute_reply": "2026-02-10T16:25:39.944026Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: replays captured LLM responses from docs/data/quickstart/ so the\n",
    "# quickstart executes without live API keys. The full pipeline logic runs;\n",
    "# only the raw model calls are mocked.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import hashlib\n",
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Resolve fixtures directory (works from notebook, markdown, and repo root CWDs)\n",
    "_FIXTURES_DIR = None\n",
    "for _candidate in [Path(\"data/quickstart\"), Path(\"../data/quickstart\"), Path(\"docs/data/quickstart\")]:\n",
    "    if _candidate.is_dir():\n",
    "        _FIXTURES_DIR = _candidate\n",
    "        break\n",
    "assert _FIXTURES_DIR is not None, \"Could not find data/quickstart fixtures directory\"\n",
    "\n",
    "# Load fixtures indexed by prompt hash for order-independent matching\n",
    "_fixtures_by_hash: dict[str, dict] = {}\n",
    "for _p in _FIXTURES_DIR.glob(\"*.json\"):\n",
    "    _data = json.loads(_p.read_text())\n",
    "    _fixtures_by_hash[_data[\"prompt_hash\"]] = _data\n",
    "\n",
    "\n",
    "def _hash_messages(messages) -> str:\n",
    "    \"\"\"Compute the same hash used during capture for fixture matching.\"\"\"\n",
    "    normalized = []\n",
    "    for msg in messages:\n",
    "        content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
    "        msg_type = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
    "        if isinstance(content, str):\n",
    "            normalized.append(f\"{msg_type}:{content}\")\n",
    "        elif isinstance(content, list):\n",
    "            text_parts = []\n",
    "            for block in content:\n",
    "                if isinstance(block, dict):\n",
    "                    text_parts.append(str(block.get(\"text\", block.get(\"input\", \"\"))))\n",
    "                else:\n",
    "                    text_parts.append(str(block))\n",
    "            normalized.append(f\"{msg_type}:{'|'.join(text_parts)}\")\n",
    "    return hashlib.sha256(\"|\".join(normalized).encode()).hexdigest()[:16]\n",
    "\n",
    "\n",
    "# Save original ainvoke for restoration\n",
    "_original_ainvoke = BaseChatModel.ainvoke\n",
    "\n",
    "\n",
    "async def _replaying_ainvoke(self, input, config=None, **kwargs):\n",
    "    \"\"\"Return the captured LLM response matching this request's prompt hash.\"\"\"\n",
    "    messages = input if isinstance(input, list) else [input]\n",
    "    prompt_hash = _hash_messages(messages)\n",
    "    fixture = _fixtures_by_hash.get(prompt_hash)\n",
    "    if fixture is None:\n",
    "        raise ValueError(f\"No fixture for prompt hash {prompt_hash}\")\n",
    "    resp = fixture[\"response\"]\n",
    "    return AIMessage(\n",
    "        content=resp[\"content\"],\n",
    "        id=resp.get(\"id\", \"fixture\"),\n",
    "        tool_calls=resp.get(\"tool_calls\", []),\n",
    "        response_metadata=resp.get(\"response_metadata\", {}),\n",
    "        usage_metadata=resp.get(\"usage_metadata\"),\n",
    "    )\n",
    "\n",
    "\n",
    "BaseChatModel.ainvoke = _replaying_ainvoke\n",
    "\n",
    "# Temp directory for save/load examples\n",
    "_tmpdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71aeead",
   "metadata": {},
   "source": [
    "## Step 1: Create a Benchmark\n",
    "\n",
    "A benchmark is the top-level container that holds questions, answer templates, rubric traits, and verification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fddc07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:39.945733Z",
     "iopub.status.busy": "2026-02-10T16:25:39.945651Z",
     "iopub.status.idle": "2026-02-10T16:25:40.233904Z",
     "shell.execute_reply": "2026-02-10T16:25:40.233654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created benchmark: Genomics Knowledge Benchmark\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    description=\"Testing LLM knowledge of genomics and molecular biology\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Your Name\",\n",
    ")\n",
    "\n",
    "print(f\"Created benchmark: {benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17201e41",
   "metadata": {},
   "source": [
    "> **Learn more**: [Creating Checkpoints](../05-creating-benchmarks/creating-checkpoint.md) · [Core Concepts](../core_concepts/index.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Add Questions\n",
    "\n",
    "Each question has a text prompt and a reference answer (the ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4de132e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:40.235022Z",
     "iopub.status.busy": "2026-02-10T16:25:40.234937Z",
     "iopub.status.idle": "2026-02-10T16:25:40.236995Z",
     "shell.execute_reply": "2026-02-10T16:25:40.236784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 questions\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    {\n",
    "        \"question\": \"How many chromosomes are in a human somatic cell?\",\n",
    "        \"answer\": \"46\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the approved drug target of Venetoclax?\",\n",
    "        \"answer\": \"BCL2\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many protein subunits does hemoglobin A have?\",\n",
    "        \"answer\": \"4\",\n",
    "    },\n",
    "]\n",
    "\n",
    "question_ids = []\n",
    "for q in questions:\n",
    "    qid = benchmark.add_question(\n",
    "        question=q[\"question\"],\n",
    "        raw_answer=q[\"answer\"],\n",
    "        author={\"name\": \"Bio Curator\", \"email\": \"curator@example.com\"},\n",
    "    )\n",
    "    question_ids.append(qid)\n",
    "\n",
    "print(f\"Added {len(question_ids)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89f95e",
   "metadata": {},
   "source": [
    "> **Learn more**: [Adding Questions](../05-creating-benchmarks/adding-questions.md) — including bulk import from Excel, CSV, and TSV files\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Write Answer Templates\n",
    "\n",
    "Answer templates are Pydantic models that define how a Judge LLM should parse and verify a model's response. Each template:\n",
    "\n",
    "1. Declares **attributes** the judge must extract (typed fields)\n",
    "2. Stores the **correct values** in `model_post_init`\n",
    "3. Implements a **`verify()`** method that compares extracted values to ground truth\n",
    "\n",
    "The class must always be named `Answer` and inherit from `BaseAnswer`.\n",
    "\n",
    "### Automatic Generation\n",
    "\n",
    "The fastest way to get started is to let Karenina generate templates for you using an LLM. This analyses each question and its reference answer, then produces a complete template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75109e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:40.237876Z",
     "iopub.status.busy": "2026-02-10T16:25:40.237821Z",
     "iopub.status.idle": "2026-02-10T16:25:40.498346Z",
     "shell.execute_reply": "2026-02-10T16:25:40.498122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated templates for 3 questions\n"
     ]
    }
   ],
   "source": [
    "benchmark.generate_all_templates(\n",
    "    model=\"claude-haiku-4-5\",\n",
    "    model_provider=\"anthropic\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(f\"Generated templates for {benchmark.question_count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650d8c9",
   "metadata": {},
   "source": [
    "You can review a generated template to see what the LLM produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edc66f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:40.499426Z",
     "iopub.status.busy": "2026-02-10T16:25:40.499357Z",
     "iopub.status.idle": "2026-02-10T16:25:40.500783Z",
     "shell.execute_reply": "2026-02-10T16:25:40.500586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Answer(BaseAnswer):\n",
      "    chromosome_count: int = Field(description=\"Provide the integer count of chromosomes in a human somatic cell as stated or implied in the response. The expected answer is 46.\")\n",
      "\n",
      "    def model_post_init(self, __context):\n",
      "        self.correct = {\"chromosome_count\": 46}\n",
      "\n",
      "    def verify(self) -> bool:\n",
      "        return self.chromosome_count == self.correct[\"chromosome_count\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_code = benchmark.get_template(question_ids[0])\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8377309",
   "metadata": {},
   "source": [
    "### Manual Definition (Class-Based)\n",
    "\n",
    "When you need precise control over verification logic, define templates as Python classes and pass them directly. This is especially useful for domain-specific comparisons or multi-field extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ff8f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:40.501762Z",
     "iopub.status.busy": "2026-02-10T16:25:40.501697Z",
     "iopub.status.idle": "2026-02-10T16:25:40.503995Z",
     "shell.execute_reply": "2026-02-10T16:25:40.503774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated template for Venetoclax question with class-based definition\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "from karenina.schemas.entities import BaseAnswer\n",
    "\n",
    "\n",
    "class Answer(BaseAnswer):\n",
    "    is_bcl2: bool = Field(\n",
    "        description=\"Whether the response identifies BCL2 as the putative target of the drug\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"is_bcl2\": True}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.is_bcl2 == self.correct[\"is_bcl2\"]\n",
    "\n",
    "\n",
    "benchmark.update_template(question_ids[1], Answer)\n",
    "\n",
    "print(\"Updated template for Venetoclax question with class-based definition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcae98",
   "metadata": {},
   "source": [
    "> **Learn more**: [Writing Templates](../05-creating-benchmarks/writing-templates.md) · [Generating Templates](../05-creating-benchmarks/generating-templates.md) · [Answer Templates (Concepts)](../core_concepts/answer-templates.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Add Rubric Traits\n",
    "\n",
    "While templates verify **correctness**, rubrics assess **quality** — properties of the raw response like conciseness, safety, or format compliance.\n",
    "\n",
    "Karenina supports four trait types: LLM, regex, callable, and metric. Here we use two.\n",
    "\n",
    "### Global Trait (evaluated for every question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bea2b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:40.505058Z",
     "iopub.status.busy": "2026-02-10T16:25:40.504998Z",
     "iopub.status.idle": "2026-02-10T16:25:40.506496Z",
     "shell.execute_reply": "2026-02-10T16:25:40.506327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added global rubric trait: Conciseness (score 1-5)\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait\n",
    "\n",
    "benchmark.add_global_rubric_trait(\n",
    "    LLMRubricTrait(\n",
    "        name=\"Conciseness\",\n",
    "        description=\"Rate how concise the answer is on a scale of 1-5, where 1 is very verbose and 5 is extremely concise.\",\n",
    "        kind=\"score\",\n",
    "    )\n",
    ")\n",
    "print(\"Added global rubric trait: Conciseness (score 1-5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace3368",
   "metadata": {},
   "source": [
    "### Question-Specific Trait (evaluated for one question)\n",
    "\n",
    "This regex trait checks that the Venetoclax answer mentions the BCL2 protein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8044dd4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:40.507423Z",
     "iopub.status.busy": "2026-02-10T16:25:40.507356Z",
     "iopub.status.idle": "2026-02-10T16:25:40.509100Z",
     "shell.execute_reply": "2026-02-10T16:25:40.508901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added regex trait 'Contains BCL2' to question urn:uuid:question-what-is-the-approved-drug-target-of-venetoclax-2a9de717\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import RegexTrait\n",
    "\n",
    "venetoclax_qid = question_ids[1]  # The Venetoclax question\n",
    "\n",
    "benchmark.add_question_rubric_trait(\n",
    "    venetoclax_qid,\n",
    "    RegexTrait(\n",
    "        name=\"Contains BCL2\",\n",
    "        description=\"The response must mention BCL2\",\n",
    "        pattern=r\"\\bBCL2\\b\",\n",
    "        case_sensitive=True,\n",
    "    ),\n",
    ")\n",
    "print(f\"Added regex trait 'Contains BCL2' to question {venetoclax_qid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6aaf7a",
   "metadata": {},
   "source": [
    "> **Learn more**: [Defining Rubrics](../05-creating-benchmarks/defining-rubrics.md) · [All Four Trait Types](../core_concepts/rubrics/index.md) — LLM, regex, callable, and metric traits\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Run Verification\n",
    "\n",
    "Configure the answering model (the model being evaluated) and the parsing model (the judge), then run verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6020c3a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:40.510071Z",
     "iopub.status.busy": "2026-02-10T16:25:40.509997Z",
     "iopub.status.idle": "2026-02-10T16:25:50.575574Z",
     "shell.execute_reply": "2026-02-10T16:25:50.575345Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapter cleanup timed out after 10 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification complete — 3 results\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku-4-5\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            model_provider=\"anthropic\",\n",
    "            interface=\"langchain\",\n",
    "            temperature=0.7,\n",
    "            system_prompt=\"You are a knowledgeable assistant. Answer accurately and concisely.\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku-4-5\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            model_provider=\"anthropic\",\n",
    "            interface=\"langchain\",\n",
    "            temperature=0.0,\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    rubric_enabled=True,\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Verification complete — {len(results.results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c648c",
   "metadata": {},
   "source": [
    "> **Learn more**: [Verification Config](../06-running-verification/verification-config.md) · [Multi-Model Evaluation](../06-running-verification/multi-model.md) · [Model Config Reference](../10-configuration-reference/model-config.md) · [CLI Verification](../09-cli-reference/verify.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Inspect Results\n",
    "\n",
    "`VerificationResultSet` provides specialized accessors that convert results into pandas DataFrames for analysis.\n",
    "\n",
    "### Template results\n",
    "\n",
    "Use `get_template_results()` to access pass/fail data and field-level comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e49ed59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:50.576779Z",
     "iopub.status.busy": "2026-02-10T16:25:50.576706Z",
     "iopub.status.idle": "2026-02-10T16:25:50.581831Z",
     "shell.execute_reply": "2026-02-10T16:25:50.581599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>field_name</th>\n",
       "      <th>gt_value</th>\n",
       "      <th>llm_value</th>\n",
       "      <th>field_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urn:uuid:question-how-many-chromosomes-are-in-...</td>\n",
       "      <td>chromosome_count</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urn:uuid:question-what-is-the-approved-drug-ta...</td>\n",
       "      <td>is_bcl2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urn:uuid:question-how-many-protein-subunits-do...</td>\n",
       "      <td>hemoglobin_a_subunit_count</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question_id  \\\n",
       "0  urn:uuid:question-how-many-chromosomes-are-in-...   \n",
       "1  urn:uuid:question-what-is-the-approved-drug-ta...   \n",
       "2  urn:uuid:question-how-many-protein-subunits-do...   \n",
       "\n",
       "                   field_name gt_value llm_value  field_match  \n",
       "0            chromosome_count       46        46         True  \n",
       "1                     is_bcl2     True      True         True  \n",
       "2  hemoglobin_a_subunit_count        4         4         True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_results = results.get_template_results()\n",
    "df_templates = template_results.to_dataframe()\n",
    "\n",
    "df_templates[[\"question_id\", \"field_name\", \"gt_value\", \"llm_value\", \"field_match\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664c305",
   "metadata": {},
   "source": [
    "### Pass rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58e5e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:50.582909Z",
     "iopub.status.busy": "2026-02-10T16:25:50.582846Z",
     "iopub.status.idle": "2026-02-10T16:25:50.585664Z",
     "shell.execute_reply": "2026-02-10T16:25:50.585460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'urn:uuid:question-how-many-chromosomes-are-in-a-human-somatic-cell-3e6df3f9': 1.0,\n",
       " 'urn:uuid:question-how-many-protein-subunits-does-hemoglobin-a-have-99d0c100': 1.0,\n",
       " 'urn:uuid:question-what-is-the-approved-drug-target-of-venetoclax-2a9de717': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_results.aggregate_pass_rate(by=\"question_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca2db6",
   "metadata": {},
   "source": [
    "### Rubric results\n",
    "\n",
    "Use `get_rubrics_results()` to access trait scores as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9237029d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:50.586742Z",
     "iopub.status.busy": "2026-02-10T16:25:50.586676Z",
     "iopub.status.idle": "2026-02-10T16:25:50.590021Z",
     "shell.execute_reply": "2026-02-10T16:25:50.589836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>trait_name</th>\n",
       "      <th>trait_score</th>\n",
       "      <th>trait_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urn:uuid:question-how-many-chromosomes-are-in-...</td>\n",
       "      <td>Conciseness</td>\n",
       "      <td>4</td>\n",
       "      <td>llm_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urn:uuid:question-what-is-the-approved-drug-ta...</td>\n",
       "      <td>Conciseness</td>\n",
       "      <td>2</td>\n",
       "      <td>llm_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urn:uuid:question-what-is-the-approved-drug-ta...</td>\n",
       "      <td>Contains BCL2</td>\n",
       "      <td>True</td>\n",
       "      <td>regex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urn:uuid:question-how-many-protein-subunits-do...</td>\n",
       "      <td>Conciseness</td>\n",
       "      <td>3</td>\n",
       "      <td>llm_score</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question_id     trait_name  \\\n",
       "0  urn:uuid:question-how-many-chromosomes-are-in-...    Conciseness   \n",
       "1  urn:uuid:question-what-is-the-approved-drug-ta...    Conciseness   \n",
       "2  urn:uuid:question-what-is-the-approved-drug-ta...  Contains BCL2   \n",
       "3  urn:uuid:question-how-many-protein-subunits-do...    Conciseness   \n",
       "\n",
       "  trait_score trait_type  \n",
       "0           4  llm_score  \n",
       "1           2  llm_score  \n",
       "2        True      regex  \n",
       "3           3  llm_score  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rubric_results = results.get_rubrics_results()\n",
    "df_rubrics = rubric_results.to_dataframe()\n",
    "\n",
    "df_rubrics[[\"question_id\", \"trait_name\", \"trait_score\", \"trait_type\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f12bd8",
   "metadata": {},
   "source": [
    "> **Learn more**: [DataFrame Analysis](../07-analyzing-results/dataframe-analysis.md) · [VerificationResult](../07-analyzing-results/verification-result.md) · [Exporting Results](../07-analyzing-results/exporting.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Save and Load\n",
    "\n",
    "Save the benchmark — including questions, templates, rubrics, and results — as a JSON-LD checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeaa38ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:50.591014Z",
     "iopub.status.busy": "2026-02-10T16:25:50.590949Z",
     "iopub.status.idle": "2026-02-10T16:25:50.593031Z",
     "shell.execute_reply": "2026-02-10T16:25:50.592829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to genomics_benchmark.jsonld\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path(_tmpdir) / \"genomics_benchmark.jsonld\"\n",
    "benchmark.save(checkpoint_path)\n",
    "print(\"Saved to genomics_benchmark.jsonld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440443c",
   "metadata": {},
   "source": [
    "Load it back later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59811f87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:50.594001Z",
     "iopub.status.busy": "2026-02-10T16:25:50.593942Z",
     "iopub.status.idle": "2026-02-10T16:25:50.595628Z",
     "shell.execute_reply": "2026-02-10T16:25:50.595442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'Genomics Knowledge Benchmark' with 3 questions\n"
     ]
    }
   ],
   "source": [
    "loaded = Benchmark.load(checkpoint_path)\n",
    "print(f\"Loaded '{loaded.name}' with {loaded.question_count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ca9fe",
   "metadata": {},
   "source": [
    "> **Learn more**: [Checkpoints](../core_concepts/checkpoints.md) · [Saving Benchmarks](../05-creating-benchmarks/saving-benchmarks.md) · [Loading Benchmarks](../06-running-verification/loading-benchmark.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "521b8736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:25:50.596593Z",
     "iopub.status.busy": "2026-02-10T16:25:50.596534Z",
     "iopub.status.idle": "2026-02-10T16:25:50.598224Z",
     "shell.execute_reply": "2026-02-10T16:25:50.598006Z"
    },
    "lines_to_next_cell": 0,
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Restore original LLM behavior and clean up temp directory\n",
    "import shutil\n",
    "\n",
    "BaseChatModel.ainvoke = _original_ainvoke\n",
    "shutil.rmtree(_tmpdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb75224",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "getting-started//md,notebooks//ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
