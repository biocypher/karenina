{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:56.567512Z",
     "iopub.status.busy": "2026-01-04T09:04:56.567447Z",
     "iopub.status.idle": "2026-01-04T09:04:58.490198Z",
     "shell.execute_reply": "2026-01-04T09:04:58.489838Z"
    },
    "jupyter": {
     "source_hidden": false
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mock setup complete\n",
      "✓ Temp directory: /var/folders/34/129m5tdd04vf10ptyj12w6f80000gp/T/karenina_docs_7gzctznk\n",
      "✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\n",
      "✓ Mock verification results enabled - examples will show realistic output\n"
     ]
    }
   ],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unittest.mock import Mock, MagicMock, patch, PropertyMock\n",
    "from typing import Any, Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    "    VerificationResultRubric,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "    def __init__(self, content: str = \"BCL2\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = kwargs.get('count', 46)\n",
    "        self.target = kwargs.get('target', 'BCL2')\n",
    "        self.subunits = kwargs.get('subunits', 4)\n",
    "        self.diseases = kwargs.get('diseases', ['asthma', 'bronchitis', 'pneumonia'])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "    \n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n",
    "    \n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "def create_mock_verification_result(question_id: str, question_text: str, answer: str, passed: bool = True):\n",
    "    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "    \n",
    "    # Create mock template result\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=f\"The answer is {answer}.\",\n",
    "        parsed_llm_response={\"value\": answer},\n",
    "        parsed_gt_response={\"value\": answer},\n",
    "        verify_result=passed,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80}\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "    )\n",
    "    \n",
    "    # Create mock rubric result\n",
    "    rubric = VerificationResultRubric(\n",
    "        rubric_evaluation_performed=True,\n",
    "        llm_trait_scores={\n",
    "            \"Conciseness\": 4,\n",
    "            \"Clarity\": True,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create metadata with all required fields\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=answer,\n",
    "        answering_model=\"gpt-4.1-mini\",\n",
    "        parsing_model=\"gpt-4.1-mini\",\n",
    "        execution_time=1.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n",
    "    )\n",
    "    \n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "# Store original run_verification\n",
    "_original_run_verification = None\n",
    "\n",
    "def mock_run_verification(self, config):\n",
    "    \"\"\"Mock run_verification that returns realistic results.\"\"\"\n",
    "    global _original_run_verification\n",
    "    \n",
    "    # Get all finished questions\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "    \n",
    "    if len(finished) == 0:\n",
    "        if _original_run_verification:\n",
    "            return _original_run_verification(self, config)\n",
    "        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "    \n",
    "    results = []\n",
    "    mock_data = [\n",
    "        {\"keywords\": [\"chromosomes\"], \"answer\": \"46\", \"passed\": True},\n",
    "        {\"keywords\": [\"venetoclax\", \"bcl2\"], \"answer\": \"BCL2\", \"passed\": True},\n",
    "        {\"keywords\": [\"hemoglobin\", \"subunits\"], \"answer\": \"4\", \"passed\": True},\n",
    "        {\"keywords\": [\"inflammatory\", \"lung\"], \"answer\": \"asthma, bronchitis, pneumonia\", \"passed\": True},\n",
    "    ]\n",
    "    \n",
    "    for question in finished:\n",
    "        q_id = question['id']\n",
    "        q_text = question['question']\n",
    "        raw_answer = question.get('raw_answer', '')\n",
    "        \n",
    "        passed = True\n",
    "        mock_ans = raw_answer\n",
    "        q_text_lower = q_text.lower()\n",
    "        \n",
    "        for data in mock_data:\n",
    "            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n",
    "                passed = data[\"passed\"]\n",
    "                mock_ans = data[\"answer\"]\n",
    "                break\n",
    "        \n",
    "        results.append(create_mock_verification_result(\n",
    "            question_id=q_id,\n",
    "            question_text=q_text,\n",
    "            answer=mock_ans,\n",
    "            passed=passed\n",
    "        ))\n",
    "    \n",
    "    template_results = TemplateResults(results=results)\n",
    "    \n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch('langchain_openai.ChatOpenAI', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('langchain_anthropic.ChatAnthropic', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('langchain_google_genai.ChatGoogleGenerativeAI', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('karenina.infrastructure.llm.interface.init_chat_model_unified', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Patch Benchmark.run_verification\n",
    "from karenina.benchmark import Benchmark\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(f\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(f\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(f\"✓ Mock verification results enabled - examples will show realistic output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "Get started with Karenina in just a few minutes! This guide walks you through creating your first benchmark, adding questions, generating templates, and running verification.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, make sure you have:\n",
    "\n",
    "1. **Installed Karenina**:\n",
    "   ```bash\n",
    "   pip install karenina\n",
    "   ```\n",
    "\n",
    "2. **Set up API keys** (for LLM providers):\n",
    "   ```bash\n",
    "   # For OpenAI\n",
    "   export OPENAI_API_KEY=\"your-api-key-here\"\n",
    "\n",
    "   # For Google Gemini\n",
    "   export GOOGLE_API_KEY=\"your-api-key-here\"\n",
    "\n",
    "   # For Anthropic Claude\n",
    "   export ANTHROPIC_API_KEY=\"your-api-key-here\"\n",
    "   ```\n",
    "\n",
    "3. **Python 3.9+** installed\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Workflow Example\n",
    "\n",
    "This example demonstrates the full Karenina workflow: creating a benchmark, adding questions, generating templates, creating a rubric, running verification, and exporting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.503672Z",
     "iopub.status.busy": "2026-01-04T09:04:58.503514Z",
     "iopub.status.idle": "2026-01-04T09:04:58.505305Z",
     "shell.execute_reply": "2026-01-04T09:04:58.504958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created benchmark: Genomics Knowledge Benchmark\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "# Create a new benchmark\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    description=\"Testing LLM knowledge of genomics and molecular biology\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Your Name\"\n",
    ")\n",
    "\n",
    "print(f\"Created benchmark: {benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Add Questions\n",
    "\n",
    "You can add questions manually or extract them from files (Excel, CSV, TSV).\n",
    "\n",
    "**Option A: Add questions manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.506473Z",
     "iopub.status.busy": "2026-01-04T09:04:58.506402Z",
     "iopub.status.idle": "2026-01-04T09:04:58.508621Z",
     "shell.execute_reply": "2026-01-04T09:04:58.508257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 questions\n"
     ]
    }
   ],
   "source": [
    "# Add a few questions with answers\n",
    "questions = [\n",
    "    {\n",
    "        \"question\": \"How many chromosomes are in a human somatic cell?\",\n",
    "        \"answer\": \"46\",\n",
    "        \"author\": {\"name\": \"Bio Curator\", \"email\": \"curator@example.com\"}\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the approved drug target of Venetoclax?\",\n",
    "        \"answer\": \"BCL2\",\n",
    "        \"author\": {\"name\": \"Bio Curator\", \"email\": \"curator@example.com\"}\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many protein subunits does hemoglobin A have?\",\n",
    "        \"answer\": \"4\",\n",
    "        \"author\": {\"name\": \"Bio Curator\", \"email\": \"curator@example.com\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "question_ids = []\n",
    "for q in questions:\n",
    "    qid = benchmark.add_question(\n",
    "        question=q[\"question\"],\n",
    "        raw_answer=q[\"answer\"],\n",
    "        author=q[\"author\"]\n",
    "    )\n",
    "    question_ids.append(qid)\n",
    "\n",
    "print(f\"Added {len(question_ids)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option B: Extract from a file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.509683Z",
     "iopub.status.busy": "2026-01-04T09:04:58.509627Z",
     "iopub.status.idle": "2026-01-04T09:04:58.511594Z",
     "shell.execute_reply": "2026-01-04T09:04:58.511236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extraction API shown above (not executed in this demo)\n"
     ]
    }
   ],
   "source": [
    "from karenina.domain.questions.extractor import extract_questions_from_file\n",
    "\n",
    "# Note: This is an example of the API.\n",
    "# In this notebook, we'll skip actual file extraction since we don't have a file.\n",
    "# In practice, you would use:\n",
    "#\n",
    "# questions = extract_questions_from_file(\n",
    "#     file_path=\"questions.xlsx\",\n",
    "#     question_column=\"Question\",\n",
    "#     answer_column=\"Answer\",\n",
    "#     author_name_column=\"Author\",  # Optional\n",
    "#     keywords_columns=[{\"column\": \"Keywords\", \"separator\": \",\"}]  # Optional\n",
    "# )\n",
    "#\n",
    "# for q in questions:\n",
    "#     benchmark.add_question(**q)\n",
    "\n",
    "print(\"File extraction API shown above (not executed in this demo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3: Generate Answer Templates\n",
    "\n",
    "Answer templates define how to extract and verify information from LLM responses. Karenina can generate these automatically using an LLM.\n",
    "\n",
    "**Automatic template generation (recommended):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.512536Z",
     "iopub.status.busy": "2026-01-04T09:04:58.512467Z",
     "iopub.status.idle": "2026-01-04T09:04:58.524935Z",
     "shell.execute_reply": "2026-01-04T09:04:58.524628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating templates...\n",
      "Generated templates for 3 questions\n"
     ]
    }
   ],
   "source": [
    "# Generate templates for all questions\n",
    "# Note: This method takes individual parameters (not a ModelConfig object)\n",
    "print(\"Generating templates...\")\n",
    "results = benchmark.generate_all_templates(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    temperature=0.1,\n",
    "    interface=\"langchain\",\n",
    "    force_regenerate=False  # Skip questions that already have templates\n",
    ")\n",
    "\n",
    "print(f\"Generated templates for {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual template creation (for advanced users):**\n",
    "\n",
    "If you prefer full control, you can write templates manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.526117Z",
     "iopub.status.busy": "2026-01-04T09:04:58.526059Z",
     "iopub.status.idle": "2026-01-04T09:04:58.527788Z",
     "shell.execute_reply": "2026-01-04T09:04:58.527506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual template example created\n"
     ]
    }
   ],
   "source": [
    "# Manual template example\n",
    "template_code = '''class Answer(BaseAnswer):\n",
    "    mentions_bcl2_as_target: bool = Field(\n",
    "        description=\"True if the response identifies BCL2 (or BCL-2, B-cell lymphoma 2) as the drug target of Venetoclax\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"mentions_bcl2_as_target\": True}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.mentions_bcl2_as_target == self.correct[\"mentions_bcl2_as_target\"]\n",
    "'''\n",
    "\n",
    "# Add template to a specific question\n",
    "benchmark.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\",\n",
    "    raw_answer=\"BCL2\",\n",
    "    answer_template=template_code,\n",
    "    finished=True  # Mark as ready for verification\n",
    ")\n",
    "\n",
    "print(\"Manual template example created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Create a Rubric (Optional)\n",
    "\n",
    "Rubrics assess qualitative aspects of answers. Karenina supports two types of rubrics:\n",
    "\n",
    "- **Global Rubrics**: Applied to ALL questions (great for general quality assessment)\n",
    "- **Question-Specific Rubrics**: Applied to ONE specific question (great for domain-specific validation)\n",
    "\n",
    "#### Global Rubric (LLM-based traits)\n",
    "\n",
    "These traits evaluate general answer quality across all questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.528674Z",
     "iopub.status.busy": "2026-01-04T09:04:58.528616Z",
     "iopub.status.idle": "2026-01-04T09:04:58.530503Z",
     "shell.execute_reply": "2026-01-04T09:04:58.530133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created global rubric with 2 traits\n",
      "This rubric will be evaluated for ALL questions\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait, Rubric\n",
    "\n",
    "# Create a global rubric with LLM-based traits\n",
    "# These will be evaluated for EVERY question in the benchmark\n",
    "global_rubric = Rubric(\n",
    "    llm_traits=[\n",
    "        LLMRubricTrait(\n",
    "            name=\"Conciseness\",\n",
    "            description=\"Rate how concise the answer is on a scale of 1-5, where 1 is very verbose and 5 is extremely concise.\",\n",
    "            kind=\"score\"  # Returns a score from 1-5\n",
    "        ),\n",
    "        LLMRubricTrait(\n",
    "            name=\"Clarity\",\n",
    "            description=\"Is the answer clear and easy to understand?\",\n",
    "            kind=\"boolean\"  # Returns pass/fail (use \"boolean\", not \"binary\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set the global rubric\n",
    "benchmark.set_global_rubric(global_rubric)\n",
    "\n",
    "print(f\"Created global rubric with {len(global_rubric.llm_traits)} traits\")\n",
    "print(\"This rubric will be evaluated for ALL questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question-Specific Rubric (Regex-based trait)\n",
    "\n",
    "This trait validates that the answer contains specific content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.531452Z",
     "iopub.status.busy": "2026-01-04T09:04:58.531394Z",
     "iopub.status.idle": "2026-01-04T09:04:58.533281Z",
     "shell.execute_reply": "2026-01-04T09:04:58.532972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created question-specific rubric for question: urn:uuid:question-what-is-the-approved-drug-target-of-venetoclax-2a9de717\n",
      "This rubric will ONLY be evaluated for the Venetoclax question\n",
      "It checks that the answer contains 'BCL2'\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import RegexTrait\n",
    "\n",
    "# Find the drug target question ID\n",
    "drug_target_qid = [qid for qid in question_ids\n",
    "                   if \"Venetoclax\" in benchmark.get_question(qid)['question']][0]\n",
    "\n",
    "# Create a regex trait specific to the drug target question\n",
    "# The answer must contain \"BCL2\" (case-insensitive)\n",
    "regex_trait = RegexTrait(\n",
    "    name=\"BCL2 Mention\",\n",
    "    description=\"Answer must mention BCL2\",\n",
    "    pattern=r\"\\bBCL2\\b\",  # Matches the exact word \"BCL2\" with word boundaries\n",
    "    case_sensitive=False,\n",
    "    invert_result=False  # Don't invert the result (match = pass)\n",
    ")\n",
    "\n",
    "# Add ONLY to the drug target question (not global!)\n",
    "# Note: Use add_question_rubric_trait for single traits\n",
    "benchmark.add_question_rubric_trait(question_id=drug_target_qid, trait=regex_trait)\n",
    "\n",
    "print(f\"Created question-specific rubric for question: {drug_target_qid}\")\n",
    "print(\"This rubric will ONLY be evaluated for the Venetoclax question\")\n",
    "print(\"It checks that the answer contains 'BCL2'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question-Specific Rubric (Metric-based trait)\n",
    "\n",
    "For questions requiring classification accuracy (e.g., identifying disease types):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.534380Z",
     "iopub.status.busy": "2026-01-04T09:04:58.534314Z",
     "iopub.status.idle": "2026-01-04T09:04:58.536337Z",
     "shell.execute_reply": "2026-01-04T09:04:58.535998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metric-based rubric for question: urn:uuid:question-which-of-the-following-are-inflammatory-lung-disea-309b7d5b\n",
      "This will compute precision, recall, and F1 score for this specific question\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import MetricRubricTrait\n",
    "\n",
    "# Example: If you had a question like \"List inflammatory lung diseases\"\n",
    "# Add this question to demonstrate metric traits\n",
    "disease_qid = benchmark.add_question(\n",
    "    question=\"Which of the following are inflammatory lung diseases: asthma, bronchitis, pneumonia, emphysema, pulmonary fibrosis?\",\n",
    "    raw_answer=\"asthma, bronchitis, pneumonia\",\n",
    "    author={\"name\": \"Bio Curator\", \"email\": \"curator@example.com\"}\n",
    ")\n",
    "\n",
    "# Create a metric trait to evaluate classification accuracy\n",
    "# Note: Use tn_instructions for items that should NOT be in the answer\n",
    "metric_trait = MetricRubricTrait(\n",
    "    name=\"Inflammatory Disease Identification\",\n",
    "    description=\"Evaluate accuracy of identifying inflammatory lung diseases\",\n",
    "    metrics=[\"precision\", \"recall\", \"f1\"],\n",
    "    tp_instructions=[\n",
    "        \"asthma\",       # Should be identified (inflammatory)\n",
    "        \"bronchitis\",   # Should be identified (inflammatory)\n",
    "        \"pneumonia\"     # Should be identified (inflammatory)\n",
    "    ],\n",
    "    tn_instructions=[  # Use tn_instructions, not fp_instructions\n",
    "        \"emphysema\",            # Should NOT be identified (obstructive, not inflammatory)\n",
    "        \"pulmonary fibrosis\"    # Should NOT be identified (restrictive, not inflammatory)\n",
    "    ],\n",
    "    repeated_extraction=True  # Remove duplicate mentions\n",
    ")\n",
    "\n",
    "# Add ONLY to the disease classification question\n",
    "# Note: Use add_question_rubric_trait for single traits\n",
    "benchmark.add_question_rubric_trait(question_id=disease_qid, trait=metric_trait)\n",
    "\n",
    "print(f\"Created metric-based rubric for question: {disease_qid}\")\n",
    "print(\"This will compute precision, recall, and F1 score for this specific question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Distinction:**\n",
    "\n",
    "- **Global rubrics** (clarity, conciseness): Assessed for every question → generic quality metrics\n",
    "- **Question-specific rubrics** (gene format, disease classification): Assessed for one question → domain-specific validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 5: Run Verification\n",
    "\n",
    "Configure models and run verification to evaluate LLM responses against your templates and rubrics.\n",
    "\n",
    "> **Tip: CLI Alternative**  \n",
    "> You can also run verification from the command line without writing Python code: `karenina verify checkpoint.jsonld --preset config.json`. See [CLI Verification](using-karenina/cli-verification.md) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.537296Z",
     "iopub.status.busy": "2026-01-04T09:04:58.537242Z",
     "iopub.status.idle": "2026-01-04T09:04:58.539132Z",
     "shell.execute_reply": "2026-01-04T09:04:58.538853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verification...\n",
      "Verification complete! Processed 1 questions\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import VerificationConfig, ModelConfig\n",
    "\n",
    "# Configure verification\n",
    "config = VerificationConfig(\n",
    "    # Models that generate answers (can use multiple for comparison)\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are a knowledgeable assistant. Answer accurately and concisely.\"\n",
    "        )\n",
    "    ],\n",
    "    # Models that parse/judge answers (usually more capable models)\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are an expert judge. Parse and evaluate responses carefully.\"\n",
    "        )\n",
    "    ],\n",
    "    # Enable rubric evaluation with proper evaluation_mode\n",
    "    evaluation_mode=\"template_and_rubric\",  # Run both template and rubric evaluation\n",
    "    rubric_enabled=True,  # Enable rubric evaluation (required when evaluation_mode=template_and_rubric)\n",
    "    replicate_count=1,    # Number of times to run each question (use >1 for statistical analysis)\n",
    "    deep_judgment_enabled=False,  # Enable for detailed feedback with excerpts\n",
    "    abstention_check_enabled=True  # Detect when models refuse to answer\n",
    ")\n",
    "\n",
    "# Run verification\n",
    "print(\"Running verification...\")\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "print(f\"Verification complete! Processed {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using different interfaces:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.540049Z",
     "iopub.status.busy": "2026-01-04T09:04:58.539966Z",
     "iopub.status.idle": "2026-01-04T09:04:58.541482Z",
     "shell.execute_reply": "2026-01-04T09:04:58.541168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration examples shown above (not executed in this demo)\n"
     ]
    }
   ],
   "source": [
    "# Examples of different ModelConfig options:\n",
    "#\n",
    "# # OpenRouter\n",
    "# ModelConfig(\n",
    "#     id=\"sonnet-4.5\",\n",
    "#     model_provider=\"openrouter\",\n",
    "#     model_name=\"anthropic/claude-sonnet-4.5\",\n",
    "#     interface=\"openrouter\"\n",
    "# )\n",
    "#\n",
    "# # OpenAI-compatible endpoint (e.g., Ollama)\n",
    "# ModelConfig(\n",
    "#     id=\"glm46\",\n",
    "#     model_name=\"glm-4.6\",\n",
    "#     interface=\"openai_endpoint\",\n",
    "#     endpoint_api_key=\"your-api-key\",\n",
    "#     endpoint_base_url=\"http://localhost:11434/v1\"\n",
    "# )\n",
    "#\n",
    "# # Manual traces (for testing/debugging without API calls)\n",
    "# ModelConfig(\n",
    "#     id=\"manual\",\n",
    "#     model_provider=\"manual\",\n",
    "#     model_name=\"manual\",\n",
    "#     interface=\"manual\"\n",
    "# )\n",
    "\n",
    "print(\"Model configuration examples shown above (not executed in this demo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 6: Access and Analyze Results\n",
    "\n",
    "After verification, you can analyze results using DataFrames (recommended) or access raw result objects.\n",
    "\n",
    "#### Option 1: Analyze with DataFrames (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.542429Z",
     "iopub.status.busy": "2026-01-04T09:04:58.542347Z",
     "iopub.status.idle": "2026-01-04T09:04:58.554806Z",
     "shell.execute_reply": "2026-01-04T09:04:58.554445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Rates by Question:\n",
      "question_id\n",
      "urn:uuid:question-what-is-the-approved-drug-target-of-venetoclax-2a9de717-1    1.0\n",
      "Name: field_match, dtype: float64\n",
      "\n",
      "Rubric Scores:\n",
      "trait_name\n",
      "Clarity        1.0\n",
      "Conciseness    4.0\n",
      "Name: trait_score, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get DataFrame for easy analysis\n",
    "template_results = results.get_template_results()  # Use get_template_results(), not get_templates()\n",
    "df = template_results.to_dataframe()\n",
    "\n",
    "# Calculate pass rates\n",
    "if 'question_id' in df.columns and 'field_match' in df.columns:\n",
    "    pass_rates = df.groupby('question_id')['field_match'].mean()\n",
    "    print(\"Pass Rates by Question:\")\n",
    "    print(pass_rates)\n",
    "\n",
    "# If rubrics are enabled\n",
    "rubric_results = results.get_rubrics_results()  # Use get_rubrics_results(), not get_rubrics()\n",
    "rubric_df = rubric_results.to_dataframe(trait_type=\"llm\")\n",
    "if not rubric_df.empty:\n",
    "    print(\"\\nRubric Scores:\")\n",
    "    print(rubric_df.groupby('trait_name')['trait_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Analyzing Results with DataFrames](using-karenina/analyzing-results-dataframes.md) for comprehensive examples.\n",
    "\n",
    "#### Option 2: Access Raw Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.555815Z",
     "iopub.status.busy": "2026-01-04T09:04:58.555749Z",
     "iopub.status.idle": "2026-01-04T09:04:58.557878Z",
     "shell.execute_reply": "2026-01-04T09:04:58.557561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the approved drug target of Venetoclax?...\n",
      "Verification: ✓ PASS\n",
      "Model Answer: The answer is BCL2.\n",
      "Rubric Scores:\n",
      "  - Conciseness: 4\n",
      "  - Clarity: True\n"
     ]
    }
   ],
   "source": [
    "# Iterate through results\n",
    "for result in results.results:\n",
    "    print(f\"\\nQuestion: {result.question_text[:50]}...\")\n",
    "    print(f\"Verification: {'✓ PASS' if result.verify_result else '✗ FAIL'}\")\n",
    "    print(f\"Model Answer: {result.raw_llm_response[:100] if len(result.raw_llm_response) < 100 else result.raw_llm_response[:100] + '...'}\")\n",
    "\n",
    "    # Access rubric scores (if rubric enabled)\n",
    "    # Note: Access through result.rubric not result.rubric_scores\n",
    "    if result.rubric and result.rubric.llm_trait_scores:\n",
    "        print(\"Rubric Scores:\")\n",
    "        for trait_name, score in result.rubric.llm_trait_scores.items():\n",
    "            print(f\"  - {trait_name}: {score}\")\n",
    "\n",
    "    # Check for abstention (if enabled)\n",
    "    # Note: Access through result.template.abstention_reasoning\n",
    "    if result.abstention_detected:\n",
    "        reasoning = result.template.abstention_reasoning if result.template else None\n",
    "        print(f\"⚠ Model abstained: {reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate aggregate metrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.558929Z",
     "iopub.status.busy": "2026-01-04T09:04:58.558878Z",
     "iopub.status.idle": "2026-01-04T09:04:58.562389Z",
     "shell.execute_reply": "2026-01-04T09:04:58.561994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Overall Pass Rate: 100.0% (1/1)\n",
      "==================================================\n",
      "\n",
      "Pass rate from raw results: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrames (recommended)\n",
    "df = results.get_template_results().to_dataframe()  # Use get_template_results()\n",
    "successful = df[df['completed_without_errors'] == True]\n",
    "pass_rate = successful['field_match'].mean() * 100\n",
    "total = len(df.drop_duplicates(subset=['result_index']))\n",
    "passed = (total * pass_rate / 100)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Overall Pass Rate: {pass_rate:.1f}% ({passed:.0f}/{total})\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Or using raw results\n",
    "total = len(results.results)\n",
    "passed = sum(1 for r in results.results if r.verify_result)\n",
    "pass_rate_raw = (passed / total) * 100\n",
    "\n",
    "print(f\"\\nPass rate from raw results: {pass_rate_raw:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 7: Save and Export\n",
    "\n",
    "Save your benchmark as a checkpoint or export results for analysis.\n",
    "\n",
    "**Save checkpoint (preserves full benchmark state):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.563390Z",
     "iopub.status.busy": "2026-01-04T09:04:58.563331Z",
     "iopub.status.idle": "2026-01-04T09:04:58.566413Z",
     "shell.execute_reply": "2026-01-04T09:04:58.566022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to /var/folders/34/129m5tdd04vf10ptyj12w6f80000gp/T/karenina_docs_7gzctznk/genomics_benchmark.jsonld\n",
      "Loaded benchmark: Genomics Knowledge Benchmark\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Save benchmark with all questions, templates, and results\n",
    "checkpoint_path = temp_path(\"genomics_benchmark.jsonld\")\n",
    "benchmark.save(checkpoint_path)\n",
    "print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "# Load later\n",
    "loaded_benchmark = Benchmark.load(checkpoint_path)\n",
    "print(f\"Loaded benchmark: {loaded_benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export verification results to CSV/JSON:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.567464Z",
     "iopub.status.busy": "2026-01-04T09:04:58.567388Z",
     "iopub.status.idle": "2026-01-04T09:04:58.569553Z",
     "shell.execute_reply": "2026-01-04T09:04:58.569177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported verification results to results.csv and results.json\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV for spreadsheet analysis\n",
    "benchmark.export_verification_results_to_file(\n",
    "    file_path=temp_path(\"results.csv\"),\n",
    "    format=\"csv\"\n",
    ")\n",
    "\n",
    "# Export to JSON for programmatic analysis\n",
    "benchmark.export_verification_results_to_file(\n",
    "    file_path=temp_path(\"results.json\"),\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "print(\"Exported verification results to results.csv and results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to database:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.570508Z",
     "iopub.status.busy": "2026-01-04T09:04:58.570445Z",
     "iopub.status.idle": "2026-01-04T09:04:58.613214Z",
     "shell.execute_reply": "2026-01-04T09:04:58.612817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from database: Genomics Knowledge Benchmark\n"
     ]
    }
   ],
   "source": [
    "# Save benchmark to SQLite database (with checkpoint file)\n",
    "db_path = temp_path(\"benchmarks.db\")\n",
    "benchmark.save_to_db(\n",
    "    storage=f\"sqlite:///{db_path}\",\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "\n",
    "# Load from database later\n",
    "loaded = Benchmark.load_from_db(\n",
    "    benchmark_name=\"Genomics Knowledge Benchmark\",\n",
    "    storage=f\"sqlite:///{db_path}\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded from database: {loaded.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Example Script\n",
    "\n",
    "Here's the entire workflow in one script with both global and question-specific rubrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:04:58.614264Z",
     "iopub.status.busy": "2026-01-04T09:04:58.614203Z",
     "iopub.status.idle": "2026-01-04T09:04:59.762366Z",
     "shell.execute_reply": "2026-01-04T09:04:59.761881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results returned (this may happen in demo/mock mode)\n",
      "Done! Check results.csv for detailed results.\n"
     ]
    }
   ],
   "source": [
    "# Complete workflow example\n",
    "from karenina import Benchmark\n",
    "from karenina.schemas import (\n",
    "    VerificationConfig, ModelConfig, LLMRubricTrait,\n",
    "    RegexTrait, MetricRubricTrait, Rubric\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Create benchmark\n",
    "benchmark2 = Benchmark.create(\n",
    "    name=\"Genomics Quiz\",\n",
    "    description=\"Basic genomics knowledge test\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Your Name\"\n",
    ")\n",
    "\n",
    "# 2. Add questions\n",
    "questions = [\n",
    "    (\"How many chromosomes are in a human somatic cell?\", \"46\"),\n",
    "    (\"What is the approved drug target of Venetoclax?\", \"BCL2\"),\n",
    "    (\"How many protein subunits does hemoglobin A have?\", \"4\")\n",
    "]\n",
    "\n",
    "question_ids2 = []\n",
    "for q, a in questions:\n",
    "    qid = benchmark2.add_question(question=q, raw_answer=a, author={\"name\": \"Bio Curator\"})\n",
    "    question_ids2.append(qid)\n",
    "\n",
    "# Add a classification question for metric trait demonstration\n",
    "disease_qid2 = benchmark2.add_question(\n",
    "    question=\"Which of the following are inflammatory lung diseases: asthma, bronchitis, pneumonia, emphysema, pulmonary fibrosis?\",\n",
    "    raw_answer=\"asthma, bronchitis, pneumonia\",\n",
    "    author={\"name\": \"Bio Curator\"}\n",
    ")\n",
    "\n",
    "# 3. Generate templates\n",
    "# Note: generate_all_templates() takes individual parameters, not ModelConfig\n",
    "benchmark2.generate_all_templates(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    temperature=0.1,\n",
    "    interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# 4. Create global rubric (applies to ALL questions)\n",
    "# Note: Create a Rubric object and use set_global_rubric\n",
    "global_rubric = Rubric(\n",
    "    llm_traits=[\n",
    "        LLMRubricTrait(\n",
    "            name=\"Conciseness\",\n",
    "            description=\"Rate conciseness 1-5\",\n",
    "            kind=\"score\"\n",
    "        ),\n",
    "        LLMRubricTrait(\n",
    "            name=\"Clarity\",\n",
    "            description=\"Is the answer clear?\",\n",
    "            kind=\"boolean\"  # Use \"boolean\" not \"binary\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "benchmark2.set_global_rubric(global_rubric)\n",
    "\n",
    "# 5. Add question-specific rubrics\n",
    "# Regex trait for Venetoclax question\n",
    "drug_target_qid2 = [qid for qid in question_ids2 if \"Venetoclax\" in benchmark2.get_question(qid)['question']][0]\n",
    "benchmark2.add_question_rubric_trait(\n",
    "    question_id=drug_target_qid2,\n",
    "    trait=RegexTrait(\n",
    "        name=\"BCL2 Mention\",\n",
    "        description=\"Answer must mention BCL2\",\n",
    "        pattern=r\"\\bBCL2\\b\",\n",
    "        case_sensitive=False,\n",
    "        invert_result=False  # Use invert_result not invert\n",
    "    )\n",
    ")\n",
    "\n",
    "# Metric trait for disease classification question\n",
    "benchmark2.add_question_rubric_trait(\n",
    "    question_id=disease_qid2,\n",
    "    trait=MetricRubricTrait(\n",
    "        name=\"Inflammatory Disease ID\",\n",
    "        description=\"Evaluate disease classification accuracy\",\n",
    "        metrics=[\"precision\", \"recall\", \"f1\"],\n",
    "        tp_instructions=[\"asthma\", \"bronchitis\", \"pneumonia\"],\n",
    "        tn_instructions=[\"emphysema\", \"pulmonary fibrosis\"],  # Use tn_instructions not fp_instructions\n",
    "        repeated_extraction=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6. Run verification\n",
    "# Note: VerificationConfig DOES use ModelConfig objects\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.1,\n",
    "    interface=\"langchain\"\n",
    ")\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    evaluation_mode=\"template_and_rubric\",  # Required when rubric_enabled=True\n",
    "    rubric_enabled=True\n",
    ")\n",
    "results2 = benchmark2.run_verification(config)\n",
    "\n",
    "# 7. Analyze results\n",
    "# Note: results2 is a VerificationResultSet, not a dict - use .results to iterate\n",
    "if len(results2.results) > 0:\n",
    "    passed = sum(1 for r in results2.results if r.verify_result)\n",
    "    print(f\"Pass Rate: {(passed/len(results2.results)*100):.1f}%\")\n",
    "else:\n",
    "    print(\"No results returned (this may happen in demo/mock mode)\")\n",
    "\n",
    "# 8. Save and export\n",
    "checkpoint_path2 = temp_path(\"genomics_quiz.jsonld\")\n",
    "benchmark2.save(checkpoint_path2)\n",
    "benchmark2.export_verification_results_to_file(\n",
    "    file_path=temp_path(\"results.csv\"),\n",
    "    format=\"csv\"\n",
    ")\n",
    "\n",
    "print(\"Done! Check results.csv for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've completed your first benchmark, explore these guides:\n",
    "\n",
    "### Core Usage\n",
    "- [Defining Benchmarks](using-karenina/defining-benchmark.md) - Benchmark creation, metadata, and organization\n",
    "- [Adding Questions](using-karenina/adding-questions.md) - File extraction, metadata mapping, and management\n",
    "- [Templates](using-karenina/templates.md) - Creating and customizing answer templates\n",
    "- [Rubrics](using-karenina/rubrics.md) - Evaluation criteria and trait types\n",
    "- [Verification](using-karenina/verification.md) - Configuration, replication, and result analysis\n",
    "- [Saving & Loading](using-karenina/saving-loading.md) - Checkpoints, database persistence, and export\n",
    "\n",
    "### Advanced Features\n",
    "- [Deep-Judgment](advanced/deep-judgment.md) - Extract detailed feedback with excerpts and reasoning\n",
    "- [Few-Shot Prompting](advanced/few-shot.md) - Guide responses with examples\n",
    "- [Abstention Detection](advanced/abstention-detection.md) - Handle model refusals\n",
    "- [Embedding Check](advanced/embedding-check.md) - Semantic similarity fallback\n",
    "- [Presets](advanced/presets.md) - Save and reuse verification configurations\n",
    "- [System Integration](advanced/integration.md) - Server and GUI integration\n",
    "\n",
    "### Reference\n",
    "- [Features Overview](features.md) - Complete feature catalog\n",
    "- [Configuration](configuration.md) - Environment variables and defaults\n",
    "- [API Reference](api-reference.md) - Complete API documentation\n",
    "- [Troubleshooting](troubleshooting.md) - Common issues and solutions\n",
    "\n",
    "---\n",
    "\n",
    "## Tips for Success\n",
    "\n",
    "1. **Start simple**: Begin with a few questions and manual templates to understand the workflow\n",
    "2. **Use template generation**: Let Karenina generate templates automatically to save time\n",
    "3. **Iterate on templates**: Review and refine generated templates for better accuracy\n",
    "4. **Leverage rubrics**: Add rubrics to assess answer quality beyond correctness\n",
    "5. **Run replications**: Use `replicate_count > 1` for statistical analysis of model consistency\n",
    "6. **Save checkpoints**: Regularly save your benchmark to avoid losing work\n",
    "7. **Export results**: Use CSV export for easy analysis in spreadsheet tools\n",
    "\n",
    "---\n",
    "\n",
    "## Common Questions\n",
    "\n",
    "**Q: Do I need to write templates manually?**  \n",
    "A: No! Karenina can generate templates automatically using LLMs. Manual creation is only needed for complex custom logic.\n",
    "\n",
    "**Q: Can I use local models?**  \n",
    "A: Yes! Use the `openai_endpoint` interface with Ollama, vLLM, or any OpenAI-compatible server.\n",
    "\n",
    "**Q: How do I compare multiple models?**  \n",
    "A: Add multiple models to `answering_models` in your verification config. Karenina will test all of them.\n",
    "\n",
    "**Q: What's the difference between templates and rubrics?**  \n",
    "A: Templates verify **factual correctness** (e.g., \"Is the answer 'BCL2'?\"), while rubrics assess **qualitative traits** (e.g., \"Is the answer concise?\").\n",
    "\n",
    "**Q: Can I test without making API calls?**  \n",
    "A: Yes! Use the `manual` interface with pre-recorded traces for testing and debugging without costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
