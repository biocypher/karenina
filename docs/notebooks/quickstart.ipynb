{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f5c8a8",
   "metadata": {},
   "source": [
    "# What is Karenina?\n",
    "\n",
    "**Karenina** is a Python framework for defining, running, and sharing LLM benchmarks in a rigorous and reproducible way. It enables systematic evaluation of large language model performance through structured, verifiable testing.\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "- **Create benchmarks** from scratch or from existing question sets\n",
    "- **Define precise evaluation criteria** using code-based answer templates (Pydantic models)\n",
    "- **Evaluate answers** using both rule-based verification and LLM-as-judge strategies\n",
    "- **Support natural, unconstrained outputs** — no rigid response formats required\n",
    "- **Assess response quality** with rubrics (LLM judgment, regex, callable, and metric traits)\n",
    "- **Track performance** across multiple models and configurations\n",
    "- **Share and reproduce** benchmark results via JSON-LD checkpoint files\n",
    "\n",
    "## When to Use Karenina\n",
    "\n",
    "Karenina is designed for data scientists and ML engineers who need to:\n",
    "\n",
    "- **Compare models systematically** across consistent criteria, not ad-hoc prompting\n",
    "- **Go beyond simple string matching** — evaluate free-form LLM outputs with structured logic\n",
    "- **Combine correctness and quality checks** — verify factual accuracy *and* assess response qualities like clarity, safety, or format compliance\n",
    "- **Automate evaluation at scale** — run hundreds of questions across multiple models with a single configuration\n",
    "- **Reproduce results** — share benchmarks as portable JSON-LD files that anyone can re-run\n",
    "\n",
    "## Ecosystem Overview\n",
    "\n",
    "Karenina has three packages that work together:\n",
    "\n",
    "| Package | Type | Purpose |\n",
    "|---------|------|---------|\n",
    "| **karenina** | Python library | Core benchmarking framework (this documentation) |\n",
    "| **karenina-server** | FastAPI backend | REST API exposing karenina functionality |\n",
    "| **karenina-gui** | React/TypeScript | No-code web interface for benchmark management |\n",
    "\n",
    "This documentation covers the **karenina** Python library. The server and GUI have their own documentation.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Karenina uses a **two-unit evaluation approach**:\n",
    "\n",
    "1. **Answer Templates** verify *correctness* — did the model give the right answer? A Judge LLM parses the model's free-text response into a structured Pydantic schema, then a programmatic `verify()` method checks it against ground truth.\n",
    "\n",
    "2. **Rubrics** assess *quality* — how well did the model answer? Trait evaluators examine the raw response for qualities like safety, conciseness, format compliance, or extraction completeness.\n",
    "\n",
    "These two units are complementary. A common pattern: use a template to verify the model extracted the correct answer, then use rubrics to check that the response was concise, cited sources, and avoided hallucination.\n",
    "\n",
    "For a deeper discussion, see [Templates vs Rubrics](template-vs-rubric.md) and [Philosophy](philosophy.md).\n",
    "\n",
    "---\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "Here is a complete working example that loads a benchmark, configures verification, runs it, and inspects results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6adb5c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:30:20.567491Z",
     "iopub.status.busy": "2026-02-06T01:30:20.567278Z",
     "iopub.status.idle": "2026-02-06T01:30:20.895503Z",
     "shell.execute_reply": "2026-02-06T01:30:20.895264Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(self)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mock cell: patches run_verification so the quickstart executes without live API keys.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "from unittest.mock import patch\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationConfig, VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "def _mock_run_verification(self, config, question_ids=None, **kwargs):\n",
    "    \"\"\"Return realistic mock results for documentation examples.\"\"\"\n",
    "    qids = question_ids or self.get_question_ids()\n",
    "    mock_results = []\n",
    "    answers = {\n",
    "        \"capital of France\": (\"Paris\", True),\n",
    "        \"6 multiplied by 7\": (\"42\", True),\n",
    "        \"atomic number 8\": (\"Oxygen (O)\", True),\n",
    "        \"17 a prime\": (\"True\", True),\n",
    "        \"machine learning\": (\"Machine learning is a subset of AI\", None),\n",
    "    }\n",
    "    for qid in qids:\n",
    "        q = self.get_question(qid)\n",
    "        question_text = q[\"question\"]\n",
    "        # Match question to mock answer\n",
    "        response, verified = (\"Mock response\", True)\n",
    "        for key, (resp, ver) in answers.items():\n",
    "            if key in question_text.lower():\n",
    "                response, verified = resp, ver\n",
    "                break\n",
    "        answering = ModelIdentity(model_name=\"gpt-4o\", interface=\"langchain\")\n",
    "        parsing = ModelIdentity(model_name=\"gpt-4o\", interface=\"langchain\")\n",
    "        ts = datetime.datetime.now(tz=datetime.UTC).isoformat()\n",
    "        result_id = VerificationResultMetadata.compute_result_id(qid, answering, parsing, ts)\n",
    "        template_result = None\n",
    "        if verified is not None:\n",
    "            template_result = VerificationResultTemplate(\n",
    "                raw_llm_response=response,\n",
    "                verify_result=verified,\n",
    "                template_verification_performed=True,\n",
    "            )\n",
    "        result = VerificationResult(\n",
    "            metadata=VerificationResultMetadata(\n",
    "                question_id=qid,\n",
    "                template_id=\"mock_template\",\n",
    "                completed_without_errors=True,\n",
    "                question_text=question_text,\n",
    "                raw_answer=q.get(\"raw_answer\"),\n",
    "                answering=answering,\n",
    "                parsing=parsing,\n",
    "                execution_time=1.2,\n",
    "                timestamp=ts,\n",
    "                result_id=result_id,\n",
    "            ),\n",
    "            template=template_result,\n",
    "        )\n",
    "        mock_results.append(result)\n",
    "    return VerificationResultSet(results=mock_results)\n",
    "\n",
    "\n",
    "_patcher_run = patch(\n",
    "    \"karenina.benchmark.benchmark.Benchmark.run_verification\",\n",
    "    _mock_run_verification,\n",
    ")\n",
    "_patcher_validate = patch.object(VerificationConfig, \"_validate_config\", lambda self: None)\n",
    "_patcher_run.start()\n",
    "_patcher_validate.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2843b8",
   "metadata": {},
   "source": [
    "### Load a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ef703e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:30:20.896674Z",
     "iopub.status.busy": "2026-02-06T01:30:20.896598Z",
     "iopub.status.idle": "2026-02-06T01:30:20.898734Z",
     "shell.execute_reply": "2026-02-06T01:30:20.898522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'Documentation Test Benchmark' with 5 questions\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.load(\"test_checkpoint.jsonld\")\n",
    "print(f\"Loaded '{benchmark.name}' with {benchmark.question_count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b9f9e",
   "metadata": {},
   "source": [
    "### Configure and Run Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7251fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:30:20.899710Z",
     "iopub.status.busy": "2026-02-06T01:30:20.899653Z",
     "iopub.status.idle": "2026-02-06T01:30:20.901542Z",
     "shell.execute_reply": "2026-02-06T01:30:20.901336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 5 verifications\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas.config import ModelConfig\n",
    "from karenina.schemas.verification import VerificationConfig\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\")],\n",
    "    parsing_models=[ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\")],\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Completed {len(results.results)} verifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c456b",
   "metadata": {},
   "source": [
    "### Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799639dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:30:20.902501Z",
     "iopub.status.busy": "2026-02-06T01:30:20.902437Z",
     "iopub.status.idle": "2026-02-06T01:30:20.904107Z",
     "shell.execute_reply": "2026-02-06T01:30:20.903918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [PASS] What is the capital of France?\n",
      "  [PASS] What is 6 multiplied by 7?\n",
      "  [PASS] What element has the atomic number 8? Provide both the name \n",
      "  [PASS] Is 17 a prime number?\n",
      "  [N/A (no template)] Explain the concept of machine learning in simple terms.\n"
     ]
    }
   ],
   "source": [
    "for result in results.results:\n",
    "    q_text = result.metadata.question_text[:60]\n",
    "    if result.template and result.template.verify_result is not None:\n",
    "        status = \"PASS\" if result.template.verify_result else \"FAIL\"\n",
    "    else:\n",
    "        status = \"N/A (no template)\"\n",
    "    print(f\"  [{status}] {q_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1acbd6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:30:20.904985Z",
     "iopub.status.busy": "2026-02-06T01:30:20.904933Z",
     "iopub.status.idle": "2026-02-06T01:30:20.906366Z",
     "shell.execute_reply": "2026-02-06T01:30:20.906145Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up the mocks\n",
    "_ = _patcher_run.stop()\n",
    "_ = _patcher_validate.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d41f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Philosophy](philosophy.md) — Why LLM-as-judge evaluation works\n",
    "- [Templates vs Rubrics](template-vs-rubric.md) — Understanding the two evaluation units\n",
    "- [Installation](../02-installation/index.md) — Install karenina and set up API keys\n",
    "- [Core Concepts](../04-core-concepts/index.md) — Deep dive into checkpoints, templates, rubrics, and more"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
