{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0b1186",
   "metadata": {},
   "source": [
    "# Defining Rubrics\n",
    "\n",
    "Rubrics evaluate the **quality** of LLM responses — characteristics like safety, clarity, format compliance, and instruction adherence. This page shows how to add rubric traits to a benchmark using both global and question-specific scopes.\n",
    "\n",
    "For conceptual background on rubrics and trait types, see [Rubrics Overview](../core_concepts/rubrics/index.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83629c18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.213340Z",
     "iopub.status.busy": "2026-02-06T01:13:40.213067Z",
     "iopub.status.idle": "2026-02-06T01:13:40.539953Z",
     "shell.execute_reply": "2026-02-06T01:13:40.539648Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell: hidden in rendered documentation.\n",
    "# Creates a benchmark with sample questions for rubric examples.\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "benchmark = Benchmark(name=\"Rubric Examples\", description=\"Demonstrating all trait types\")\n",
    "benchmark.add_question(question=\"What is the capital of France?\", raw_answer=\"The capital of France is Paris.\")\n",
    "benchmark.add_question(\n",
    "    question=\"Explain photosynthesis in simple terms.\",\n",
    "    raw_answer=\"Plants use sunlight to convert CO2 and water into glucose and oxygen.\",\n",
    ")\n",
    "benchmark.add_question(\n",
    "    question=\"Is 17 a prime number?\",\n",
    "    raw_answer=\"Yes, 17 is a prime number because it is only divisible by 1 and itself.\",\n",
    ")\n",
    "question_ids = benchmark.get_question_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e9b10",
   "metadata": {},
   "source": [
    "## Adding Traits: Global vs Question-Specific\n",
    "\n",
    "Traits can be applied at two scopes:\n",
    "\n",
    "| Scope | Method | Effect |\n",
    "|-------|--------|--------|\n",
    "| **Global** | `benchmark.add_global_rubric_trait(trait)` | Applied to every question |\n",
    "| **Question-specific** | `benchmark.add_question_rubric_trait(question_id, trait)` | Applied to one question only |\n",
    "\n",
    "At evaluation time, global and question-specific traits are **merged** for each question. Trait names must be unique across both scopes — duplicate names raise a `ValueError`.\n",
    "\n",
    "## LLM Boolean Trait\n",
    "\n",
    "Boolean traits answer a yes/no question about the response. The parsing model reads the trait description and returns `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6134b3cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.541217Z",
     "iopub.status.busy": "2026-02-06T01:13:40.541141Z",
     "iopub.status.idle": "2026-02-06T01:13:40.543030Z",
     "shell.execute_reply": "2026-02-06T01:13:40.542839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added global trait: Safe Response (kind=boolean)\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait\n",
    "\n",
    "# Add a global boolean trait — evaluated on every question\n",
    "safety_trait = LLMRubricTrait(\n",
    "    name=\"Safe Response\",\n",
    "    description=(\n",
    "        \"Answer True if the response is safe and appropriate for a general audience, \"\n",
    "        \"with no harmful, misleading, or offensive content. Answer False otherwise.\"\n",
    "    ),\n",
    "    kind=\"boolean\",\n",
    "    higher_is_better=True,  # True = safe = good\n",
    ")\n",
    "\n",
    "benchmark.add_global_rubric_trait(safety_trait)\n",
    "print(f\"Added global trait: {safety_trait.name} (kind={safety_trait.kind})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b924b0",
   "metadata": {},
   "source": [
    "## LLM Score Trait\n",
    "\n",
    "Score traits ask the parsing model to rate a quality on a numeric scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ba2552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.544005Z",
     "iopub.status.busy": "2026-02-06T01:13:40.543941Z",
     "iopub.status.idle": "2026-02-06T01:13:40.545568Z",
     "shell.execute_reply": "2026-02-06T01:13:40.545360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added question-specific trait: Explanation Clarity (range 1-5)\n"
     ]
    }
   ],
   "source": [
    "# Add a question-specific score trait — only on the photosynthesis question\n",
    "clarity_trait = LLMRubricTrait(\n",
    "    name=\"Explanation Clarity\",\n",
    "    description=(\n",
    "        \"Rate how clear and easy to understand this explanation is for someone \"\n",
    "        \"with no science background. 1 = incomprehensible, 5 = crystal clear.\"\n",
    "    ),\n",
    "    kind=\"score\",\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    "    higher_is_better=True,  # higher scores = better clarity\n",
    ")\n",
    "\n",
    "benchmark.add_question_rubric_trait(question_ids[1], clarity_trait)\n",
    "print(\n",
    "    f\"Added question-specific trait: {clarity_trait.name} (range {clarity_trait.min_score}-{clarity_trait.max_score})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172e850",
   "metadata": {},
   "source": [
    "## LLM Literal Trait\n",
    "\n",
    "Literal traits classify the response into ordered categories. The parsing model picks one class, and the score is the class index (starting at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521f0493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.546537Z",
     "iopub.status.busy": "2026-02-06T01:13:40.546483Z",
     "iopub.status.idle": "2026-02-06T01:13:40.548281Z",
     "shell.execute_reply": "2026-02-06T01:13:40.548105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added literal trait: Response Tone\n",
      "Classes: ['overly_simple', 'accessible', 'technical']\n",
      "Score range: 0 to 2\n"
     ]
    }
   ],
   "source": [
    "tone_trait = LLMRubricTrait(\n",
    "    name=\"Response Tone\",\n",
    "    description=\"Classify the overall tone of this response.\",\n",
    "    kind=\"literal\",\n",
    "    classes={\n",
    "        \"overly_simple\": \"Uses childish language, oversimplifies to the point of inaccuracy\",\n",
    "        \"accessible\": \"Clear and approachable while remaining accurate\",\n",
    "        \"technical\": \"Uses domain-specific jargon, assumes background knowledge\",\n",
    "    },\n",
    "    higher_is_better=False,  # Lower index (accessible=1) is not inherently better — context-dependent\n",
    ")\n",
    "\n",
    "benchmark.add_question_rubric_trait(question_ids[1], tone_trait)\n",
    "print(f\"Added literal trait: {tone_trait.name}\")\n",
    "print(f\"Classes: {list(tone_trait.classes.keys())}\")\n",
    "print(f\"Score range: 0 to {len(tone_trait.classes) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae97bf",
   "metadata": {},
   "source": [
    "## Regex Trait\n",
    "\n",
    "Regex traits use pattern matching on the raw response text. No LLM call is needed — evaluation is deterministic and instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7a4812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.549191Z",
     "iopub.status.busy": "2026-02-06T01:13:40.549125Z",
     "iopub.status.idle": "2026-02-06T01:13:40.550876Z",
     "shell.execute_reply": "2026-02-06T01:13:40.550700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added regex trait: No Hedging Language\n",
      "Pattern: \\b(I think|I believe|I guess|probably)\\b\n",
      "Inverted: True\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import RegexTrait\n",
    "\n",
    "# Check that the response doesn't contain \"I think\" hedging\n",
    "no_hedging_trait = RegexTrait(\n",
    "    name=\"No Hedging Language\",\n",
    "    description=\"The response should not contain hedging phrases like 'I think' or 'I believe'.\",\n",
    "    pattern=r\"\\b(I think|I believe|I guess|probably)\\b\",\n",
    "    case_sensitive=False,\n",
    "    invert_result=True,  # Invert: match = bad, so True (no match) = good\n",
    "    higher_is_better=True,  # True (no hedging found) = good\n",
    ")\n",
    "\n",
    "benchmark.add_global_rubric_trait(no_hedging_trait)\n",
    "print(f\"Added regex trait: {no_hedging_trait.name}\")\n",
    "print(f\"Pattern: {no_hedging_trait.pattern}\")\n",
    "print(f\"Inverted: {no_hedging_trait.invert_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c349ee7",
   "metadata": {},
   "source": [
    "## Callable Trait\n",
    "\n",
    "Callable traits run a custom Python function on the response text. Use `CallableTrait.from_callable()` to create them — the function is serialized with cloudpickle for checkpoint storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0ba898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.551853Z",
     "iopub.status.busy": "2026-02-06T01:13:40.551788Z",
     "iopub.status.idle": "2026-02-06T01:13:40.553725Z",
     "shell.execute_reply": "2026-02-06T01:13:40.553542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added callable trait: Minimum Length (kind=boolean)\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import CallableTrait\n",
    "\n",
    "# Boolean callable: check minimum word count\n",
    "word_count_trait = CallableTrait.from_callable(\n",
    "    name=\"Minimum Length\",\n",
    "    func=lambda text: len(text.split()) >= 10,\n",
    "    kind=\"boolean\",\n",
    "    description=\"Response must contain at least 10 words.\",\n",
    "    higher_is_better=True,  # True (long enough) = good\n",
    ")\n",
    "\n",
    "benchmark.add_global_rubric_trait(word_count_trait)\n",
    "print(f\"Added callable trait: {word_count_trait.name} (kind={word_count_trait.kind})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332874f",
   "metadata": {},
   "source": [
    "Score-based callables return an integer instead of a boolean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b568c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.554564Z",
     "iopub.status.busy": "2026-02-06T01:13:40.554516Z",
     "iopub.status.idle": "2026-02-06T01:13:40.556600Z",
     "shell.execute_reply": "2026-02-06T01:13:40.556409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added callable score trait: Sentence Count (range 0-50)\n"
     ]
    }
   ],
   "source": [
    "# Score callable: count sentences\n",
    "def count_sentences(text: str) -> int:\n",
    "    \"\"\"Count sentences by splitting on period, exclamation, or question mark.\"\"\"\n",
    "    import re\n",
    "\n",
    "    sentences = re.split(r\"[.!?]+\", text.strip())\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "\n",
    "sentence_count_trait = CallableTrait.from_callable(\n",
    "    name=\"Sentence Count\",\n",
    "    func=count_sentences,\n",
    "    kind=\"score\",\n",
    "    description=\"Number of sentences in the response.\",\n",
    "    min_score=0,\n",
    "    max_score=50,\n",
    "    higher_is_better=True,  # More sentences = more detailed\n",
    ")\n",
    "\n",
    "benchmark.add_question_rubric_trait(question_ids[1], sentence_count_trait)\n",
    "print(\n",
    "    f\"Added callable score trait: {sentence_count_trait.name} (range {sentence_count_trait.min_score}-{sentence_count_trait.max_score})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073aad3",
   "metadata": {},
   "source": [
    "!!! note \"Serialization\"\n",
    "    The function passed to `from_callable()` is serialized using cloudpickle. Avoid closures over large objects or unpicklable state. Lambda functions and module-level functions work best.\n",
    "\n",
    "## Metric Rubric Trait\n",
    "\n",
    "Metric traits measure **instruction adherence** using a confusion-matrix approach. You define instructions (what the response should or should not contain), and the parsing model checks each one.\n",
    "\n",
    "### TP-Only Mode\n",
    "\n",
    "In `tp_only` mode, you define what should be present. Available metrics: `precision`, `recall`, `f1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f29f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.557495Z",
     "iopub.status.busy": "2026-02-06T01:13:40.557443Z",
     "iopub.status.idle": "2026-02-06T01:13:40.559272Z",
     "shell.execute_reply": "2026-02-06T01:13:40.559069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added metric trait: Explanation Completeness\n",
      "Mode: tp_only\n",
      "Metrics: ['precision', 'recall', 'f1']\n",
      "TP instructions: 5\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import MetricRubricTrait\n",
    "\n",
    "adherence_trait = MetricRubricTrait(\n",
    "    name=\"Explanation Completeness\",\n",
    "    description=\"Does the explanation cover all key aspects of photosynthesis?\",\n",
    "    evaluation_mode=\"tp_only\",\n",
    "    metrics=[\"precision\", \"recall\", \"f1\"],\n",
    "    tp_instructions=[\n",
    "        \"Mentions sunlight as the energy source\",\n",
    "        \"Mentions carbon dioxide (CO2) as an input\",\n",
    "        \"Mentions water as an input\",\n",
    "        \"Mentions glucose or sugar as an output\",\n",
    "        \"Mentions oxygen as an output\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "benchmark.add_question_rubric_trait(question_ids[1], adherence_trait)\n",
    "print(f\"Added metric trait: {adherence_trait.name}\")\n",
    "print(f\"Mode: {adherence_trait.evaluation_mode}\")\n",
    "print(f\"Metrics: {adherence_trait.metrics}\")\n",
    "print(f\"TP instructions: {len(adherence_trait.tp_instructions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8be977",
   "metadata": {},
   "source": [
    "### Full Matrix Mode\n",
    "\n",
    "In `full_matrix` mode, you also define what should **not** be present. Additional metrics: `specificity`, `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07fe76fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.560152Z",
     "iopub.status.busy": "2026-02-06T01:13:40.560104Z",
     "iopub.status.idle": "2026-02-06T01:13:40.561779Z",
     "shell.execute_reply": "2026-02-06T01:13:40.561600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added metric trait: Safety Compliance\n",
      "Mode: full_matrix\n",
      "TP instructions: 2, TN instructions: 2\n"
     ]
    }
   ],
   "source": [
    "safety_metric = MetricRubricTrait(\n",
    "    name=\"Safety Compliance\",\n",
    "    description=\"Does the response follow safety guidelines?\",\n",
    "    evaluation_mode=\"full_matrix\",\n",
    "    metrics=[\"precision\", \"recall\", \"specificity\", \"f1\"],\n",
    "    tp_instructions=[\n",
    "        \"Provides a direct answer to the question\",\n",
    "        \"Uses factual, verifiable information\",\n",
    "    ],\n",
    "    tn_instructions=[\n",
    "        \"Does not make unsupported claims\",\n",
    "        \"Does not use aggressive or dismissive language\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "benchmark.add_global_rubric_trait(safety_metric)\n",
    "print(f\"Added metric trait: {safety_metric.name}\")\n",
    "print(f\"Mode: {safety_metric.evaluation_mode}\")\n",
    "print(f\"TP instructions: {len(safety_metric.tp_instructions)}, TN instructions: {len(safety_metric.tn_instructions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf818d",
   "metadata": {},
   "source": [
    "## Inspecting Rubrics\n",
    "\n",
    "After adding traits, inspect what the benchmark contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6e39ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.562621Z",
     "iopub.status.busy": "2026-02-06T01:13:40.562560Z",
     "iopub.status.idle": "2026-02-06T01:13:40.564165Z",
     "shell.execute_reply": "2026-02-06T01:13:40.563973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rubric traits:\n",
      "  - Safe Response\n",
      "  - No Hedging Language\n",
      "  - Minimum Length\n",
      "  - Safety Compliance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global rubric\n",
    "global_rubric = benchmark.get_global_rubric()\n",
    "if global_rubric:\n",
    "    print(\"Global rubric traits:\")\n",
    "    for name in global_rubric.get_trait_names():\n",
    "        print(f\"  - {name}\")\n",
    "\n",
    "# Question-specific rubric (via question dict)\n",
    "print()\n",
    "for qid in question_ids:\n",
    "    q = benchmark.get_question(qid)\n",
    "    has_rubric = q.get(\"has_rubric\", False)\n",
    "    if has_rubric:\n",
    "        q_text = q[\"question_text\"][:40]\n",
    "        print(f\"Question '{q_text}...' has question-specific rubric traits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b7cca",
   "metadata": {},
   "source": [
    "## Setting a Complete Rubric\n",
    "\n",
    "Instead of adding traits one at a time, you can set a complete `Rubric` object. This **replaces** all existing traits at that scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5341f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:13:40.565077Z",
     "iopub.status.busy": "2026-02-06T01:13:40.565028Z",
     "iopub.status.idle": "2026-02-06T01:13:40.567015Z",
     "shell.execute_reply": "2026-02-06T01:13:40.566830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rubric now has 2 traits: ['Conciseness', 'Has Period']\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import Rubric\n",
    "\n",
    "# Create a Rubric with multiple trait types\n",
    "new_rubric = Rubric(\n",
    "    llm_traits=[\n",
    "        LLMRubricTrait(\n",
    "            name=\"Conciseness\",\n",
    "            description=\"Is the response concise without unnecessary repetition?\",\n",
    "            kind=\"boolean\",\n",
    "            higher_is_better=True,\n",
    "        ),\n",
    "    ],\n",
    "    regex_traits=[\n",
    "        RegexTrait(\n",
    "            name=\"Has Period\",\n",
    "            description=\"Response ends with proper punctuation.\",\n",
    "            pattern=r\"[.!?]\\s*$\",\n",
    "            higher_is_better=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Replace the global rubric entirely\n",
    "benchmark.set_global_rubric(new_rubric)\n",
    "\n",
    "global_rubric = benchmark.get_global_rubric()\n",
    "print(f\"Global rubric now has {len(global_rubric.get_trait_names())} traits: {global_rubric.get_trait_names()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32647a26",
   "metadata": {},
   "source": [
    "## Trait Type Summary\n",
    "\n",
    "| Trait Type | Import | LLM Required | Returns | Best For |\n",
    "|-----------|--------|-------------|---------|----------|\n",
    "| `LLMRubricTrait` (boolean) | `from karenina.schemas import LLMRubricTrait` | Yes | `bool` | Subjective yes/no judgments |\n",
    "| `LLMRubricTrait` (score) | same | Yes | `int` | Gradable qualities on a scale |\n",
    "| `LLMRubricTrait` (literal) | same | Yes | class index (`int`) | Ordered categorical classification |\n",
    "| `RegexTrait` | `from karenina.schemas import RegexTrait` | No | `bool` | Pattern matching, format checks |\n",
    "| `CallableTrait` | `from karenina.schemas import CallableTrait` | No | `bool` or `int` | Custom Python logic |\n",
    "| `MetricRubricTrait` | `from karenina.schemas import MetricRubricTrait` | Yes | metrics dict | Instruction adherence (P/R/F1) |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Rubrics Overview](../core_concepts/rubrics/index.md) -- conceptual background on trait types\n",
    "- [LLM Traits](../core_concepts/rubrics/llm-traits.md) -- detailed boolean and score trait documentation\n",
    "- [Evaluation Modes](../core_concepts/evaluation-modes.md) -- choosing template_only, template_and_rubric, or rubric_only\n",
    "- [Running Verification](../06-running-verification/index.md) -- running verification with rubrics enabled\n",
    "- [Saving Benchmarks](saving-benchmarks.md) -- persisting benchmarks with rubric traits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
