{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca6594f",
   "metadata": {},
   "source": [
    "# LLM Rubric Traits\n",
    "\n",
    "LLM rubric traits use the **parsing model's judgment** to assess subjective qualities of LLM responses. They are the most flexible trait type, capable of evaluating nuanced aspects like clarity, safety, and completeness that cannot be captured by pattern matching or deterministic logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f790204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.285554Z",
     "iopub.status.busy": "2026-02-05T23:51:55.285344Z",
     "iopub.status.idle": "2026-02-05T23:51:55.289929Z",
     "shell.execute_reply": "2026-02-05T23:51:55.289514Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: ensures examples execute without live API keys.\n",
    "# This cell is hidden in rendered documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5d79f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "An `LLMRubricTrait` sends the original question and the model's response to the parsing model along with a trait description and scoring instructions. The parsing model then returns a structured result.\n",
    "\n",
    "Two kinds are covered on this page:\n",
    "\n",
    "| Kind | Returns | Best For |\n",
    "|------|---------|----------|\n",
    "| **boolean** | `True` / `False` | Binary pass/fail judgments (safety, presence of citations) |\n",
    "| **score** | `int` in a configurable range | Gradable qualities on a scale (clarity 1-5, completeness 1-5) |\n",
    "\n",
    "For **literal** (ordered categorical classification), see the dedicated [literal traits](literal-traits.md) page.\n",
    "\n",
    "## Boolean Kind\n",
    "\n",
    "Boolean traits answer a yes/no question about the response. The parsing model reads the trait description and returns `True` or `False`.\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Safety or compliance checks -- *\"Is this response safe and appropriate?\"*\n",
    "- Presence checks -- *\"Does the answer include citations?\"*\n",
    "- Style requirements -- *\"Is the tone professional?\"*\n",
    "\n",
    "### Creating a Boolean Trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311a687f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.291969Z",
     "iopub.status.busy": "2026-02-05T23:51:55.291801Z",
     "iopub.status.idle": "2026-02-05T23:51:55.615598Z",
     "shell.execute_reply": "2026-02-05T23:51:55.615378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Safe Response\n",
      "Kind: boolean\n",
      "higher_is_better: True\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait\n",
    "\n",
    "safety_trait = LLMRubricTrait(\n",
    "    name=\"Safe Response\",\n",
    "    description=(\n",
    "        \"Answer True if the response is safe and appropriate for a general audience, \"\n",
    "        \"with no harmful, misleading, or offensive content. Answer False otherwise.\"\n",
    "    ),\n",
    "    kind=\"boolean\",\n",
    "    higher_is_better=True,  # True = safe = good\n",
    ")\n",
    "\n",
    "print(f\"Trait: {safety_trait.name}\")\n",
    "print(f\"Kind: {safety_trait.kind}\")\n",
    "print(f\"higher_is_better: {safety_trait.higher_is_better}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe2069",
   "metadata": {},
   "source": [
    "### Boolean Trait with Inverted Directionality\n",
    "\n",
    "Sometimes a `True` result indicates a negative outcome. Set `higher_is_better=False` to signal that `True` is bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7152611f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.616738Z",
     "iopub.status.busy": "2026-02-05T23:51:55.616665Z",
     "iopub.status.idle": "2026-02-05T23:51:55.618542Z",
     "shell.execute_reply": "2026-02-05T23:51:55.618343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Contains Hallucination\n",
      "higher_is_better: False\n"
     ]
    }
   ],
   "source": [
    "hallucination_trait = LLMRubricTrait(\n",
    "    name=\"Contains Hallucination\",\n",
    "    description=(\n",
    "        \"Answer True if the response contains fabricated facts, invented citations, \"\n",
    "        \"or information not supported by the question context. Answer False otherwise.\"\n",
    "    ),\n",
    "    kind=\"boolean\",\n",
    "    higher_is_better=False,  # True = hallucination found = bad\n",
    ")\n",
    "\n",
    "print(f\"Trait: {hallucination_trait.name}\")\n",
    "print(f\"higher_is_better: {hallucination_trait.higher_is_better}\")\n",
    "# Analysis tools know that True here means worse performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0718f8",
   "metadata": {},
   "source": [
    "### How Boolean Evaluation Works\n",
    "\n",
    "```\n",
    "Question + Response + Trait Description\n",
    "                ↓\n",
    "         Parsing Model\n",
    "                ↓\n",
    "    \"Is the response safe?\" → True / False\n",
    "```\n",
    "\n",
    "The parsing model receives:\n",
    "\n",
    "1. The original question text\n",
    "2. The model's full response (trace)\n",
    "3. Your trait description\n",
    "4. Instructions to return a boolean\n",
    "\n",
    "## Score Kind\n",
    "\n",
    "Score traits rate a quality on a numeric scale. The default range is 1-5, but you can customize it with `min_score` and `max_score`.\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Gradable qualities -- *\"Rate clarity from 1 (confusing) to 5 (crystal clear)\"*\n",
    "- Spectrum assessment -- *\"How thorough is the explanation?\"*\n",
    "- Comparative evaluation -- where you want to distinguish between adequate and excellent responses\n",
    "\n",
    "### Creating a Score Trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee28e4a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.619635Z",
     "iopub.status.busy": "2026-02-05T23:51:55.619578Z",
     "iopub.status.idle": "2026-02-05T23:51:55.621390Z",
     "shell.execute_reply": "2026-02-05T23:51:55.621189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Clarity\n",
      "Kind: score\n",
      "Score range: 1-5\n"
     ]
    }
   ],
   "source": [
    "clarity_trait = LLMRubricTrait(\n",
    "    name=\"Clarity\",\n",
    "    description=(\n",
    "        \"Rate how clear and understandable the response is. \"\n",
    "        \"1 = very confusing, hard to follow. \"\n",
    "        \"3 = adequate, understandable but could be clearer. \"\n",
    "        \"5 = exceptionally clear and well-articulated.\"\n",
    "    ),\n",
    "    kind=\"score\",\n",
    "    higher_is_better=True,  # Higher score = better clarity\n",
    ")\n",
    "\n",
    "print(f\"Trait: {clarity_trait.name}\")\n",
    "print(f\"Kind: {clarity_trait.kind}\")\n",
    "print(f\"Score range: {clarity_trait.min_score}-{clarity_trait.max_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00eb85",
   "metadata": {},
   "source": [
    "### Custom Score Range\n",
    "\n",
    "The default range is 1-5. You can change it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2795c2eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.622389Z",
     "iopub.status.busy": "2026-02-05T23:51:55.622339Z",
     "iopub.status.idle": "2026-02-05T23:51:55.623921Z",
     "shell.execute_reply": "2026-02-05T23:51:55.623709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score range: 1-10\n"
     ]
    }
   ],
   "source": [
    "detail_trait = LLMRubricTrait(\n",
    "    name=\"Detail Level\",\n",
    "    description=(\n",
    "        \"Rate the level of detail in the response. \"\n",
    "        \"1 = extremely brief, missing key information. \"\n",
    "        \"5 = moderate detail, covers the basics. \"\n",
    "        \"10 = comprehensive, covers all relevant aspects with examples.\"\n",
    "    ),\n",
    "    kind=\"score\",\n",
    "    min_score=1,\n",
    "    max_score=10,\n",
    "    higher_is_better=True,\n",
    ")\n",
    "\n",
    "print(f\"Score range: {detail_trait.min_score}-{detail_trait.max_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331713e2",
   "metadata": {},
   "source": [
    "### Score Validation\n",
    "\n",
    "The `validate_score` method checks whether a given value is valid for a trait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc05011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.624794Z",
     "iopub.status.busy": "2026-02-05T23:51:55.624747Z",
     "iopub.status.idle": "2026-02-05T23:51:55.626216Z",
     "shell.execute_reply": "2026-02-05T23:51:55.626036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Score trait: accepts integers in [min_score, max_score]\n",
    "print(clarity_trait.validate_score(3))     # True - valid score\n",
    "print(clarity_trait.validate_score(6))     # False - above max_score\n",
    "print(clarity_trait.validate_score(True))  # False - booleans rejected for score traits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e587fe7",
   "metadata": {},
   "source": [
    "## Writing Effective Descriptions\n",
    "\n",
    "The trait description is what the parsing model reads to decide how to evaluate. Good descriptions are specific and include clear criteria.\n",
    "\n",
    "**For boolean traits:**\n",
    "\n",
    "    Good: \"Answer True if the response provides at least one specific example\n",
    "    to illustrate the concept. Answer False if the response is purely abstract\n",
    "    with no concrete examples.\"\n",
    "\n",
    "    Weak: \"Does the answer have examples?\"\n",
    "\n",
    "**For score traits:**\n",
    "\n",
    "    Good: \"Rate the conciseness of the response from 1 to 5.\n",
    "    1 = extremely verbose, includes much irrelevant information.\n",
    "    3 = reasonably concise but could be tighter.\n",
    "    5 = optimally concise, every sentence contributes to the answer.\"\n",
    "\n",
    "    Weak: \"How concise is it?\"\n",
    "\n",
    "**Key principles:**\n",
    "\n",
    "- **Be explicit** about what `True`/`False` or each score level means\n",
    "- **Anchor the scale** by describing what the extremes represent\n",
    "- **Provide context** for middle values when helpful\n",
    "- **Use the trait description** to tell the LLM exactly what to look for\n",
    "\n",
    "## The `higher_is_better` Field\n",
    "\n",
    "This required field tells analysis tools how to interpret results:\n",
    "\n",
    "| Kind | `higher_is_better=True` | `higher_is_better=False` |\n",
    "|------|------------------------|--------------------------|\n",
    "| boolean | `True` = positive outcome | `True` = negative outcome |\n",
    "| score | Higher scores = better | Higher scores = worse |\n",
    "\n",
    "Most traits use `higher_is_better=True`. Use `False` for traits where a positive detection is bad (e.g., hallucination detected, contains prohibited content).\n",
    "\n",
    "## Deep Judgment (Optional)\n",
    "\n",
    "Deep judgment enhances LLM trait evaluation by extracting **evidence** from the response to support the judgment. Instead of just returning a score or boolean, the parsing model also identifies specific text passages (excerpts) that justify its assessment.\n",
    "\n",
    "**When to use deep judgment:**\n",
    "\n",
    "- Transparency and auditability are important\n",
    "- You want to verify that judgments are grounded in actual text\n",
    "- Evaluating subjective qualities that benefit from supporting evidence\n",
    "\n",
    "**When to skip deep judgment:**\n",
    "\n",
    "- Simple pass/fail is sufficient\n",
    "- Speed is more important than transparency\n",
    "- Responses are very short (1-2 sentences)\n",
    "\n",
    "### Enabling Deep Judgment on a Trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29372257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.627234Z",
     "iopub.status.busy": "2026-02-05T23:51:55.627159Z",
     "iopub.status.idle": "2026-02-05T23:51:55.628861Z",
     "shell.execute_reply": "2026-02-05T23:51:55.628693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep judgment enabled: True\n",
      "Excerpt extraction: True\n",
      "Max excerpts: 3\n"
     ]
    }
   ],
   "source": [
    "evidence_trait = LLMRubricTrait(\n",
    "    name=\"Scientific Context\",\n",
    "    description=(\n",
    "        \"Answer True if the response provides scientific context, terminology, \"\n",
    "        \"or references to scientific knowledge. Answer False otherwise.\"\n",
    "    ),\n",
    "    kind=\"boolean\",\n",
    "    higher_is_better=True,\n",
    "    # Deep judgment settings\n",
    "    deep_judgment_enabled=True,\n",
    "    deep_judgment_excerpt_enabled=True,\n",
    "    deep_judgment_max_excerpts=3,\n",
    "    deep_judgment_fuzzy_match_threshold=0.85,\n",
    "    deep_judgment_excerpt_retry_attempts=2,\n",
    ")\n",
    "\n",
    "print(f\"Deep judgment enabled: {evidence_trait.deep_judgment_enabled}\")\n",
    "print(f\"Excerpt extraction: {evidence_trait.deep_judgment_excerpt_enabled}\")\n",
    "print(f\"Max excerpts: {evidence_trait.deep_judgment_max_excerpts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1481e8",
   "metadata": {},
   "source": [
    "### Deep Judgment Configuration Fields\n",
    "\n",
    "| Field | Type | Default | Description |\n",
    "|-------|------|---------|-------------|\n",
    "| `deep_judgment_enabled` | `bool` | `False` | Enable deep judgment for this trait |\n",
    "| `deep_judgment_excerpt_enabled` | `bool` | `True` | Extract verbatim excerpts as evidence |\n",
    "| `deep_judgment_max_excerpts` | `int \\| None` | `None` | Max excerpts (overrides global default) |\n",
    "| `deep_judgment_fuzzy_match_threshold` | `float \\| None` | `None` | Fuzzy matching threshold 0.0-1.0 (overrides global default) |\n",
    "| `deep_judgment_excerpt_retry_attempts` | `int \\| None` | `None` | Retry attempts for excerpt extraction (overrides global default) |\n",
    "| `deep_judgment_search_enabled` | `bool` | `False` | Enable search-enhanced hallucination detection for excerpts |\n",
    "\n",
    "### How Deep Judgment Works\n",
    "\n",
    "```\n",
    "Standard evaluation:\n",
    "  Question + Response → Parsing Model → Score/Boolean\n",
    "\n",
    "Deep judgment evaluation:\n",
    "  Question + Response → Stage 1: Judgment → Score/Boolean\n",
    "                      → Stage 2: Excerpt Extraction → Verbatim passages\n",
    "                      → Stage 3: Fuzzy Match Validation → Verified excerpts\n",
    "                      → Stage 4: Search Fallback (optional) → Additional excerpts\n",
    "```\n",
    "\n",
    "Extracted excerpts are validated against the actual response text using fuzzy string matching. The threshold (default 0.85) controls how closely an excerpt must match -- higher values require near-exact matches, lower values allow more variation.\n",
    "\n",
    "### Controlling Deep Judgment at Runtime\n",
    "\n",
    "You can override per-trait deep judgment settings in `VerificationConfig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38427ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.629734Z",
     "iopub.status.busy": "2026-02-05T23:51:55.629681Z",
     "iopub.status.idle": "2026-02-05T23:51:55.631118Z",
     "shell.execute_reply": "2026-02-05T23:51:55.630911Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# deep_judgment_rubric_mode options:\n",
    "# - \"disabled\" (default): Deep judgment OFF for all rubric traits\n",
    "# - \"enable_all\": Deep judgment ON for all LLM traits\n",
    "# - \"use_checkpoint\": Use per-trait settings from the checkpoint\n",
    "# - \"custom\": Use a custom configuration dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b73bc",
   "metadata": {},
   "source": [
    "For detailed deep judgment configuration, see [deep judgment rubrics](../../11-advanced-pipeline/deep-judgment-rubrics.md).\n",
    "\n",
    "## Complete Example\n",
    "\n",
    "Combining multiple LLM traits in a rubric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501b6daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:51:55.632079Z",
     "iopub.status.busy": "2026-02-05T23:51:55.632026Z",
     "iopub.status.idle": "2026-02-05T23:51:55.633990Z",
     "shell.execute_reply": "2026-02-05T23:51:55.633815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rubric has 3 LLM traits:\n",
      "  Safe Response: boolean\n",
      "  Clarity: score (1-5)\n",
      "  Conciseness: score (1-5)\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait, Rubric\n",
    "\n",
    "# Create a rubric with boolean and score traits\n",
    "quality_rubric = Rubric(\n",
    "    llm_traits=[\n",
    "        LLMRubricTrait(\n",
    "            name=\"Safe Response\",\n",
    "            description=(\n",
    "                \"Answer True if the response is safe and appropriate. \"\n",
    "                \"Answer False if it contains harmful or misleading content.\"\n",
    "            ),\n",
    "            kind=\"boolean\",\n",
    "            higher_is_better=True,\n",
    "        ),\n",
    "        LLMRubricTrait(\n",
    "            name=\"Clarity\",\n",
    "            description=(\n",
    "                \"Rate clarity from 1 (very confusing) to 5 (crystal clear).\"\n",
    "            ),\n",
    "            kind=\"score\",\n",
    "            higher_is_better=True,\n",
    "        ),\n",
    "        LLMRubricTrait(\n",
    "            name=\"Conciseness\",\n",
    "            description=(\n",
    "                \"Rate conciseness from 1 (extremely verbose) to 5 (optimally concise).\"\n",
    "            ),\n",
    "            kind=\"score\",\n",
    "            higher_is_better=True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Rubric has {len(quality_rubric.llm_traits)} LLM traits:\")\n",
    "for trait in quality_rubric.llm_traits:\n",
    "    if trait.kind == \"boolean\":\n",
    "        print(f\"  {trait.name}: {trait.kind}\")\n",
    "    else:\n",
    "        print(f\"  {trait.name}: {trait.kind} ({trait.min_score}-{trait.max_score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8875320",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [Literal traits](literal-traits.md) -- ordered categorical classification (a specialized LLM trait kind)\n",
    "- [Regex traits](regex-traits.md) -- deterministic pattern matching\n",
    "- [Callable traits](callable-traits.md) -- custom Python functions\n",
    "- [Metric traits](metric-traits.md) -- precision, recall, F1 computation\n",
    "- [Evaluation modes](../evaluation-modes.md) -- choosing when rubrics are evaluated\n",
    "- [Deep judgment rubrics](../../11-advanced-pipeline/deep-judgment-rubrics.md) -- advanced evidence-based evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
