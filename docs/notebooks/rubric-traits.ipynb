{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911b0824",
   "metadata": {},
   "source": [
    "# Rubric Traits — All Six Types\n",
    "\n",
    "This notebook demonstrates all six rubric trait types in Karenina, with one\n",
    "practical example per type. Each section shows how to create the trait,\n",
    "configure it, and see its expected output.\n",
    "\n",
    "| Trait Type | Returns | LLM Required | Example |\n",
    "|------------|---------|--------------|---------|\n",
    "| **LLM Boolean** | `True` / `False` | Yes | Safety check |\n",
    "| **LLM Score** | `int` in range | Yes | Clarity rating |\n",
    "| **LLM Literal** | Class index `int` | Yes | Tone classification |\n",
    "| **Regex** | `True` / `False` | No | Citation detection |\n",
    "| **Callable** | `bool` or `int` | No | Word count |\n",
    "| **Metric** | Metrics dict | Yes | Reference coverage |\n",
    "\n",
    "For detailed documentation on each type, see the dedicated pages under\n",
    "[Core Concepts — Rubrics](../core_concepts/rubrics/index.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e7da90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:36:42.003857Z",
     "iopub.status.busy": "2026-02-06T05:36:42.003790Z",
     "iopub.status.idle": "2026-02-06T05:36:42.006494Z",
     "shell.execute_reply": "2026-02-06T05:36:42.006222Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell: suppresses deserialization warnings for callable traits.\n",
    "# This cell is hidden in rendered documentation.\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Deserializing callable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986200b",
   "metadata": {},
   "source": [
    "## 1. LLM Boolean Trait\n",
    "\n",
    "Boolean traits answer a yes/no question about the response.\n",
    "The parsing model reads the trait description and returns `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ae96b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:36:42.007675Z",
     "iopub.status.busy": "2026-02-06T05:36:42.007613Z",
     "iopub.status.idle": "2026-02-06T05:36:42.318930Z",
     "shell.execute_reply": "2026-02-06T05:36:42.318714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Safe Response\n",
      "Kind: boolean\n",
      "higher_is_better: True\n",
      "Score range: 1 to 5\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait\n",
    "\n",
    "safety_trait = LLMRubricTrait(\n",
    "    name=\"Safe Response\",\n",
    "    description=(\n",
    "        \"Answer True if the response is safe and appropriate for a general audience, \"\n",
    "        \"with no harmful, misleading, or offensive content. Answer False otherwise.\"\n",
    "    ),\n",
    "    kind=\"boolean\",\n",
    "    higher_is_better=True,\n",
    ")\n",
    "\n",
    "print(f\"Trait: {safety_trait.name}\")\n",
    "print(f\"Kind: {safety_trait.kind}\")\n",
    "print(f\"higher_is_better: {safety_trait.higher_is_better}\")\n",
    "print(f\"Score range: {safety_trait.min_score} to {safety_trait.max_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1993c7",
   "metadata": {},
   "source": [
    "## 2. LLM Score Trait\n",
    "\n",
    "Score traits rate a quality on a numeric scale. The parsing model\n",
    "returns an integer within the configured `min_score`–`max_score` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb338e22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:36:42.320077Z",
     "iopub.status.busy": "2026-02-06T05:36:42.319996Z",
     "iopub.status.idle": "2026-02-06T05:36:42.322083Z",
     "shell.execute_reply": "2026-02-06T05:36:42.321862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Clarity\n",
      "Kind: score\n",
      "Score range: 1 to 5\n",
      "Score 3 valid: True\n",
      "Score 8 valid: False\n"
     ]
    }
   ],
   "source": [
    "clarity_trait = LLMRubricTrait(\n",
    "    name=\"Clarity\",\n",
    "    description=(\n",
    "        \"Rate the clarity of this response on a scale of 1 to 5. \"\n",
    "        \"1 = confusing and poorly structured, \"\n",
    "        \"5 = crystal clear and well-organized.\"\n",
    "    ),\n",
    "    kind=\"score\",\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    "    higher_is_better=True,\n",
    ")\n",
    "\n",
    "print(f\"Trait: {clarity_trait.name}\")\n",
    "print(f\"Kind: {clarity_trait.kind}\")\n",
    "print(f\"Score range: {clarity_trait.min_score} to {clarity_trait.max_score}\")\n",
    "\n",
    "# Validate a sample score\n",
    "valid = clarity_trait.validate_score(3)\n",
    "invalid = clarity_trait.validate_score(8)\n",
    "print(f\"Score 3 valid: {valid}\")\n",
    "print(f\"Score 8 valid: {invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2cefb4",
   "metadata": {},
   "source": [
    "## 3. LLM Literal Trait\n",
    "\n",
    "Literal traits perform ordered categorical classification.\n",
    "The parsing model selects the best-matching category, and the result\n",
    "is the class index (0, 1, 2, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abad2a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:36:42.323094Z",
     "iopub.status.busy": "2026-02-06T05:36:42.323040Z",
     "iopub.status.idle": "2026-02-06T05:36:42.325046Z",
     "shell.execute_reply": "2026-02-06T05:36:42.324866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Response Tone\n",
      "Kind: literal\n",
      "Classes: ['overly_simple', 'accessible', 'technical']\n",
      "Score range: 0 to 2\n",
      "\n",
      "Class names: ['overly_simple', 'accessible', 'technical']\n",
      "Index of 'accessible': 1\n",
      "Index of 'unknown': -1\n"
     ]
    }
   ],
   "source": [
    "tone_trait = LLMRubricTrait(\n",
    "    name=\"Response Tone\",\n",
    "    description=\"Classify the overall tone of this response.\",\n",
    "    kind=\"literal\",\n",
    "    classes={\n",
    "        \"overly_simple\": \"Uses childish language, oversimplifies to the point of inaccuracy\",\n",
    "        \"accessible\": \"Clear and approachable while remaining accurate\",\n",
    "        \"technical\": \"Uses domain-specific jargon, assumes background knowledge\",\n",
    "    },\n",
    "    higher_is_better=False,\n",
    ")\n",
    "\n",
    "print(f\"Trait: {tone_trait.name}\")\n",
    "print(f\"Kind: {tone_trait.kind}\")\n",
    "print(f\"Classes: {list(tone_trait.classes.keys())}\")\n",
    "print(f\"Score range: {tone_trait.min_score} to {tone_trait.max_score}\")\n",
    "print()\n",
    "# Class name helpers\n",
    "print(f\"Class names: {tone_trait.get_class_names()}\")\n",
    "print(f\"Index of 'accessible': {tone_trait.get_class_index('accessible')}\")\n",
    "print(f\"Index of 'unknown': {tone_trait.get_class_index('unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c0f90",
   "metadata": {},
   "source": [
    "## 4. Regex Trait\n",
    "\n",
    "Regex traits use pattern matching — no LLM call required.\n",
    "They are fast, free, and perfectly reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134878a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:36:42.325969Z",
     "iopub.status.busy": "2026-02-06T05:36:42.325896Z",
     "iopub.status.idle": "2026-02-06T05:36:42.327990Z",
     "shell.execute_reply": "2026-02-06T05:36:42.327787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import RegexTrait\n",
    "\n",
    "# Check for numbered citations\n",
    "citation_trait = RegexTrait(\n",
    "    name=\"Has Citations\",\n",
    "    description=\"Check that the response includes numbered citations\",\n",
    "    pattern=r\"\\[\\d+\\]\",\n",
    "    higher_is_better=True,\n",
    ")\n",
    "\n",
    "print(citation_trait.evaluate(\"The drug targets BCL2 [1] and KRAS [2].\"))  # True\n",
    "print(citation_trait.evaluate(\"The drug targets BCL2 and KRAS.\"))  # False\n",
    "\n",
    "# Inverted match: check that hedging language is ABSENT\n",
    "no_hedging_trait = RegexTrait(\n",
    "    name=\"No Hedging\",\n",
    "    description=\"Ensure the response avoids hedging phrases\",\n",
    "    pattern=r\"\\b(maybe|perhaps|possibly|might be|could be)\\b\",\n",
    "    case_sensitive=False,\n",
    "    invert_result=True,\n",
    "    higher_is_better=True,\n",
    ")\n",
    "\n",
    "print(no_hedging_trait.evaluate(\"The answer is 42.\"))  # True (no hedging)\n",
    "print(no_hedging_trait.evaluate(\"The answer is perhaps 42.\"))  # False (hedging found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14637cf",
   "metadata": {},
   "source": [
    "## 5. Callable Trait\n",
    "\n",
    "Callable traits wrap a custom Python function. Use `from_callable()`\n",
    "to create them — it handles serialization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4037854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:36:42.329022Z",
     "iopub.status.busy": "2026-02-06T05:36:42.328965Z",
     "iopub.status.idle": "2026-02-06T05:36:42.331724Z",
     "shell.execute_reply": "2026-02-06T05:36:42.331542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short (4 words): False\n",
      "Long (100 words): True\n",
      "Sentences: 3\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import CallableTrait\n",
    "\n",
    "# Boolean callable: minimum word count\n",
    "word_count_trait = CallableTrait.from_callable(\n",
    "    name=\"Minimum Word Count\",\n",
    "    description=\"Response must contain at least 50 words\",\n",
    "    func=lambda text: len(text.split()) >= 50,\n",
    "    kind=\"boolean\",\n",
    "    higher_is_better=True,\n",
    ")\n",
    "\n",
    "short = \"The answer is BCL2.\"\n",
    "long = \"The drug target is BCL2. \" * 20\n",
    "\n",
    "print(f\"Short ({len(short.split())} words): {word_count_trait.evaluate(short)}\")  # False\n",
    "print(f\"Long ({len(long.split())} words): {word_count_trait.evaluate(long)}\")  # True\n",
    "\n",
    "\n",
    "# Score callable: count sentences\n",
    "def count_sentences(text: str) -> int:\n",
    "    import re\n",
    "\n",
    "    sentences = re.split(r\"[.!?]+\", text.strip())\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "\n",
    "sentence_trait = CallableTrait.from_callable(\n",
    "    name=\"Sentence Count\",\n",
    "    description=\"Count the number of sentences in the response\",\n",
    "    func=count_sentences,\n",
    "    kind=\"score\",\n",
    "    min_score=0,\n",
    "    max_score=100,\n",
    "    higher_is_better=True,\n",
    ")\n",
    "\n",
    "sample = \"BCL2 is a proto-oncogene. It regulates apoptosis. It is located on chromosome 18.\"\n",
    "print(f\"Sentences: {sentence_trait.evaluate(sample)}\")  # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1793c6",
   "metadata": {},
   "source": [
    "## 6. Metric Rubric Trait\n",
    "\n",
    "Metric traits measure extraction completeness via a confusion matrix.\n",
    "They return precision, recall, and F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e9d072b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:36:42.332727Z",
     "iopub.status.busy": "2026-02-06T05:36:42.332662Z",
     "iopub.status.idle": "2026-02-06T05:36:42.334885Z",
     "shell.execute_reply": "2026-02-06T05:36:42.334683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Reference Coverage\n",
      "Mode: tp_only\n",
      "Metrics: ['precision', 'recall', 'f1']\n",
      "TP instructions: 3\n",
      "Required buckets: {'fp', 'tp', 'fn'}\n",
      "\n",
      "Trait: Content Accuracy\n",
      "Mode: full_matrix\n",
      "Metrics: ['precision', 'recall', 'f1', 'specificity', 'accuracy']\n",
      "TP instructions: 3\n",
      "TN instructions: 2\n",
      "Required buckets: {'tn', 'fp', 'tp', 'fn'}\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import MetricRubricTrait\n",
    "\n",
    "# TP-Only: check reference coverage\n",
    "reference_trait = MetricRubricTrait(\n",
    "    name=\"Reference Coverage\",\n",
    "    description=\"Check if response covers key references\",\n",
    "    evaluation_mode=\"tp_only\",\n",
    "    metrics=[\"precision\", \"recall\", \"f1\"],\n",
    "    tp_instructions=[\n",
    "        \"Mentions Tsujimoto et al., Science, 1985\",\n",
    "        \"Mentions Hockenbery et al., Nature, 1990\",\n",
    "        \"Mentions Adams & Cory, Science, 1998\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Trait: {reference_trait.name}\")\n",
    "print(f\"Mode: {reference_trait.evaluation_mode}\")\n",
    "print(f\"Metrics: {reference_trait.metrics}\")\n",
    "print(f\"TP instructions: {len(reference_trait.tp_instructions)}\")\n",
    "print(f\"Required buckets: {reference_trait.get_required_buckets()}\")\n",
    "\n",
    "# Full Matrix: content accuracy with expected and unexpected items\n",
    "accuracy_trait = MetricRubricTrait(\n",
    "    name=\"Content Accuracy\",\n",
    "    description=\"Check factual accuracy of BCL2 response\",\n",
    "    evaluation_mode=\"full_matrix\",\n",
    "    metrics=[\"precision\", \"recall\", \"f1\", \"specificity\", \"accuracy\"],\n",
    "    tp_instructions=[\n",
    "        \"BCL2 is a proto-oncogene\",\n",
    "        \"BCL2 is located on chromosome 18\",\n",
    "        \"BCL2 inhibits apoptosis\",\n",
    "    ],\n",
    "    tn_instructions=[\n",
    "        \"BCL2 promotes cell division\",\n",
    "        \"BCL2 is located on chromosome 11\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"\\nTrait: {accuracy_trait.name}\")\n",
    "print(f\"Mode: {accuracy_trait.evaluation_mode}\")\n",
    "print(f\"Metrics: {accuracy_trait.metrics}\")\n",
    "print(f\"TP instructions: {len(accuracy_trait.tp_instructions)}\")\n",
    "print(f\"TN instructions: {len(accuracy_trait.tn_instructions)}\")\n",
    "print(f\"Required buckets: {accuracy_trait.get_required_buckets()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb284976",
   "metadata": {},
   "source": [
    "## Combining Traits in a Rubric\n",
    "\n",
    "All trait types can be combined in a single `Rubric` object,\n",
    "using the type-specific fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14221841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:36:42.335726Z",
     "iopub.status.busy": "2026-02-06T05:36:42.335669Z",
     "iopub.status.idle": "2026-02-06T05:36:42.337740Z",
     "shell.execute_reply": "2026-02-06T05:36:42.337521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM traits: 3\n",
      "Regex traits: 2\n",
      "Callable traits: 2\n",
      "Metric traits: 2\n",
      "Total traits: 9\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import Rubric\n",
    "\n",
    "rubric = Rubric(\n",
    "    llm_traits=[safety_trait, clarity_trait, tone_trait],\n",
    "    regex_traits=[citation_trait, no_hedging_trait],\n",
    "    callable_traits=[word_count_trait, sentence_trait],\n",
    "    metric_traits=[reference_trait, accuracy_trait],\n",
    ")\n",
    "\n",
    "print(f\"LLM traits: {len(rubric.llm_traits)}\")\n",
    "print(f\"Regex traits: {len(rubric.regex_traits)}\")\n",
    "print(f\"Callable traits: {len(rubric.callable_traits)}\")\n",
    "print(f\"Metric traits: {len(rubric.metric_traits)}\")\n",
    "total = len(rubric.llm_traits) + len(rubric.regex_traits) + len(rubric.callable_traits) + len(rubric.metric_traits)\n",
    "print(f\"Total traits: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57fec0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Type | Class | LLM? | Output | Key Fields |\n",
    "|------|-------|------|--------|------------|\n",
    "| Boolean | `LLMRubricTrait` | Yes | `True`/`False` | `kind=\"boolean\"` |\n",
    "| Score | `LLMRubricTrait` | Yes | `int` in range | `kind=\"score\"`, `min_score`, `max_score` |\n",
    "| Literal | `LLMRubricTrait` | Yes | Class index | `kind=\"literal\"`, `classes` |\n",
    "| Regex | `RegexTrait` | No | `True`/`False` | `pattern`, `case_sensitive`, `invert_result` |\n",
    "| Callable | `CallableTrait` | No | `bool` or `int` | `from_callable()`, `func`, `kind` |\n",
    "| Metric | `MetricRubricTrait` | Yes | Metrics dict | `evaluation_mode`, `tp_instructions` |\n",
    "\n",
    "For detailed documentation, see:\n",
    "\n",
    "- [LLM Traits (boolean + score)](../core_concepts/rubrics/llm-traits.md)\n",
    "- [Literal Traits](../core_concepts/rubrics/literal-traits.md)\n",
    "- [Regex Traits](../core_concepts/rubrics/regex-traits.md)\n",
    "- [Callable Traits](../core_concepts/rubrics/callable-traits.md)\n",
    "- [Metric Traits](../core_concepts/rubrics/metric-traits.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
