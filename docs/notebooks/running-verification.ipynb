{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29ad841",
   "metadata": {},
   "source": [
    "# Running Verification: End-to-End Workflow\n",
    "\n",
    "This notebook walks through the complete verification workflow — from loading a\n",
    "benchmark through configuration, running verification, and inspecting results.\n",
    "\n",
    "For conceptual background, see the [Running Verification](../06-running-verification/index.md) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9623f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.497837Z",
     "iopub.status.busy": "2026-02-06T05:29:53.497631Z",
     "iopub.status.idle": "2026-02-06T05:29:53.827638Z",
     "shell.execute_reply": "2026-02-06T05:29:53.827415Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: patches run_verification so examples execute without live API keys.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationConfig, VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "# Change to notebooks directory so test_checkpoint.jsonld is found\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "\n",
    "def _mock_run_verification(self, config, question_ids=None, **kwargs):  # noqa: ARG001\n",
    "    \"\"\"Return realistic mock results for documentation examples.\"\"\"\n",
    "    qids = question_ids or self.get_question_ids()\n",
    "    mock_results = []\n",
    "    answers = {\n",
    "        \"capital of France\": (\"Paris\", True),\n",
    "        \"6 multiplied by 7\": (\"42\", True),\n",
    "        \"atomic number 8\": (\"Oxygen (O)\", True),\n",
    "        \"17 a prime\": (\"True\", True),\n",
    "        \"machine learning\": (\"Machine learning is a subset of AI\", None),\n",
    "    }\n",
    "    for qid in qids:\n",
    "        q = self.get_question(qid)\n",
    "        question_text = q[\"question\"]\n",
    "        response, verified = (\"Mock response\", True)\n",
    "        for key, (resp, ver) in answers.items():\n",
    "            if key in question_text.lower():\n",
    "                response, verified = resp, ver\n",
    "                break\n",
    "        answering = ModelIdentity(model_name=\"gpt-4o\", interface=\"langchain\")\n",
    "        parsing = ModelIdentity(model_name=\"gpt-4o\", interface=\"langchain\")\n",
    "        ts = datetime.datetime.now(tz=datetime.UTC).isoformat()\n",
    "        result_id = VerificationResultMetadata.compute_result_id(qid, answering, parsing, ts)\n",
    "        template_result = None\n",
    "        if verified is not None:\n",
    "            template_result = VerificationResultTemplate(\n",
    "                raw_llm_response=response,\n",
    "                verify_result=verified,\n",
    "                template_verification_performed=True,\n",
    "            )\n",
    "        rubric_result = None\n",
    "        if \"capital\" in question_text.lower():\n",
    "            rubric_result = VerificationResultRubric(\n",
    "                rubric_evaluation_performed=True,\n",
    "                llm_trait_scores={\"Is the response concise?\": True},\n",
    "            )\n",
    "        result = VerificationResult(\n",
    "            metadata=VerificationResultMetadata(\n",
    "                question_id=qid,\n",
    "                template_id=\"mock_template\" if verified is not None else \"no_template\",\n",
    "                completed_without_errors=True,\n",
    "                question_text=question_text,\n",
    "                raw_answer=q.get(\"raw_answer\"),\n",
    "                answering=answering,\n",
    "                parsing=parsing,\n",
    "                execution_time=1.2,\n",
    "                timestamp=ts,\n",
    "                result_id=result_id,\n",
    "            ),\n",
    "            template=template_result,\n",
    "            rubric=rubric_result,\n",
    "        )\n",
    "        mock_results.append(result)\n",
    "    return VerificationResultSet(results=mock_results)\n",
    "\n",
    "\n",
    "_patcher_run = patch(\n",
    "    \"karenina.benchmark.benchmark.Benchmark.run_verification\",\n",
    "    _mock_run_verification,\n",
    ")\n",
    "_patcher_validate = patch.object(\n",
    "    VerificationConfig,\n",
    "    \"_validate_config\",\n",
    "    lambda self: None,  # noqa: ARG005\n",
    ")\n",
    "_patcher_run.start()\n",
    "_patcher_validate.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc8fc4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load a Benchmark\n",
    "\n",
    "Load a benchmark from a JSON-LD checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39874e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.828828Z",
     "iopub.status.busy": "2026-02-06T05:29:53.828742Z",
     "iopub.status.idle": "2026-02-06T05:29:53.830936Z",
     "shell.execute_reply": "2026-02-06T05:29:53.830736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: Documentation Test Benchmark\n",
      "Questions: 5\n",
      "Complete:  False\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.load(\"test_checkpoint.jsonld\")\n",
    "\n",
    "print(f\"Benchmark: {benchmark.name}\")\n",
    "print(f\"Questions: {benchmark.question_count}\")\n",
    "print(f\"Complete:  {benchmark.is_complete}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58c740",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Configure Verification\n",
    "\n",
    "Create a `VerificationConfig` to control which models to use, evaluation mode,\n",
    "and optional pipeline features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb8563e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.831922Z",
     "iopub.status.busy": "2026-02-06T05:29:53.831862Z",
     "iopub.status.idle": "2026-02-06T05:29:53.833375Z",
     "shell.execute_reply": "2026-02-06T05:29:53.833207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation mode: template_and_rubric\n",
      "Rubric enabled:  True\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas.config import ModelConfig\n",
    "from karenina.schemas.verification import VerificationConfig\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\"),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"gpt-4o\", model_name=\"gpt-4o\", interface=\"langchain\"),\n",
    "    ],\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    rubric_enabled=True,\n",
    ")\n",
    "\n",
    "print(f\"Evaluation mode: {config.evaluation_mode}\")\n",
    "print(f\"Rubric enabled:  {config.rubric_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb146ab",
   "metadata": {},
   "source": [
    "For simpler setups, use the `from_overrides` convenience method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6584a200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.834286Z",
     "iopub.status.busy": "2026-02-06T05:29:53.834230Z",
     "iopub.status.idle": "2026-02-06T05:29:53.835770Z",
     "shell.execute_reply": "2026-02-06T05:29:53.835583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick config mode: template_only\n"
     ]
    }
   ],
   "source": [
    "quick_config = VerificationConfig.from_overrides(\n",
    "    answering_id=\"gpt-4o\",\n",
    "    answering_model=\"gpt-4o\",\n",
    "    parsing_id=\"gpt-4o\",\n",
    "    parsing_model=\"gpt-4o\",\n",
    ")\n",
    "print(f\"Quick config mode: {quick_config.evaluation_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bd152",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Run Verification\n",
    "\n",
    "### Full Run\n",
    "\n",
    "Run verification across all questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30464799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.836726Z",
     "iopub.status.busy": "2026-02-06T05:29:53.836667Z",
     "iopub.status.idle": "2026-02-06T05:29:53.838199Z",
     "shell.execute_reply": "2026-02-06T05:29:53.838012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 5 verifications across 5 questions\n"
     ]
    }
   ],
   "source": [
    "results = benchmark.run_verification(config)\n",
    "print(f\"Completed: {len(results)} verifications across {benchmark.question_count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4bf982",
   "metadata": {},
   "source": [
    "### Partial Run\n",
    "\n",
    "Verify only a subset of questions by passing `question_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f00f91ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.839069Z",
     "iopub.status.busy": "2026-02-06T05:29:53.839017Z",
     "iopub.status.idle": "2026-02-06T05:29:53.840571Z",
     "shell.execute_reply": "2026-02-06T05:29:53.840376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified 2 of 5 questions\n"
     ]
    }
   ],
   "source": [
    "question_ids = benchmark.get_question_ids()[:2]\n",
    "partial_results = benchmark.run_verification(config, question_ids=question_ids)\n",
    "print(f\"Verified {len(partial_results)} of {benchmark.question_count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031bafb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Inspect Results\n",
    "\n",
    "`run_verification()` returns a `VerificationResultSet` — a container with\n",
    "filtering, grouping, and analysis methods.\n",
    "\n",
    "### Iterating Over Results\n",
    "\n",
    "Each result is a `VerificationResult` with template (correctness) and\n",
    "rubric (quality) sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b549735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.841428Z",
     "iopub.status.busy": "2026-02-06T05:29:53.841380Z",
     "iopub.status.idle": "2026-02-06T05:29:53.843081Z",
     "shell.execute_reply": "2026-02-06T05:29:53.842892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [PASS] What is the capital of France? | rubric traits: 1\n",
      "  [PASS] What is 6 multiplied by 7?\n",
      "  [PASS] What element has the atomic number 8? Provide both\n",
      "  [PASS] Is 17 a prime number?\n",
      "  [N/A] Explain the concept of machine learning in simple \n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    meta = result.metadata\n",
    "    q_text = meta.question_text[:50]\n",
    "\n",
    "    # Template result (correctness)\n",
    "    if result.template and result.template.verify_result is not None:\n",
    "        status = \"PASS\" if result.template.verify_result else \"FAIL\"\n",
    "    else:\n",
    "        status = \"N/A\"\n",
    "\n",
    "    # Rubric result (quality)\n",
    "    rubric_info = \"\"\n",
    "    if result.rubric and result.rubric.rubric_evaluation_performed:\n",
    "        scores = result.rubric.llm_trait_scores or {}\n",
    "        rubric_info = f\" | rubric traits: {len(scores)}\"\n",
    "\n",
    "    print(f\"  [{status}] {q_text}{rubric_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f887bf",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "478673d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.843997Z",
     "iopub.status.busy": "2026-02-06T05:29:53.843933Z",
     "iopub.status.idle": "2026-02-06T05:29:53.845579Z",
     "shell.execute_reply": "2026-02-06T05:29:53.845406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  5\n",
      "Completed:      5\n",
      "With template:  4\n",
      "With rubric:    1\n",
      "Unique models:  1\n"
     ]
    }
   ],
   "source": [
    "summary = results.get_summary()\n",
    "print(f\"Total results:  {summary['num_results']}\")\n",
    "print(f\"Completed:      {summary['num_completed']}\")\n",
    "print(f\"With template:  {summary['num_with_template']}\")\n",
    "print(f\"With rubric:    {summary['num_with_rubric']}\")\n",
    "print(f\"Unique models:  {summary['num_models']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc30474",
   "metadata": {},
   "source": [
    "### Filtering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c7d65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.846474Z",
     "iopub.status.busy": "2026-02-06T05:29:53.846427Z",
     "iopub.status.idle": "2026-02-06T05:29:53.847837Z",
     "shell.execute_reply": "2026-02-06T05:29:53.847663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 4 results with template verification\n"
     ]
    }
   ],
   "source": [
    "filtered = results.filter(completed_only=True, has_template=True)\n",
    "print(f\"Filtered: {len(filtered)} results with template verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0062d5d",
   "metadata": {},
   "source": [
    "### Grouping Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61157e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.848732Z",
     "iopub.status.busy": "2026-02-06T05:29:53.848683Z",
     "iopub.status.idle": "2026-02-06T05:29:53.850239Z",
     "shell.execute_reply": "2026-02-06T05:29:53.850059Z"
    }
   },
   "outputs": [],
   "source": [
    "by_question = results.group_by_question()\n",
    "for _qid, group in by_question.items():\n",
    "    first = group.results[0]\n",
    "    q_text = first.metadata.question_text[:40]\n",
    "    print(f\"  {q_text}: {len(group)} result(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a5a12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered the complete verification workflow:\n",
    "\n",
    "| Step | What | Key API |\n",
    "|------|------|---------|\n",
    "| 1 | Load benchmark | `Benchmark.load()` |\n",
    "| 2 | Configure | `VerificationConfig()` or `.from_overrides()` |\n",
    "| 3 | Run verification | `benchmark.run_verification(config)` |\n",
    "| 4 | Inspect results | Iterate, `.get_summary()`, `.filter()`, `.group_by_question()` |\n",
    "\n",
    "For more details, see:\n",
    "- [VerificationConfig](../06-running-verification/verification-config.md) — Full configuration tutorial\n",
    "- [Analyzing Results](../07-analyzing-results/index.md) — DataFrame analysis and export\n",
    "- [CLI Verification](../06-running-verification/cli.md) — Running from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c99f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:29:53.851143Z",
     "iopub.status.busy": "2026-02-06T05:29:53.851078Z",
     "iopub.status.idle": "2026-02-06T05:29:53.852513Z",
     "shell.execute_reply": "2026-02-06T05:29:53.852280Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up the mocks\n",
    "_ = _patcher_run.stop()\n",
    "_ = _patcher_validate.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
