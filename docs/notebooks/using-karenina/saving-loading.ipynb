{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unittest.mock import Mock, MagicMock, patch, PropertyMock\n",
    "from typing import Any, Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    "    VerificationResultRubric,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # Set common attributes with realistic defaults\n",
    "        self.count = kwargs.get('count', 46)\n",
    "        self.target = kwargs.get('target', 'BCL2')\n",
    "        self.subunits = kwargs.get('subunits', 4)\n",
    "        self.diseases = kwargs.get('diseases', ['asthma', 'bronchitis', 'pneumonia'])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "def create_mock_verification_result(question_id: str, question_text: str, answer: str, passed: bool = True):\n",
    "    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "\n",
    "    # Create mock template result\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=f\"The answer is {answer}.\",\n",
    "        parsed_llm_response={\"value\": answer},\n",
    "        parsed_gt_response={\"value\": answer},\n",
    "        verify_result=passed,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80}\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "    )\n",
    "\n",
    "    # Create mock rubric result\n",
    "    rubric = VerificationResultRubric(\n",
    "        rubric_evaluation_performed=True,\n",
    "        llm_trait_scores={\n",
    "            \"Conciseness\": 4,\n",
    "            \"Clarity\": True,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create metadata with all required fields\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=answer,\n",
    "        answering_model=\"gpt-4.1-mini\",\n",
    "        parsing_model=\"gpt-4.1-mini\",\n",
    "        execution_time=1.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n",
    "    )\n",
    "\n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "# Store original run_verification\n",
    "_original_run_verification = None\n",
    "\n",
    "def mock_run_verification(self, config):\n",
    "    \"\"\"Mock run_verification that returns realistic results.\"\"\"\n",
    "    global _original_run_verification\n",
    "\n",
    "    # Get all finished questions\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "\n",
    "    if len(finished) == 0:\n",
    "        if _original_run_verification:\n",
    "            return _original_run_verification(self, config)\n",
    "        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "\n",
    "    results = []\n",
    "    # Map question keywords to expected answers\n",
    "    mock_data = [\n",
    "        {\"keywords\": [\"chromosomes\"], \"answer\": \"46\", \"passed\": True},\n",
    "        {\"keywords\": [\"venetoclax\", \"bcl2\"], \"answer\": \"BCL2\", \"passed\": True},\n",
    "        {\"keywords\": [\"hemoglobin\", \"subunits\"], \"answer\": \"4\", \"passed\": True},\n",
    "        {\"keywords\": [\"inflammatory\", \"lung\"], \"answer\": \"asthma, bronchitis, pneumonia\", \"passed\": True},\n",
    "    ]\n",
    "\n",
    "    for question in finished:\n",
    "        q_id = question['id']\n",
    "        q_text = question['question']\n",
    "        raw_answer = question.get('raw_answer', '')\n",
    "\n",
    "        passed = True\n",
    "        mock_ans = raw_answer\n",
    "        q_text_lower = q_text.lower()\n",
    "\n",
    "        for data in mock_data:\n",
    "            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n",
    "                passed = data[\"passed\"]\n",
    "                mock_ans = data[\"answer\"]\n",
    "                break\n",
    "\n",
    "        results.append(create_mock_verification_result(\n",
    "            question_id=q_id,\n",
    "            question_text=q_text,\n",
    "            answer=mock_ans,\n",
    "            passed=passed\n",
    "        ))\n",
    "\n",
    "    template_results = TemplateResults(results=results)\n",
    "\n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch('langchain_openai.ChatOpenAI', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('langchain_anthropic.ChatAnthropic', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('langchain_google_genai.ChatGoogleGenerativeAI', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('karenina.infrastructure.llm.interface.init_chat_model_unified', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Patch Benchmark.run_verification\n",
    "from karenina.benchmark import Benchmark\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(f\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(f\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(f\"✓ Mock verification results enabled - examples will show realistic output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Benchmarks\n",
    "\n",
    "This guide covers how to persist, restore, and export benchmarks using Karenina's checkpoint and database systems.\n",
    "\n",
    "## Understanding Persistence\n",
    "\n",
    "Karenina provides two main approaches for persisting benchmarks:\n",
    "\n",
    "1. **Checkpoints (JSON-LD files)**: Portable, human-readable files perfect for sharing and version control\n",
    "2. **Database storage (SQLite)**: Structured storage with query capabilities for production use\n",
    "\n",
    "You can use both approaches together: databases for primary storage and checkpoints for backups and sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Files (JSON-LD)\n",
    "\n",
    "Checkpoints are JSON-LD files that capture the complete state of a benchmark.\n",
    "\n",
    "### What Gets Saved\n",
    "\n",
    "A checkpoint includes:\n",
    "- Benchmark metadata (name, description, version)\n",
    "- All questions with their metadata\n",
    "- Answer templates\n",
    "- Rubrics (global and question-specific)\n",
    "- Verification results (if available)\n",
    "\n",
    "### JSON-LD Format\n",
    "\n",
    "Karenina uses **JSON-LD (JSON for Linked Data)** format following schema.org conventions:\n",
    "\n",
    "**Benefits:**\n",
    "- **Structured and semantic**: Machine-readable with clear data relationships\n",
    "- **Human-readable**: Open in any text editor to inspect contents\n",
    "- **Cross-platform**: Works across different environments\n",
    "- **Version-compatible**: Maintains backward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Checkpoints\n",
    "\n",
    "### Basic Save\n",
    "\n",
    "Save your benchmark to a JSON-LD checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from karenina import Benchmark\n",
    "\n",
    "# Basic save\n",
    "# benchmark.save(Path(\"genomics_benchmark.jsonld\"))\n",
    "\n",
    "# Save to specific directory\n",
    "# benchmark.save(Path(\"benchmarks/genomics_benchmark.jsonld\"))\n",
    "\n",
    "print(\"Code example: Saving checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens When You Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "# Create and populate benchmark\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\",\n",
    "    raw_answer=\"46\",\n",
    "    author={\"name\": \"Bio Curator\"}\n",
    ")\n",
    "\n",
    "benchmark.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\",\n",
    "    raw_answer=\"BCL2\",\n",
    "    author={\"name\": \"Bio Curator\"}\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = temp_path(\"genomics_benchmark.jsonld\")\n",
    "benchmark.save(checkpoint_path)\n",
    "\n",
    "print(f\"✓ Saved checkpoint to {checkpoint_path.name}\")\n",
    "print(f\"  Questions: {benchmark.question_count}\")\n",
    "print(f\"  Size: {checkpoint_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "✓ Saved checkpoint to genomics_benchmark.jsonld\n",
    "  Questions: 2\n",
    "  Size: 4532 bytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Checkpoints\n",
    "\n",
    "### Basic Load\n",
    "\n",
    "Load a benchmark from a checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from pathlib import Path\n",
    "\n",
    "# Load benchmark\n",
    "benchmark = Benchmark.load(checkpoint_path)\n",
    "\n",
    "print(f\"Loaded benchmark: {benchmark.name}\")\n",
    "print(f\"Version: {benchmark.version}\")\n",
    "print(f\"Questions: {benchmark.question_count}\")\n",
    "\n",
    "# Access questions\n",
    "question_ids = benchmark.get_question_ids()\n",
    "for qid in question_ids[:3]:\n",
    "    question = benchmark.get_question(qid)\n",
    "    print(f\"  • {question['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Loaded benchmark: Genomics Knowledge Benchmark\n",
    "Version: 1.0.0\n",
    "Questions: 2\n",
    "  • How many chromosomes are in a human somatic ce...\n",
    "  • What is the approved drug target of Venetoclax...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_verify(checkpoint_path: Path):\n",
    "    \"\"\"Load benchmark with validation\"\"\"\n",
    "    try:\n",
    "        benchmark = Benchmark.load(checkpoint_path)\n",
    "\n",
    "        # Basic validation\n",
    "        assert benchmark.question_count > 0, \"No questions found\"\n",
    "        assert benchmark.name, \"Missing benchmark name\"\n",
    "\n",
    "        print(f\"✓ Successfully loaded: {benchmark.name}\")\n",
    "        print(f\"  Questions: {benchmark.question_count}\")\n",
    "\n",
    "        # Check templates\n",
    "        all_questions = benchmark.get_all_questions(ids_only=False)\n",
    "        questions_with_templates = sum(\n",
    "            1 for q in all_questions\n",
    "            if q.get('answer_template') is not None\n",
    "        )\n",
    "        print(f\"  Templates: {questions_with_templates}/{benchmark.question_count}\")\n",
    "\n",
    "        return benchmark\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load with validation\n",
    "benchmark = load_and_verify(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Storage\n",
    "\n",
    "Database storage provides structured persistence with query capabilities.\n",
    "\n",
    "### Quick Database Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark, DBConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add questions\n",
    "benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\",\n",
    "    raw_answer=\"46\"\n",
    ")\n",
    "\n",
    "# Save to database with checkpoint backup\n",
    "db_path = temp_path(\"benchmarks.db\")\n",
    "checkpoint_db_path = temp_path(\"genomics_from_db.jsonld\")\n",
    "\n",
    "benchmark.save_to_db(\n",
    "    storage=f\"sqlite:///{db_path}\",\n",
    "    checkpoint_path=checkpoint_db_path\n",
    ")\n",
    "\n",
    "print(\"✓ Saved to database and checkpoint\")\n",
    "\n",
    "# Load from database\n",
    "loaded = Benchmark.load_from_db(\n",
    "    benchmark_name=\"Genomics Knowledge Benchmark\",\n",
    "    storage=f\"sqlite:///{db_path}\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded from database: {loaded.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Database vs Checkpoints\n",
    "\n",
    "| Use Case | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| **Development and prototyping** | Checkpoints only |\n",
    "| **Sharing benchmarks** | Checkpoints (portable files) |\n",
    "| **Production deployment** | Database primary, checkpoints for backup |\n",
    "| **Version control (Git)** | Checkpoints (diff-friendly) |\n",
    "| **Multi-user collaboration** | Database with checkpoint backups |\n",
    "| **Query and analytics** | Database |\n",
    "| **Backups** | Checkpoints |\n",
    "\n",
    "**Best Practice:** Use both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Save to database for primary storage\n",
    "# benchmark.save_to_db(\"sqlite:///production.db\")\n",
    "\n",
    "# Also save checkpoint for backup/sharing\n",
    "# benchmark.save(Path(\"backups/genomics_v1.0.0.jsonld\"))\n",
    "\n",
    "print(\"Code example: Using both database and checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Database Storage During Verification\n",
    "\n",
    "Karenina can automatically save verification results to a database as they are generated. This is especially useful for long-running verification jobs where you want results persisted immediately.\n",
    "\n",
    "**Configure automatic storage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.schemas import VerificationConfig, ModelConfig\n",
    "from karenina.storage import DBConfig\n",
    "\n",
    "# Create database configuration\n",
    "db_config = DBConfig(\n",
    "    storage_url=\"sqlite:///benchmarks.db\",\n",
    "    auto_create=True  # Create tables if they don't exist\n",
    ")\n",
    "\n",
    "print(f\"✓ Database configuration created\")\n",
    "print(f\"  Storage URL: {db_config.storage_url}\")\n",
    "print(f\"  Auto create: {db_config.auto_create}\")\n",
    "print(f\"  Dialect: {db_config.dialect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works:**\n",
    "\n",
    "1. When `db_config` is set in `VerificationConfig`, verification results are automatically saved to the specified database after completion\n",
    "2. The `AUTOSAVE_DATABASE` environment variable controls this behavior (defaults to `\"true\"`)\n",
    "3. Results are saved with metadata including run name, timestamp, and configuration details\n",
    "4. This happens transparently without requiring manual `save_to_db()` calls\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- **No data loss**: Results are persisted immediately after verification completes\n",
    "- **Automatic**: No need to remember to call `save_to_db()` after verification\n",
    "- **Production-ready**: Ideal for automated pipelines and long-running jobs\n",
    "- **Queryable**: Results are immediately available for database queries and analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disabling auto-save:**\n",
    "\n",
    "If you need to disable automatic database storage temporarily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable (example)\n",
    "# export AUTOSAVE_DATABASE=\"false\"\n",
    "\n",
    "# Or use db_config=None in VerificationConfig\n",
    "config_no_db = VerificationConfig(\n",
    "    answering_models=[ModelConfig(\n",
    "        id=\"gpt-4.1-mini\",\n",
    "        model_provider=\"openai\",\n",
    "        model_name=\"gpt-4.1-mini\",\n",
    "        interface=\"langchain\"\n",
    "    )],\n",
    "    parsing_models=[ModelConfig(\n",
    "        id=\"gpt-4.1-mini\",\n",
    "        model_provider=\"openai\",\n",
    "        model_name=\"gpt-4.1-mini\",\n",
    "        interface=\"langchain\"\n",
    "    )],\n",
    "    db_config=None  # No automatic database storage\n",
    ")\n",
    "\n",
    "print(\"✓ Configured without automatic database storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example with full workflow:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from karenina import Benchmark\nfrom karenina.schemas import VerificationConfig, ModelConfig\nfrom karenina.storage import DBConfig\nfrom pathlib import Path\n\n# Load benchmark\nbenchmark = Benchmark.load(checkpoint_path)\n\n# Configure database\ndb_config = DBConfig(\n    storage_url=f\"sqlite:///{db_path}\",\n    auto_create=True\n)\n\n# Configure verification with automatic database storage\nmodel_config = ModelConfig(\n    id=\"gpt-4.1-mini\",\n    model_provider=\"openai\",\n    model_name=\"gpt-4.1-mini\",\n    temperature=0.7,\n    interface=\"langchain\"\n)\n\nconfig = VerificationConfig(\n    answering_models=[model_config],\n    parsing_models=[model_config],\n    evaluation_mode=\"template_and_rubric\",  # Required when rubric_enabled=True\n    rubric_enabled=True,\n    replicate_count=3,\n    db_config=db_config  # Automatic storage enabled\n)\n\nprint(\"✓ Verification configured with automatic database storage\")\nprint(f\"  Replicate count: {config.replicate_count}\")\nprint(f\"  Evaluation mode: {config.evaluation_mode}\")\nprint(f\"  Rubric enabled: {config.rubric_enabled}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Verification Results\n",
    "\n",
    "After running verification, export results for analysis and reporting.\n",
    "\n",
    "### Export to CSV\n",
    "\n",
    "CSV format is ideal for spreadsheet analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from karenina.schemas import VerificationConfig, ModelConfig\n",
    "\n",
    "# Run verification first\n",
    "config = VerificationConfig(\n",
    "    answering_models=[ModelConfig(\n",
    "        id=\"gpt-4.1-mini\",\n",
    "        model_provider=\"openai\",\n",
    "        model_name=\"gpt-4.1-mini\",\n",
    "        interface=\"langchain\"\n",
    "    )],\n",
    "    parsing_models=[ModelConfig(\n",
    "        id=\"gpt-judge\",\n",
    "        model_provider=\"openai\",\n",
    "        model_name=\"gpt-4.1-mini\",\n",
    "        interface=\"langchain\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = temp_path(\"results.csv\")\n",
    "benchmark.export_verification_results_to_file(\n",
    "    file_path=csv_path,\n",
    "    format=\"csv\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Exported to {csv_path.name}\")\n",
    "print(f\"  File size: {csv_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSV Output Structure:**\n",
    "\n",
    "| question_id | question | expected_answer | model_answer | template_passed | answering_model | parsing_model | timestamp |\n",
    "|-------------|----------|-----------------|--------------|-----------------|-----------------|---------------|-----------|\n",
    "| abc123... | How many chromosomes... | 46 | There are 46 chromosomes... | True | gpt-4.1-mini | gpt-judge | 2024-03-15 14:30:22 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to JSON\n",
    "\n",
    "JSON format is ideal for programmatic analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "json_path = temp_path(\"results.json\")\n",
    "benchmark.export_verification_results_to_file(\n",
    "    file_path=json_path,\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Exported to {json_path.name}\")\n",
    "print(f\"  File size: {json_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Output Structure:**\n",
    "```json\n",
    "{\n",
    "  \"benchmark_name\": \"Genomics Knowledge Benchmark\",\n",
    "  \"export_timestamp\": \"2024-03-15T14:30:22\",\n",
    "  \"total_results\": 3,\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"question_id\": \"abc123...\",\n",
    "      \"question\": \"How many chromosomes are in a human somatic cell?\",\n",
    "      \"expected_answer\": \"46\",\n",
    "      \"raw_response\": \"There are 46 chromosomes in a human somatic cell.\",\n",
    "      \"parsed_response\": {\"count\": 46},\n",
    "      \"verify_result\": true,\n",
    "      \"answering_model_id\": \"gpt-4.1-mini\",\n",
    "      \"parsing_model_id\": \"gpt-judge\",\n",
    "      \"timestamp\": \"2024-03-15T14:30:22\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Specific Questions\n",
    "\n",
    "Export results for a subset of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question IDs for chromosomes questions\n",
    "all_questions = benchmark.get_all_questions(ids_only=False)\n",
    "chromosome_qids = [\n",
    "    q['id'] for q in all_questions\n",
    "    if \"chromosome\" in q['question'].lower()\n",
    "]\n",
    "\n",
    "print(f\"Found {len(chromosome_qids)} chromosome-related questions\")\n",
    "\n",
    "# Export only chromosome questions\n",
    "if chromosome_qids:\n",
    "    chromosome_csv_path = temp_path(\"chromosome_results.csv\")\n",
    "    benchmark.export_verification_results_to_file(\n",
    "        file_path=chromosome_csv_path,\n",
    "        format=\"csv\",\n",
    "        question_ids=chromosome_qids\n",
    "    )\n",
    "    print(f\"✓ Exported {len(chromosome_qids)} chromosome questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Management\n",
    "\n",
    "### Incremental Checkpoints\n",
    "\n",
    "Save checkpoints at key stages of your workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import ModelConfig, LLMRubricTrait, Rubric\n",
    "from pathlib import Path\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = TEMP_DIR / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add questions\n",
    "benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\",\n",
    "    raw_answer=\"46\"\n",
    ")\n",
    "benchmark.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\",\n",
    "    raw_answer=\"BCL2\"\n",
    ")\n",
    "\n",
    "# Checkpoint 1: After adding questions\n",
    "cp1 = checkpoint_dir / \"01_questions_added.jsonld\"\n",
    "benchmark.save(cp1)\n",
    "print(\"✓ Checkpoint 1: Questions added\")\n",
    "\n",
    "# Generate templates\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    interface=\"langchain\"\n",
    ")\n",
    "benchmark.generate_all_templates(model=\"gpt-4.1-mini\")\n",
    "\n",
    "# Checkpoint 2: After template generation\n",
    "cp2 = checkpoint_dir / \"02_templates_generated.jsonld\"\n",
    "benchmark.save(cp2)\n",
    "print(\"✓ Checkpoint 2: Templates generated\")\n",
    "\n",
    "# Create rubric\n",
    "rubric = Rubric(llm_traits=[\n",
    "    LLMRubricTrait(\n",
    "        name=\"Conciseness\",\n",
    "        description=\"Rate conciseness 1-5\",\n",
    "        kind=\"score\"\n",
    "    )\n",
    "])\n",
    "benchmark.set_global_rubric(rubric)\n",
    "\n",
    "# Checkpoint 3: After rubrics\n",
    "cp3 = checkpoint_dir / \"03_rubrics_created.jsonld\"\n",
    "benchmark.save(cp3)\n",
    "print(\"✓ Checkpoint 3: Rubrics created\")\n",
    "\n",
    "print(f\"\\nAll checkpoints saved to: {checkpoint_dir}\")\n",
    "for cp in sorted(checkpoint_dir.glob(\"*.jsonld\")):\n",
    "    print(f\"  - {cp.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamped Backups\n",
    "\n",
    "Create timestamped backups automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Create backup directory\n",
    "backup_dir = TEMP_DIR / \"backups\"\n",
    "backup_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def save_with_timestamp(benchmark, base_name: str):\n",
    "    \"\"\"Save benchmark with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{base_name}_{timestamp}.jsonld\"\n",
    "    path = backup_dir / filename\n",
    "\n",
    "    benchmark.save(path)\n",
    "\n",
    "    print(f\"✓ Backup saved: {filename}\")\n",
    "    return path\n",
    "\n",
    "# Save with timestamp\n",
    "save_with_timestamp(benchmark, \"genomics_benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "✓ Backup saved: genomics_benchmark_20240315_143022.jsonld\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Checkpoints\n",
    "\n",
    "Compare two checkpoints to see what changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_checkpoints(path1: Path, path2: Path):\n    \"\"\"Compare two benchmark checkpoints\"\"\"\n    from karenina import Benchmark\n\n    bench1 = Benchmark.load(path1)\n    bench2 = Benchmark.load(path2)\n\n    print(f\"=== Comparing Checkpoints ===\")\n    print(f\"Checkpoint 1: {path1.name}\")\n    print(f\"Checkpoint 2: {path2.name}\")\n    print()\n\n    # Compare questions\n    print(f\"Questions:\")\n    print(f\"  {path1.name}: {bench1.question_count}\")\n    print(f\"  {path2.name}: {bench2.question_count}\")\n\n    # Compare templates\n    q1 = bench1.get_all_questions(ids_only=False)\n    q2 = bench2.get_all_questions(ids_only=False)\n    templates1 = sum(1 for q in q1 if q.get('answer_template') is not None)\n    templates2 = sum(1 for q in q2 if q.get('answer_template') is not None)\n    print(f\"\\nTemplates:\")\n    print(f\"  {path1.name}: {templates1}\")\n    print(f\"  {path2.name}: {templates2}\")\n\n    # Compare rubrics\n    has_global1 = bench1.get_global_rubric() is not None\n    has_global2 = bench2.get_global_rubric() is not None\n    print(f\"\\nGlobal Rubric:\")\n    print(f\"  {path1.name}: {'Yes' if has_global1 else 'No'}\")\n    print(f\"  {path2.name}: {'Yes' if has_global2 else 'No'}\")\n\n# Compare before and after rubrics\ncompare_checkpoints(cp2, cp3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portability and Sharing\n",
    "\n",
    "### Sharing Benchmarks with Collaborators\n",
    "\n",
    "Checkpoints are portable and can be easily shared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare benchmark for sharing\n",
    "share_path = temp_path(\"genomics_benchmark_v1.0.0.jsonld\")\n",
    "benchmark.save(share_path)\n",
    "\n",
    "# Collaborator loads it\n",
    "from karenina import Benchmark\n",
    "\n",
    "shared_benchmark = Benchmark.load(share_path)\n",
    "print(f\"Loaded shared benchmark: {shared_benchmark.name}\")\n",
    "print(f\"Version: {shared_benchmark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharing checklist:**\n",
    "\n",
    "- ✅ Save to descriptive filename with version\n",
    "- ✅ Include README with benchmark purpose and usage\n",
    "- ✅ Document any special requirements (API keys, models)\n",
    "- ✅ Test loading on a different machine\n",
    "\n",
    "### Version Control with Git\n",
    "\n",
    "Checkpoints work well with Git:\n",
    "\n",
    "```bash\n",
    "# Add checkpoint to Git\n",
    "git add genomics_benchmark_v1.0.0.jsonld\n",
    "git commit -m \"Add genomics benchmark v1.0.0\"\n",
    "git push\n",
    "\n",
    "# Track benchmark evolution over time\n",
    "git log -- genomics_benchmark_v1.0.0.jsonld\n",
    "```\n",
    "\n",
    "**Git best practices:**\n",
    "\n",
    "- Use semantic versioning for checkpoint filenames\n",
    "- Include descriptive commit messages\n",
    "- Tag important versions: `git tag v1.0.0`\n",
    "- Use `.gitignore` for temporary checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Workflow Example\n",
    "\n",
    "Here's a complete example showing checkpoints, database storage, and export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from karenina import Benchmark\nfrom karenina.schemas import VerificationConfig, ModelConfig, LLMRubricTrait, Rubric\nfrom pathlib import Path\n\n# 1. Create benchmark\nbenchmark = Benchmark.create(\n    name=\"Genomics Knowledge Benchmark\",\n    description=\"Testing LLM knowledge of genomics\",\n    version=\"1.0.0\",\n    creator=\"Bio Team\"\n)\n\n# 2. Add questions\nquestions = [\n    (\"How many chromosomes are in a human somatic cell?\", \"46\"),\n    (\"What is the approved drug target of Venetoclax?\", \"BCL2\"),\n    (\"How many protein subunits does hemoglobin A have?\", \"4\")\n]\n\nfor q, a in questions:\n    benchmark.add_question(question=q, raw_answer=a, author={\"name\": \"Bio Curator\"})\n\n# Checkpoint 1\nstep1 = checkpoint_dir / \"step1_questions.jsonld\"\nbenchmark.save(step1)\nprint(\"✓ Checkpoint 1: Questions added\")\n\n# 3. Generate templates\nmodel_config = ModelConfig(\n    id=\"gpt-4.1-mini\",\n    model_provider=\"openai\",\n    model_name=\"gpt-4.1-mini\",\n    temperature=0.1,\n    interface=\"langchain\"\n)\nbenchmark.generate_all_templates(model=\"gpt-4.1-mini\")\n\n# Checkpoint 2\nstep2 = checkpoint_dir / \"step2_templates.jsonld\"\nbenchmark.save(step2)\nprint(\"✓ Checkpoint 2: Templates generated\")\n\n# 4. Create rubric\nrubric = Rubric(llm_traits=[\n    LLMRubricTrait(\n        name=\"Conciseness\",\n        description=\"Rate conciseness 1-5\",\n        kind=\"score\"\n    ),\n    LLMRubricTrait(\n        name=\"Clarity\",\n        description=\"Is the answer clear?\",\n        kind=\"boolean\"\n    )\n])\nbenchmark.set_global_rubric(rubric)\n\n# Checkpoint 3\nstep3 = checkpoint_dir / \"step3_rubrics.jsonld\"\nbenchmark.save(step3)\nprint(\"✓ Checkpoint 3: Rubrics created\")\n\n# 5. Run verification\nconfig = VerificationConfig(\n    answering_models=[model_config],\n    parsing_models=[model_config],\n    evaluation_mode=\"template_and_rubric\",  # Required when rubric_enabled=True\n    rubric_enabled=True\n)\nresults = benchmark.run_verification(config)\n\n# Checkpoint 4\nstep4 = checkpoint_dir / \"step4_verified.jsonld\"\nbenchmark.save(step4)\nprint(\"✓ Checkpoint 4: Verification complete\")\n\n# 6. Save to database with checkpoint\nfinal_checkpoint = temp_path(\"genomics_benchmark_v1.0.0.jsonld\")\nbenchmark.save_to_db(\n    storage=f\"sqlite:///{db_path}\",\n    checkpoint_path=final_checkpoint\n)\nprint(\"✓ Saved to database with checkpoint\")\n\n# 7. Export results\nresults_csv = temp_path(\"results.csv\")\nresults_json = temp_path(\"results.json\")\nbenchmark.export_verification_results_to_file(\n    file_path=results_csv,\n    format=\"csv\"\n)\nbenchmark.export_verification_results_to_file(\n    file_path=results_json,\n    format=\"json\"\n)\nprint(\"✓ Exported results to CSV and JSON\")\n\n# 8. Create timestamped backup\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nbackup_path = backup_dir / f\"genomics_{timestamp}.jsonld\"\nbenchmark.save(backup_path)\nprint(f\"✓ Backup saved: {backup_path.name}\")\n\nprint(\"\\n=== Summary ===\")\nprint(f\"Benchmark: {benchmark.name} v{benchmark.version}\")\nprint(f\"Questions: {benchmark.question_count}\")\nprint(f\"Verification results: {len(results)}\")\nprint(f\"Checkpoints: 4\")\nprint(f\"Database: Yes (sqlite:///{db_path.name})\")\nprint(f\"Exports: CSV and JSON\")\nprint(f\"Backup: Yes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "✓ Checkpoint 1: Questions added\n",
    "✓ Checkpoint 2: Templates generated\n",
    "✓ Checkpoint 3: Rubrics created\n",
    "✓ Checkpoint 4: Verification complete\n",
    "✓ Saved to database with checkpoint\n",
    "✓ Exported results to CSV and JSON\n",
    "✓ Backup saved: genomics_20240315_143022.jsonld\n",
    "\n",
    "=== Summary ===\n",
    "Benchmark: Genomics Knowledge Benchmark v1.0.0\n",
    "Questions: 3\n",
    "Verification results: 3\n",
    "Checkpoints: 4\n",
    "Database: Yes (sqlite:///benchmarks.db)\n",
    "Exports: CSV and JSON\n",
    "Backup: Yes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Naming Conventions\n",
    "\n",
    "Use descriptive, versioned filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Good: Descriptive with version and date\n",
    "# benchmark.save(Path(\"genomics_benchmark_v1.0.0_20240315.jsonld\"))\n",
    "# benchmark.save(Path(\"drug_targets_benchmark_v2.1.0.jsonld\"))\n",
    "\n",
    "# ❌ Bad: Generic or unclear names\n",
    "# benchmark.save(Path(\"benchmark.jsonld\"))\n",
    "# benchmark.save(Path(\"test.jsonld\"))\n",
    "\n",
    "print(\"Examples of good checkpoint names:\")\n",
    "print(\"  - genomics_benchmark_v1.0.0_20240315.jsonld\")\n",
    "print(\"  - drug_targets_benchmark_v2.1.0.jsonld\")\n",
    "print(\"  - clinical_questions_v1.2.3-beta.jsonld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Organization\n",
    "\n",
    "Organize checkpoints logically:\n",
    "\n",
    "```\n",
    "project/\n",
    "├── benchmarks/\n",
    "│   ├── genomics_v1.0.0.jsonld\n",
    "│   ├── drug_targets_v1.0.0.jsonld\n",
    "│   └── proteins_v1.0.0.jsonld\n",
    "├── checkpoints/\n",
    "│   └── genomics/\n",
    "│       ├── 01_questions.jsonld\n",
    "│       ├── 02_templates.jsonld\n",
    "│       ├── 03_rubrics.jsonld\n",
    "│       └── 04_verified.jsonld\n",
    "├── backups/\n",
    "│   ├── genomics_20240315.jsonld\n",
    "│   └── genomics_20240316.jsonld\n",
    "└── exports/\n",
    "    ├── results.csv\n",
    "    └── results.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Strategy\n",
    "\n",
    "Implement a regular backup strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def backup_benchmark(benchmark, backup_dir: Path):\n",
    "    \"\"\"Create daily backup\"\"\"\n",
    "    backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Daily backup\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "    daily_backup = backup_dir / f\"{benchmark.name}_{date_str}.jsonld\"\n",
    "\n",
    "    if not daily_backup.exists():\n",
    "        benchmark.save(daily_backup)\n",
    "        print(f\"✓ Daily backup: {daily_backup.name}\")\n",
    "    else:\n",
    "        print(f\"  Daily backup already exists for {date_str}\")\n",
    "\n",
    "# Backup after important changes\n",
    "backup_benchmark(benchmark, backup_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup Old Checkpoints\n",
    "\n",
    "Remove old temporary checkpoints periodically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_dir: Path, keep_days: int = 30):\n",
    "    \"\"\"Remove checkpoint files older than keep_days\"\"\"\n",
    "    cutoff_date = datetime.now() - timedelta(days=keep_days)\n",
    "\n",
    "    removed = 0\n",
    "    for checkpoint in checkpoint_dir.glob(\"*.jsonld\"):\n",
    "        # Check if it's a temporary checkpoint (contains 'checkpoint' in name)\n",
    "        if \"checkpoint\" in checkpoint.name.lower():\n",
    "            mtime = datetime.fromtimestamp(checkpoint.stat().st_mtime)\n",
    "            if mtime < cutoff_date:\n",
    "                checkpoint.unlink()\n",
    "                removed += 1\n",
    "                print(f\"  Removed: {checkpoint.name}\")\n",
    "\n",
    "    print(f\"✓ Removed {removed} old checkpoints (older than {keep_days} days)\")\n",
    "\n",
    "# Note: This would remove files matching criteria\n",
    "# cleanup_old_checkpoints(checkpoint_dir, keep_days=30)\n",
    "\n",
    "print(\"Cleanup function defined (not executed in demo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After saving and loading benchmarks:\n",
    "\n",
    "- [Run Verification](verification.md) - Evaluate LLM responses\n",
    "- [Advanced Features](../advanced/deep-judgment.md) - Use deep-judgment for detailed feedback\n",
    "- [Share Benchmarks](#portability-and-sharing) - Collaborate with your team\n",
    "\n",
    "## Related Documentation\n",
    "\n",
    "- [Defining Benchmarks](defining-benchmark.md) - Benchmark creation and database persistence\n",
    "- [Verification](verification.md) - Run evaluations\n",
    "- [Templates](templates.md) - Structured answer evaluation\n",
    "- [Rubrics](rubrics.md) - Qualitative assessment criteria\n",
    "- [Quick Start](../quickstart.md) - End-to-end workflow example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}