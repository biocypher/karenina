{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Set common attributes with realistic defaults\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        self.subunits = kwargs.get(\"subunits\", 4)\n",
    "        self.diseases = kwargs.get(\"diseases\", [\"asthma\", \"bronchitis\", \"pneumonia\"])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def create_mock_verification_result(question_id: str, question_text: str, answer: str, passed: bool = True):\n",
    "    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "\n",
    "    # Create mock template result\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=f\"The answer is {answer}.\",\n",
    "        parsed_llm_response={\"value\": answer},\n",
    "        parsed_gt_response={\"value\": answer},\n",
    "        verify_result=passed,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80},\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "    )\n",
    "\n",
    "    # Create mock rubric result\n",
    "    rubric = VerificationResultRubric(\n",
    "        rubric_evaluation_performed=True,\n",
    "        llm_trait_scores={\n",
    "            \"Conciseness\": 4,\n",
    "            \"Clarity\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Create metadata with all required fields\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=answer,\n",
    "        answering_model=\"gpt-4.1-mini\",\n",
    "        parsing_model=\"gpt-4.1-mini\",\n",
    "        execution_time=1.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n",
    "    )\n",
    "\n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "\n",
    "# Store original run_verification\n",
    "_original_run_verification = None\n",
    "\n",
    "\n",
    "def mock_run_verification(self, config):\n",
    "    \"\"\"Mock run_verification that returns realistic results.\"\"\"\n",
    "    global _original_run_verification\n",
    "\n",
    "    # Get all finished questions\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "\n",
    "    if len(finished) == 0:\n",
    "        if _original_run_verification:\n",
    "            return _original_run_verification(self, config)\n",
    "        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "\n",
    "    results = []\n",
    "    # Map question keywords to expected answers\n",
    "    mock_data = [\n",
    "        {\"keywords\": [\"chromosomes\"], \"answer\": \"46\", \"passed\": True},\n",
    "        {\"keywords\": [\"venetoclax\", \"bcl2\"], \"answer\": \"BCL2\", \"passed\": True},\n",
    "        {\"keywords\": [\"hemoglobin\", \"subunits\"], \"answer\": \"4\", \"passed\": True},\n",
    "        {\"keywords\": [\"inflammatory\", \"lung\"], \"answer\": \"asthma, bronchitis, pneumonia\", \"passed\": True},\n",
    "    ]\n",
    "\n",
    "    for question in finished:\n",
    "        q_id = question[\"id\"]\n",
    "        q_text = question[\"question\"]\n",
    "        raw_answer = question.get(\"raw_answer\", \"\")\n",
    "\n",
    "        passed = True\n",
    "        mock_ans = raw_answer\n",
    "        q_text_lower = q_text.lower()\n",
    "\n",
    "        for data in mock_data:\n",
    "            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n",
    "                passed = data[\"passed\"]\n",
    "                mock_ans = data[\"answer\"]\n",
    "                break\n",
    "\n",
    "        results.append(\n",
    "            create_mock_verification_result(question_id=q_id, question_text=q_text, answer=mock_ans, passed=passed)\n",
    "        )\n",
    "\n",
    "    template_results = TemplateResults(results=results)\n",
    "\n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Patch Benchmark.run_verification\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(\"✓ Mock verification results enabled - examples will show realistic output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Benchmarks\n",
    "\n",
    "This guide covers how to persist, restore, and export benchmarks using Karenina's checkpoint and database systems.\n",
    "\n",
    "## Understanding Persistence\n",
    "\n",
    "Karenina provides two main approaches for persisting benchmarks:\n",
    "\n",
    "1. **Checkpoints (JSON-LD files)**: Portable, human-readable files perfect for sharing and version control\n",
    "2. **Database storage (SQLite)**: Structured storage with query capabilities for production use\n",
    "\n",
    "You can use both approaches together: databases for primary storage and checkpoints for backups and sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Files (JSON-LD)\n",
    "\n",
    "Checkpoints are JSON-LD files that capture the complete state of a benchmark.\n",
    "\n",
    "### What Gets Saved\n",
    "\n",
    "A checkpoint includes:\n",
    "- Benchmark metadata (name, description, version)\n",
    "- All questions with their metadata\n",
    "- Answer templates\n",
    "- Rubrics (global and question-specific)\n",
    "- Verification results (if available)\n",
    "\n",
    "### JSON-LD Format\n",
    "\n",
    "Karenina uses **JSON-LD (JSON for Linked Data)** format following schema.org conventions:\n",
    "\n",
    "**Benefits:**\n",
    "- **Structured and semantic**: Machine-readable with clear data relationships\n",
    "- **Human-readable**: Open in any text editor to inspect contents\n",
    "- **Cross-platform**: Works across different environments\n",
    "- **Version-compatible**: Maintains backward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Checkpoints\n",
    "\n",
    "### Basic Save\n",
    "\n",
    "Save your benchmark to a JSON-LD checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina import Benchmark\n",
    "\n",
    "# Basic save\n",
    "# benchmark.save(Path(\"genomics_benchmark.jsonld\"))\n",
    "\n",
    "# Save to specific directory\n",
    "# benchmark.save(Path(\"benchmarks/genomics_benchmark.jsonld\"))\n",
    "\n",
    "print(\"Code example: Saving checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens When You Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "# Create and populate benchmark\n",
    "benchmark = Benchmark.create(name=\"Genomics Knowledge Benchmark\", version=\"1.0.0\")\n",
    "\n",
    "benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\", raw_answer=\"46\", author={\"name\": \"Bio Curator\"}\n",
    ")\n",
    "\n",
    "benchmark.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\", raw_answer=\"BCL2\", author={\"name\": \"Bio Curator\"}\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = temp_path(\"genomics_benchmark.jsonld\")\n",
    "benchmark.save(checkpoint_path)\n",
    "\n",
    "print(f\"✓ Saved checkpoint to {checkpoint_path.name}\")\n",
    "print(f\"  Questions: {benchmark.question_count}\")\n",
    "print(f\"  Size: {checkpoint_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "✓ Saved checkpoint to genomics_benchmark.jsonld\n",
    "  Questions: 2\n",
    "  Size: 4532 bytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Checkpoints\n",
    "\n",
    "### Basic Load\n",
    "\n",
    "Load a benchmark from a checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina import Benchmark\n",
    "\n",
    "# Load benchmark\n",
    "benchmark = Benchmark.load(checkpoint_path)\n",
    "\n",
    "print(f\"Loaded benchmark: {benchmark.name}\")\n",
    "print(f\"Version: {benchmark.version}\")\n",
    "print(f\"Questions: {benchmark.question_count}\")\n",
    "\n",
    "# Access questions\n",
    "question_ids = benchmark.get_question_ids()\n",
    "for qid in question_ids[:3]:\n",
    "    question = benchmark.get_question(qid)\n",
    "    print(f\"  • {question['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Loaded benchmark: Genomics Knowledge Benchmark\n",
    "Version: 1.0.0\n",
    "Questions: 2\n",
    "  • How many chromosomes are in a human somatic ce...\n",
    "  • What is the approved drug target of Venetoclax...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_verify(checkpoint_path: Path):\n",
    "    \"\"\"Load benchmark with validation\"\"\"\n",
    "    try:\n",
    "        benchmark = Benchmark.load(checkpoint_path)\n",
    "\n",
    "        # Basic validation\n",
    "        assert benchmark.question_count > 0, \"No questions found\"\n",
    "        assert benchmark.name, \"Missing benchmark name\"\n",
    "\n",
    "        print(f\"✓ Successfully loaded: {benchmark.name}\")\n",
    "        print(f\"  Questions: {benchmark.question_count}\")\n",
    "\n",
    "        # Check templates\n",
    "        all_questions = benchmark.get_all_questions(ids_only=False)\n",
    "        questions_with_templates = sum(1 for q in all_questions if q.get(\"answer_template\") is not None)\n",
    "        print(f\"  Templates: {questions_with_templates}/{benchmark.question_count}\")\n",
    "\n",
    "        return benchmark\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load with validation\n",
    "benchmark = load_and_verify(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Storage\n",
    "\n",
    "Database storage provides structured persistence with query capabilities.\n",
    "\n",
    "### Quick Database Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina import Benchmark, DBConfig\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark.create(name=\"Genomics Knowledge Benchmark\", version=\"1.0.0\")\n",
    "\n",
    "# Add questions\n",
    "benchmark.add_question(question=\"How many chromosomes are in a human somatic cell?\", raw_answer=\"46\")\n",
    "\n",
    "# Save to database with checkpoint backup\n",
    "db_path = temp_path(\"benchmarks.db\")\n",
    "checkpoint_db_path = temp_path(\"genomics_from_db.jsonld\")\n",
    "\n",
    "benchmark.save_to_db(storage=f\"sqlite:///{db_path}\", checkpoint_path=checkpoint_db_path)\n",
    "\n",
    "print(\"✓ Saved to database and checkpoint\")\n",
    "\n",
    "# Load from database\n",
    "loaded = Benchmark.load_from_db(benchmark_name=\"Genomics Knowledge Benchmark\", storage=f\"sqlite:///{db_path}\")\n",
    "\n",
    "print(f\"✓ Loaded from database: {loaded.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Database vs Checkpoints\n",
    "\n",
    "| Use Case | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| **Development and prototyping** | Checkpoints only |\n",
    "| **Sharing benchmarks** | Checkpoints (portable files) |\n",
    "| **Production deployment** | Database primary, checkpoints for backup |\n",
    "| **Version control (Git)** | Checkpoints (diff-friendly) |\n",
    "| **Multi-user collaboration** | Database with checkpoint backups |\n",
    "| **Query and analytics** | Database |\n",
    "| **Backups** | Checkpoints |\n",
    "\n",
    "**Best Practice:** Use both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Save to database for primary storage\n",
    "# benchmark.save_to_db(\"sqlite:///production.db\")\n",
    "\n",
    "# Also save checkpoint for backup/sharing\n",
    "# benchmark.save(Path(\"backups/genomics_v1.0.0.jsonld\"))\n",
    "\n",
    "print(\"Code example: Using both database and checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Database Storage During Verification\n",
    "\n",
    "Karenina can automatically save verification results to a database as they are generated. This is especially useful for long-running verification jobs where you want results persisted immediately.\n",
    "\n",
    "**Configure automatic storage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "from karenina.storage import DBConfig\n",
    "\n",
    "# Create database configuration\n",
    "db_config = DBConfig(\n",
    "    storage_url=\"sqlite:///benchmarks.db\",\n",
    "    auto_create=True,  # Create tables if they don't exist\n",
    ")\n",
    "\n",
    "print(\"✓ Database configuration created\")\n",
    "print(f\"  Storage URL: {db_config.storage_url}\")\n",
    "print(f\"  Auto create: {db_config.auto_create}\")\n",
    "print(f\"  Dialect: {db_config.dialect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works:**\n",
    "\n",
    "1. When `db_config` is set in `VerificationConfig`, verification results are automatically saved to the specified database after completion\n",
    "2. The `AUTOSAVE_DATABASE` environment variable controls this behavior (defaults to `\"true\"`)\n",
    "3. Results are saved with metadata including run name, timestamp, and configuration details\n",
    "4. This happens transparently without requiring manual `save_to_db()` calls\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- **No data loss**: Results are persisted immediately after verification completes\n",
    "- **Automatic**: No need to remember to call `save_to_db()` after verification\n",
    "- **Production-ready**: Ideal for automated pipelines and long-running jobs\n",
    "- **Queryable**: Results are immediately available for database queries and analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disabling auto-save:**\n",
    "\n",
    "If you need to disable automatic database storage temporarily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable (example)\n",
    "# export AUTOSAVE_DATABASE=\"false\"\n",
    "\n",
    "# Or use db_config=None in VerificationConfig\n",
    "config_no_db = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    "    db_config=None,  # No automatic database storage\n",
    ")\n",
    "\n",
    "print(\"✓ Configured without automatic database storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example with full workflow:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "from karenina.storage import DBConfig\n",
    "\n",
    "# Load benchmark\n",
    "benchmark = Benchmark.load(checkpoint_path)\n",
    "\n",
    "# Configure database\n",
    "db_config = DBConfig(storage_url=f\"sqlite:///{db_path}\", auto_create=True)\n",
    "\n",
    "# Configure verification with automatic database storage\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.7, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    evaluation_mode=\"template_and_rubric\",  # Required when rubric_enabled=True\n",
    "    rubric_enabled=True,\n",
    "    replicate_count=3,\n",
    "    db_config=db_config,  # Automatic storage enabled\n",
    ")\n",
    "\n",
    "print(\"✓ Verification configured with automatic database storage\")\n",
    "print(f\"  Replicate count: {config.replicate_count}\")\n",
    "print(f\"  Evaluation mode: {config.evaluation_mode}\")\n",
    "print(f\"  Rubric enabled: {config.rubric_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Verification Results\n",
    "\n",
    "After running verification, export results for analysis and reporting.\n",
    "\n",
    "### Export to CSV\n",
    "\n",
    "CSV format is ideal for spreadsheet analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Run verification first\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"gpt-judge\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = temp_path(\"results.csv\")\n",
    "benchmark.export_verification_results_to_file(file_path=csv_path, format=\"csv\")\n",
    "\n",
    "print(f\"✓ Exported to {csv_path.name}\")\n",
    "print(f\"  File size: {csv_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSV Output Structure:**\n",
    "\n",
    "| question_id | question | expected_answer | model_answer | template_passed | answering_model | parsing_model | timestamp |\n",
    "|-------------|----------|-----------------|--------------|-----------------|-----------------|---------------|-----------|\n",
    "| abc123... | How many chromosomes... | 46 | There are 46 chromosomes... | True | gpt-4.1-mini | gpt-judge | 2024-03-15 14:30:22 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to JSON\n",
    "\n",
    "JSON format is ideal for programmatic analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "json_path = temp_path(\"results.json\")\n",
    "benchmark.export_verification_results_to_file(file_path=json_path, format=\"json\")\n",
    "\n",
    "print(f\"✓ Exported to {json_path.name}\")\n",
    "print(f\"  File size: {json_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Output Structure:**\n",
    "```json\n",
    "{\n",
    "  \"benchmark_name\": \"Genomics Knowledge Benchmark\",\n",
    "  \"export_timestamp\": \"2024-03-15T14:30:22\",\n",
    "  \"total_results\": 3,\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"question_id\": \"abc123...\",\n",
    "      \"question\": \"How many chromosomes are in a human somatic cell?\",\n",
    "      \"expected_answer\": \"46\",\n",
    "      \"raw_response\": \"There are 46 chromosomes in a human somatic cell.\",\n",
    "      \"parsed_response\": {\"count\": 46},\n",
    "      \"verify_result\": true,\n",
    "      \"answering_model_id\": \"gpt-4.1-mini\",\n",
    "      \"parsing_model_id\": \"gpt-judge\",\n",
    "      \"timestamp\": \"2024-03-15T14:30:22\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Specific Questions\n",
    "\n",
    "Export results for a subset of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question IDs for chromosomes questions\n",
    "all_questions = benchmark.get_all_questions(ids_only=False)\n",
    "chromosome_qids = [q[\"id\"] for q in all_questions if \"chromosome\" in q[\"question\"].lower()]\n",
    "\n",
    "print(f\"Found {len(chromosome_qids)} chromosome-related questions\")\n",
    "\n",
    "# Export only chromosome questions\n",
    "if chromosome_qids:\n",
    "    chromosome_csv_path = temp_path(\"chromosome_results.csv\")\n",
    "    benchmark.export_verification_results_to_file(\n",
    "        file_path=chromosome_csv_path, format=\"csv\", question_ids=chromosome_qids\n",
    "    )\n",
    "    print(f\"✓ Exported {len(chromosome_qids)} chromosome questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Management\n",
    "\n",
    "### Incremental Checkpoints\n",
    "\n",
    "Save checkpoints at key stages of your workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.schemas import LLMRubricTrait, ModelConfig, Rubric\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = TEMP_DIR / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark.create(name=\"Genomics Knowledge Benchmark\", version=\"1.0.0\")\n",
    "\n",
    "# Add questions\n",
    "benchmark.add_question(question=\"How many chromosomes are in a human somatic cell?\", raw_answer=\"46\")\n",
    "benchmark.add_question(question=\"What is the approved drug target of Venetoclax?\", raw_answer=\"BCL2\")\n",
    "\n",
    "# Checkpoint 1: After adding questions\n",
    "cp1 = checkpoint_dir / \"01_questions_added.jsonld\"\n",
    "benchmark.save(cp1)\n",
    "print(\"✓ Checkpoint 1: Questions added\")\n",
    "\n",
    "# Generate templates\n",
    "model_config = ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "benchmark.generate_all_templates(model=\"gpt-4.1-mini\")\n",
    "\n",
    "# Checkpoint 2: After template generation\n",
    "cp2 = checkpoint_dir / \"02_templates_generated.jsonld\"\n",
    "benchmark.save(cp2)\n",
    "print(\"✓ Checkpoint 2: Templates generated\")\n",
    "\n",
    "# Create rubric\n",
    "rubric = Rubric(llm_traits=[LLMRubricTrait(name=\"Conciseness\", description=\"Rate conciseness 1-5\", kind=\"score\")])\n",
    "benchmark.set_global_rubric(rubric)\n",
    "\n",
    "# Checkpoint 3: After rubrics\n",
    "cp3 = checkpoint_dir / \"03_rubrics_created.jsonld\"\n",
    "benchmark.save(cp3)\n",
    "print(\"✓ Checkpoint 3: Rubrics created\")\n",
    "\n",
    "print(f\"\\nAll checkpoints saved to: {checkpoint_dir}\")\n",
    "for cp in sorted(checkpoint_dir.glob(\"*.jsonld\")):\n",
    "    print(f\"  - {cp.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamped Backups\n",
    "\n",
    "Create timestamped backups automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create backup directory\n",
    "backup_dir = TEMP_DIR / \"backups\"\n",
    "backup_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def save_with_timestamp(benchmark, base_name: str):\n",
    "    \"\"\"Save benchmark with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{base_name}_{timestamp}.jsonld\"\n",
    "    path = backup_dir / filename\n",
    "\n",
    "    benchmark.save(path)\n",
    "\n",
    "    print(f\"✓ Backup saved: {filename}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# Save with timestamp\n",
    "save_with_timestamp(benchmark, \"genomics_benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "✓ Backup saved: genomics_benchmark_20240315_143022.jsonld\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Checkpoints\n",
    "\n",
    "Compare two checkpoints to see what changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_checkpoints(path1: Path, path2: Path):\n",
    "    \"\"\"Compare two benchmark checkpoints\"\"\"\n",
    "    from karenina import Benchmark\n",
    "\n",
    "    bench1 = Benchmark.load(path1)\n",
    "    bench2 = Benchmark.load(path2)\n",
    "\n",
    "    print(\"=== Comparing Checkpoints ===\")\n",
    "    print(f\"Checkpoint 1: {path1.name}\")\n",
    "    print(f\"Checkpoint 2: {path2.name}\")\n",
    "    print()\n",
    "\n",
    "    # Compare questions\n",
    "    print(\"Questions:\")\n",
    "    print(f\"  {path1.name}: {bench1.question_count}\")\n",
    "    print(f\"  {path2.name}: {bench2.question_count}\")\n",
    "\n",
    "    # Compare templates\n",
    "    q1 = bench1.get_all_questions(ids_only=False)\n",
    "    q2 = bench2.get_all_questions(ids_only=False)\n",
    "    templates1 = sum(1 for q in q1 if q.get(\"answer_template\") is not None)\n",
    "    templates2 = sum(1 for q in q2 if q.get(\"answer_template\") is not None)\n",
    "    print(\"\\nTemplates:\")\n",
    "    print(f\"  {path1.name}: {templates1}\")\n",
    "    print(f\"  {path2.name}: {templates2}\")\n",
    "\n",
    "    # Compare rubrics\n",
    "    has_global1 = bench1.get_global_rubric() is not None\n",
    "    has_global2 = bench2.get_global_rubric() is not None\n",
    "    print(\"\\nGlobal Rubric:\")\n",
    "    print(f\"  {path1.name}: {'Yes' if has_global1 else 'No'}\")\n",
    "    print(f\"  {path2.name}: {'Yes' if has_global2 else 'No'}\")\n",
    "\n",
    "\n",
    "# Compare before and after rubrics\n",
    "compare_checkpoints(cp2, cp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portability and Sharing\n",
    "\n",
    "### Sharing Benchmarks with Collaborators\n",
    "\n",
    "Checkpoints are portable and can be easily shared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare benchmark for sharing\n",
    "share_path = temp_path(\"genomics_benchmark_v1.0.0.jsonld\")\n",
    "benchmark.save(share_path)\n",
    "\n",
    "# Collaborator loads it\n",
    "from karenina import Benchmark\n",
    "\n",
    "shared_benchmark = Benchmark.load(share_path)\n",
    "print(f\"Loaded shared benchmark: {shared_benchmark.name}\")\n",
    "print(f\"Version: {shared_benchmark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharing checklist:**\n",
    "\n",
    "- ✅ Save to descriptive filename with version\n",
    "- ✅ Include README with benchmark purpose and usage\n",
    "- ✅ Document any special requirements (API keys, models)\n",
    "- ✅ Test loading on a different machine\n",
    "\n",
    "### Version Control with Git\n",
    "\n",
    "Checkpoints work well with Git:\n",
    "\n",
    "```bash\n",
    "# Add checkpoint to Git\n",
    "git add genomics_benchmark_v1.0.0.jsonld\n",
    "git commit -m \"Add genomics benchmark v1.0.0\"\n",
    "git push\n",
    "\n",
    "# Track benchmark evolution over time\n",
    "git log -- genomics_benchmark_v1.0.0.jsonld\n",
    "```\n",
    "\n",
    "**Git best practices:**\n",
    "\n",
    "- Use semantic versioning for checkpoint filenames\n",
    "- Include descriptive commit messages\n",
    "- Tag important versions: `git tag v1.0.0`\n",
    "- Use `.gitignore` for temporary checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Workflow Example\n",
    "\n",
    "Here's a complete example showing checkpoints, database storage, and export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.schemas import LLMRubricTrait, ModelConfig, Rubric, VerificationConfig\n",
    "\n",
    "# 1. Create benchmark\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    description=\"Testing LLM knowledge of genomics\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Bio Team\",\n",
    ")\n",
    "\n",
    "# 2. Add questions\n",
    "questions = [\n",
    "    (\"How many chromosomes are in a human somatic cell?\", \"46\"),\n",
    "    (\"What is the approved drug target of Venetoclax?\", \"BCL2\"),\n",
    "    (\"How many protein subunits does hemoglobin A have?\", \"4\"),\n",
    "]\n",
    "\n",
    "for q, a in questions:\n",
    "    benchmark.add_question(question=q, raw_answer=a, author={\"name\": \"Bio Curator\"})\n",
    "\n",
    "# Checkpoint 1\n",
    "step1 = checkpoint_dir / \"step1_questions.jsonld\"\n",
    "benchmark.save(step1)\n",
    "print(\"✓ Checkpoint 1: Questions added\")\n",
    "\n",
    "# 3. Generate templates\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.1, interface=\"langchain\"\n",
    ")\n",
    "benchmark.generate_all_templates(model=\"gpt-4.1-mini\")\n",
    "\n",
    "# Checkpoint 2\n",
    "step2 = checkpoint_dir / \"step2_templates.jsonld\"\n",
    "benchmark.save(step2)\n",
    "print(\"✓ Checkpoint 2: Templates generated\")\n",
    "\n",
    "# 4. Create rubric\n",
    "rubric = Rubric(\n",
    "    llm_traits=[\n",
    "        LLMRubricTrait(name=\"Conciseness\", description=\"Rate conciseness 1-5\", kind=\"score\"),\n",
    "        LLMRubricTrait(name=\"Clarity\", description=\"Is the answer clear?\", kind=\"boolean\"),\n",
    "    ]\n",
    ")\n",
    "benchmark.set_global_rubric(rubric)\n",
    "\n",
    "# Checkpoint 3\n",
    "step3 = checkpoint_dir / \"step3_rubrics.jsonld\"\n",
    "benchmark.save(step3)\n",
    "print(\"✓ Checkpoint 3: Rubrics created\")\n",
    "\n",
    "# 5. Run verification\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    evaluation_mode=\"template_and_rubric\",  # Required when rubric_enabled=True\n",
    "    rubric_enabled=True,\n",
    ")\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "# Checkpoint 4\n",
    "step4 = checkpoint_dir / \"step4_verified.jsonld\"\n",
    "benchmark.save(step4)\n",
    "print(\"✓ Checkpoint 4: Verification complete\")\n",
    "\n",
    "# 6. Save to database with checkpoint\n",
    "final_checkpoint = temp_path(\"genomics_benchmark_v1.0.0.jsonld\")\n",
    "benchmark.save_to_db(storage=f\"sqlite:///{db_path}\", checkpoint_path=final_checkpoint)\n",
    "print(\"✓ Saved to database with checkpoint\")\n",
    "\n",
    "# 7. Export results\n",
    "results_csv = temp_path(\"results.csv\")\n",
    "results_json = temp_path(\"results.json\")\n",
    "benchmark.export_verification_results_to_file(file_path=results_csv, format=\"csv\")\n",
    "benchmark.export_verification_results_to_file(file_path=results_json, format=\"json\")\n",
    "print(\"✓ Exported results to CSV and JSON\")\n",
    "\n",
    "# 8. Create timestamped backup\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_path = backup_dir / f\"genomics_{timestamp}.jsonld\"\n",
    "benchmark.save(backup_path)\n",
    "print(f\"✓ Backup saved: {backup_path.name}\")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Benchmark: {benchmark.name} v{benchmark.version}\")\n",
    "print(f\"Questions: {benchmark.question_count}\")\n",
    "print(f\"Verification results: {len(results)}\")\n",
    "print(\"Checkpoints: 4\")\n",
    "print(f\"Database: Yes (sqlite:///{db_path.name})\")\n",
    "print(\"Exports: CSV and JSON\")\n",
    "print(\"Backup: Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "✓ Checkpoint 1: Questions added\n",
    "✓ Checkpoint 2: Templates generated\n",
    "✓ Checkpoint 3: Rubrics created\n",
    "✓ Checkpoint 4: Verification complete\n",
    "✓ Saved to database with checkpoint\n",
    "✓ Exported results to CSV and JSON\n",
    "✓ Backup saved: genomics_20240315_143022.jsonld\n",
    "\n",
    "=== Summary ===\n",
    "Benchmark: Genomics Knowledge Benchmark v1.0.0\n",
    "Questions: 3\n",
    "Verification results: 3\n",
    "Checkpoints: 4\n",
    "Database: Yes (sqlite:///benchmarks.db)\n",
    "Exports: CSV and JSON\n",
    "Backup: Yes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Naming Conventions\n",
    "\n",
    "Use descriptive, versioned filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Good: Descriptive with version and date\n",
    "# benchmark.save(Path(\"genomics_benchmark_v1.0.0_20240315.jsonld\"))\n",
    "# benchmark.save(Path(\"drug_targets_benchmark_v2.1.0.jsonld\"))\n",
    "\n",
    "# ❌ Bad: Generic or unclear names\n",
    "# benchmark.save(Path(\"benchmark.jsonld\"))\n",
    "# benchmark.save(Path(\"test.jsonld\"))\n",
    "\n",
    "print(\"Examples of good checkpoint names:\")\n",
    "print(\"  - genomics_benchmark_v1.0.0_20240315.jsonld\")\n",
    "print(\"  - drug_targets_benchmark_v2.1.0.jsonld\")\n",
    "print(\"  - clinical_questions_v1.2.3-beta.jsonld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Organization\n",
    "\n",
    "Organize checkpoints logically:\n",
    "\n",
    "```\n",
    "project/\n",
    "├── benchmarks/\n",
    "│   ├── genomics_v1.0.0.jsonld\n",
    "│   ├── drug_targets_v1.0.0.jsonld\n",
    "│   └── proteins_v1.0.0.jsonld\n",
    "├── checkpoints/\n",
    "│   └── genomics/\n",
    "│       ├── 01_questions.jsonld\n",
    "│       ├── 02_templates.jsonld\n",
    "│       ├── 03_rubrics.jsonld\n",
    "│       └── 04_verified.jsonld\n",
    "├── backups/\n",
    "│   ├── genomics_20240315.jsonld\n",
    "│   └── genomics_20240316.jsonld\n",
    "└── exports/\n",
    "    ├── results.csv\n",
    "    └── results.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Strategy\n",
    "\n",
    "Implement a regular backup strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def backup_benchmark(benchmark, backup_dir: Path):\n",
    "    \"\"\"Create daily backup\"\"\"\n",
    "    backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Daily backup\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "    daily_backup = backup_dir / f\"{benchmark.name}_{date_str}.jsonld\"\n",
    "\n",
    "    if not daily_backup.exists():\n",
    "        benchmark.save(daily_backup)\n",
    "        print(f\"✓ Daily backup: {daily_backup.name}\")\n",
    "    else:\n",
    "        print(f\"  Daily backup already exists for {date_str}\")\n",
    "\n",
    "\n",
    "# Backup after important changes\n",
    "backup_benchmark(benchmark, backup_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup Old Checkpoints\n",
    "\n",
    "Remove old temporary checkpoints periodically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_dir: Path, keep_days: int = 30):\n",
    "    \"\"\"Remove checkpoint files older than keep_days\"\"\"\n",
    "    cutoff_date = datetime.now() - timedelta(days=keep_days)\n",
    "\n",
    "    removed = 0\n",
    "    for checkpoint in checkpoint_dir.glob(\"*.jsonld\"):\n",
    "        # Check if it's a temporary checkpoint (contains 'checkpoint' in name)\n",
    "        if \"checkpoint\" in checkpoint.name.lower():\n",
    "            mtime = datetime.fromtimestamp(checkpoint.stat().st_mtime)\n",
    "            if mtime < cutoff_date:\n",
    "                checkpoint.unlink()\n",
    "                removed += 1\n",
    "                print(f\"  Removed: {checkpoint.name}\")\n",
    "\n",
    "    print(f\"✓ Removed {removed} old checkpoints (older than {keep_days} days)\")\n",
    "\n",
    "\n",
    "# Note: This would remove files matching criteria\n",
    "# cleanup_old_checkpoints(checkpoint_dir, keep_days=30)\n",
    "\n",
    "print(\"Cleanup function defined (not executed in demo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After saving and loading benchmarks:\n",
    "\n",
    "- [Run Verification](verification.md) - Evaluate LLM responses\n",
    "- [Advanced Features](../advanced/deep-judgment.md) - Use deep-judgment for detailed feedback\n",
    "- [Share Benchmarks](#portability-and-sharing) - Collaborate with your team\n",
    "\n",
    "## Related Documentation\n",
    "\n",
    "- [Defining Benchmarks](defining-benchmark.md) - Benchmark creation and database persistence\n",
    "- [Verification](verification.md) - Run evaluations\n",
    "- [Templates](templates.md) - Structured answer evaluation\n",
    "- [Rubrics](rubrics.md) - Qualitative assessment criteria\n",
    "- [Quick Start](../quickstart.md) - End-to-end workflow example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
