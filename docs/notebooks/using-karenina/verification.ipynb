{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "import copy\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"46 chromosomes\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        self.subunits = kwargs.get(\"subunits\", 4)\n",
    "        self.diseases = kwargs.get(\"diseases\", [\"asthma\", \"bronchitis\", \"pneumonia\"])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def create_mock_verification_result(\n",
    "    question_id: str,\n",
    "    question_text: str,\n",
    "    answer: str,\n",
    "    answering_model: str = \"gpt-4.1-mini\",\n",
    "    parsing_model: str = \"gpt-4.1-mini\",\n",
    "    passed: bool = True,\n",
    "):\n",
    "    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "\n",
    "    # Create mock template result\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=f\"The answer is {answer}.\",\n",
    "        parsed_llm_response={\"value\": answer},\n",
    "        parsed_gt_response={\"value\": answer},\n",
    "        verify_result=passed,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80},\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "    )\n",
    "\n",
    "    # Create mock rubric result\n",
    "    rubric = VerificationResultRubric(\n",
    "        rubric_evaluation_performed=True,\n",
    "        llm_trait_scores={\n",
    "            \"Conciseness\": 4,\n",
    "            \"Clarity\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Create metadata with all required fields\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=answer,\n",
    "        answering_model=answering_model,\n",
    "        parsing_model=parsing_model,\n",
    "        execution_time=1.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, answering_model, parsing_model, timestamp),\n",
    "    )\n",
    "\n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "\n",
    "# Store original run_verification\n",
    "_original_run_verification = None\n",
    "\n",
    "\n",
    "def mock_run_verification(self, config, question_ids=None, progress_callback=None):\n",
    "    \"\"\"Mock run_verification that returns realistic results.\"\"\"\n",
    "    global _original_run_verification\n",
    "\n",
    "    # Get all finished questions\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "\n",
    "    # Filter by question_ids if provided\n",
    "    if question_ids is not None:\n",
    "        finished = [q for q in finished if q[\"id\"] in question_ids]\n",
    "\n",
    "    if len(finished) == 0:\n",
    "        if _original_run_verification:\n",
    "            return _original_run_verification(self, config)\n",
    "        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Map question keywords to expected answers\n",
    "    mock_data = [\n",
    "        {\"keywords\": [\"chromosomes\"], \"answer\": \"46\", \"passed\": True},\n",
    "        {\"keywords\": [\"venetoclax\", \"bcl2\"], \"answer\": \"BCL2\", \"passed\": True},\n",
    "        {\"keywords\": [\"hemoglobin\", \"subunits\"], \"answer\": \"4\", \"passed\": True},\n",
    "        {\"keywords\": [\"inflammatory\", \"lung\"], \"answer\": \"asthma, bronchitis, pneumonia\", \"passed\": True},\n",
    "    ]\n",
    "\n",
    "    for question in finished:\n",
    "        q_id = question[\"id\"]\n",
    "        q_text = question[\"question\"]\n",
    "        raw_answer = question.get(\"raw_answer\", \"\")\n",
    "        passed = True\n",
    "        mock_ans = raw_answer\n",
    "        q_text_lower = q_text.lower()\n",
    "        for data in mock_data:\n",
    "            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n",
    "                passed = data[\"passed\"]\n",
    "                mock_ans = data[\"answer\"]\n",
    "                break\n",
    "        results.append(\n",
    "            create_mock_verification_result(question_id=q_id, question_text=q_text, answer=mock_ans, passed=passed)\n",
    "        )\n",
    "\n",
    "    # Handle multiple models in config\n",
    "    if hasattr(config, \"answering_models\") and len(config.answering_models) > 1:\n",
    "        model_results = []\n",
    "        for model_config in config.answering_models:\n",
    "            for result in results:\n",
    "                # Create a copy with different answering model\n",
    "                result_copy = copy.deepcopy(result)\n",
    "                result_copy.metadata.answering_model = model_config.id\n",
    "                # Recompute result_id\n",
    "                new_result_id = compute_result_id(\n",
    "                    result.metadata.question_id,\n",
    "                    model_config.id,\n",
    "                    result.metadata.parsing_model,\n",
    "                    result.metadata.timestamp,\n",
    "                )\n",
    "                result_copy.metadata.result_id = new_result_id\n",
    "                model_results.append(result_copy)\n",
    "        results = model_results\n",
    "\n",
    "    template_results = TemplateResults(results=results)\n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Patch Benchmark.run_verification\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(\"✓ Mock verification results enabled - examples will show realistic output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Verification",
    "",
    "This guide covers how to configure and execute verification to evaluate LLM responses using your benchmark questions, templates, and rubrics.",
    "",
    "!!! tip \"Command-Line Interface Available\"",
    "    Prefer working from the terminal? Karenina provides a comprehensive CLI for running verifications without writing Python code. See **[CLI Verification](cli-verification.md)** for details on command-line usage, presets, and automation.",
    "",
    "**Quick Navigation:**",
    "",
    "- [Understanding Verification](#understanding-verification) - Core concepts and workflow",
    "- [Basic Configuration](#basic-verification-configuration) - Setting up VerificationConfig",
    "- [Running Verification](#running-verification) - Execute verification and select questions",
    "- [Multi-Model Support](#multi-model-support) - Test multiple models simultaneously",
    "- [Replication](#replication-for-statistical-analysis) - Statistical significance through repeated runs",
    "- [Evaluation Modes](#evaluation-modes) - Template-only, rubric-only, or combined",
    "- [Advanced Options](#advanced-configuration-options) - Abstention, deep judgment, system prompts",
    "- [LLM Interfaces](#using-different-llm-interfaces) - LangChain, OpenRouter, local models, manual",
    "- [Results](#accessing-verification-results) - Access and analyze verification data",
    "- [Automatic Database Storage](#automatic-database-storage) - Auto-save results to database",
    "- [Progress Tracking](#progress-tracking) - Monitor real-time verification progress",
    "- [Answer Caching](#answer-caching) - Automatic efficiency optimization",
    "- [Complete Example](#complete-example) - End-to-end verification workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Understanding Verification",
    "",
    "Verification in Karenina evaluates LLM responses through a structured workflow:",
    "",
    "1. **Question Selection**: Choose which questions to evaluate",
    "2. **Answer Generation**: LLMs generate responses to questions",
    "3. **Answer Parsing**: Judge LLMs extract structured data using templates",
    "4. **Template Verification**: Check if extracted data matches expected answers",
    "5. **Rubric Evaluation**: Assess qualitative traits (if enabled)",
    "6. **Result Aggregation**: Collect metrics and scores for analysis",
    "",
    "This two-model approach (answering model + judge model) allows natural language responses while maintaining structured evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Workflow",
    "",
    "```",
    "Questions → Answering Models → Raw Responses → Judge Models → Parsed Data → Verification",
    "                                                                                    ↓",
    "                                                                              Results with",
    "                                                                              Scores & Metrics",
    "```",
    "",
    "**Key Concepts:**",
    "",
    "- **Answering Models**: Generate responses to questions (can be any LLM)",
    "- **Parsing Models** (Judges): Extract structured data from responses using templates",
    "- **Templates**: Pydantic classes defining expected answer structure",
    "- **Rubrics**: Qualitative evaluation criteria (optional)",
    "- **Replication**: Run same question multiple times for statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Basic Verification Configuration",
    "",
    "### Configure Verification",
    "",
    "Use `VerificationConfig` to specify how verification runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Configure verification\n",
    "config = VerificationConfig(\n",
    "    # Models that generate answers\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    # Models that parse/judge answers\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,  # Deterministic parsing\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    # Evaluation settings\n",
    "    evaluation_mode=\"template_only\",  # or \"template_and_rubric\", \"rubric_only\"\n",
    "    rubric_enabled=False,\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "print(\"VerificationConfig created with:\")\n",
    "print(f\"  - {len(config.answering_models)} answering model(s)\")\n",
    "print(f\"  - {len(config.parsing_models)} parsing model(s)\")\n",
    "print(f\"  - Evaluation mode: {config.evaluation_mode}\")\n",
    "print(f\"  - Rubric enabled: {config.rubric_enabled}\")\n",
    "print(f\"  - Replicate count: {config.replicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a comprehensive guide to `ModelConfig` including all parameters, interfaces, providers, and the `extra_kwargs` feature, see the **[Model Configuration Guide](model-configuration.md)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Running Verification",
    "",
    "### Basic Verification",
    "",
    "Once you have templates and optionally rubrics, run verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a simple benchmark with questions for demonstration\n",
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Verification Demo\", description=\"Demo benchmark for verification examples\", version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add some questions\n",
    "questions = [\n",
    "    (\"How many chromosomes are in a human somatic cell?\", \"46\"),\n",
    "    (\"What is the approved drug target of Venetoclax?\", \"BCL2\"),\n",
    "    (\"How many protein subunits does hemoglobin A have?\", \"4\"),\n",
    "]\n",
    "\n",
    "question_ids = []\n",
    "for q, a in questions:\n",
    "    qid = benchmark.add_question(question=q, raw_answer=a)\n",
    "    question_ids.append(qid)\n",
    "\n",
    "# Generate templates (using mock setup)\n",
    "benchmark.generate_all_templates(model=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.1, interface=\"langchain\")\n",
    "\n",
    "print(f\"Created benchmark with {len(question_ids)} questions\")\n",
    "\n",
    "# Now run verification\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "print(f\"Verification complete: {len(results.results)} results generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Specific Questions",
    "",
    "Verify only a subset of questions by providing question IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question IDs for specific category (e.g., chromosome-related)\n",
    "genomics_question_ids = [\n",
    "    qid for qid in benchmark.get_question_ids() if \"chromosome\" in benchmark.get_question(qid)[\"question\"].lower()\n",
    "]\n",
    "\n",
    "print(f\"Found {len(genomics_question_ids)} chromosome-related questions\")\n",
    "\n",
    "# Run verification on subset\n",
    "results_subset = benchmark.run_verification(config=config, question_ids=genomics_question_ids)\n",
    "\n",
    "print(f\"Verified {len(results_subset.results)} genomics questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Multi-Model Support",
    "",
    "Karenina supports testing multiple models simultaneously and using different models for answering vs judging.",
    "",
    "### Test Multiple Answering Models",
    "",
    "Compare performance across different LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple answering models to compare\n",
    "config_multi = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"claude-sonnet\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-sonnet-4.5\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"gemini-pro\",\n",
    "            model_provider=\"google\",\n",
    "            model_name=\"gemini-2.5-flash\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    # Single judge model for consistent evaluation\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# This will generate 3 results per question (one per answering model)\n",
    "results_multi = benchmark.run_verification(config_multi)\n",
    "\n",
    "print(f\"Multi-model verification complete: {len(results_multi.results)} results\")\n",
    "print(\"Results per answering model:\")\n",
    "from collections import defaultdict\n",
    "\n",
    "results_by_model = defaultdict(list)\n",
    "for r in results_multi.results:\n",
    "    results_by_model[r.metadata.answering_model].append(r)\n",
    "for model, model_results in results_by_model.items():\n",
    "    print(f\"  - {model}: {len(model_results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Answering and Judge Models",
    "",
    "Use a more capable model for judging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast model for generating answers, more capable model for judging\n",
    "config_judge = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    # More capable model for judging\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-large-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Config created: fast answering model + capable judge model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Judge Models",
    "",
    "Compare how different judges evaluate the same answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single answering model, multiple judges for comparison\n",
    "config_multi_judge = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    # Multiple judges for comparison\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-judge\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.0, interface=\"langchain\"\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"claude-judge\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-sonnet-4.5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Config created: single answering model + multiple judges\")\n",
    "print(\"Answer caching will ensure the same answer is evaluated by all judges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Replication for Statistical Analysis",
    "",
    "Run the same question multiple times to assess model consistency and compute statistical metrics.",
    "",
    "### Configure Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_replicate = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    replicate_count=5,  # Run each question 5 times\n",
    ")\n",
    "\n",
    "print(f\"Replication configured: each question will run {config_replicate.replicate_count} times\")\n",
    "print(\"This enables statistical analysis of model consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Replication Results",
    "",
    "**Recommended: Use DataFrames** (see [DataFrame Quick Reference](dataframe-quick-reference.md)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DataFrame and group by question\n",
    "# Note: Use get_template_results() for template verification data\n",
    "template_results = results.get_template_results()\n",
    "df = template_results.to_dataframe()\n",
    "\n",
    "# Calculate pass rate per question\n",
    "if \"question_id\" in df.columns and \"field_match\" in df.columns:\n",
    "    pass_rates = df.groupby(\"question_id\")[\"field_match\"].mean()\n",
    "    print(\"Pass Rates by Question:\")\n",
    "    for qid, rate in pass_rates.items():\n",
    "        q = benchmark.get_question(qid)\n",
    "        print(f\"  {q['question'][:50]}...: {rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative: Group raw results manually:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group results by question\n",
    "results_by_question = defaultdict(list)\n",
    "for result in results.results:\n",
    "    results_by_question[result.question_id].append(result)\n",
    "\n",
    "# Compute pass rate for each question\n",
    "for question_id, question_results in results_by_question.items():\n",
    "    question = benchmark.get_question(question_id)\n",
    "    pass_count = sum(1 for r in question_results if r.verify_result)\n",
    "    total = len(question_results)\n",
    "    pass_rate = pass_count / total\n",
    "\n",
    "    print(f\"{question['question'][:50]}...\")\n",
    "    print(f\"  Pass Rate: {pass_rate:.1%} ({pass_count}/{total})\")\n",
    "\n",
    "    # Check consistency\n",
    "    if pass_rate == 1.0:\n",
    "        print(\"  ✓ Consistent: Always correct\")\n",
    "    elif pass_rate == 0.0:\n",
    "        print(\"  ✗ Consistent: Always incorrect\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Inconsistent: {pass_rate:.1%} accuracy\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Cases:**",
    "",
    "- **Model Reliability**: Assess how consistently a model answers correctly",
    "- **Statistical Significance**: Run k replicates for robust metrics",
    "- **Temperature Effects**: Compare variance at different temperature settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Evaluation Modes",
    "",
    "Karenina supports three evaluation modes that control what gets evaluated during verification.",
    "",
    "### Mode 1: template_only (Default)",
    "",
    "Evaluate responses against templates only. Fast and focused on factual correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_template_only = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    rubric_enabled=False,  # Must be False\n",
    ")\n",
    "\n",
    "results_template = benchmark.run_verification(config_template_only)\n",
    "\n",
    "print(\"Template-only verification results:\")\n",
    "print(\"  - template_verification_performed = True\")\n",
    "print(\"  - verify_result = True/False (template pass/fail)\")\n",
    "print(\"  - rubric_evaluation_performed = False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use:**",
    "",
    "- Testing template parsing accuracy",
    "- Fastest verification (no rubric overhead)",
    "- Focus on structured output correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode 2: template_and_rubric",
    "",
    "Evaluate both template correctness AND rubric criteria. Comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.schemas import LLMRubricTrait, Rubric\n",
    "\n",
    "# Add a global rubric first\n",
    "global_rubric = Rubric(\n",
    "    llm_traits=[\n",
    "        LLMRubricTrait(\n",
    "            name=\"Conciseness\", description=\"Rate how concise the answer is on a scale of 1-5\", kind=\"score\"\n",
    "        ),\n",
    "        LLMRubricTrait(name=\"Clarity\", description=\"Is the answer clear and easy to understand?\", kind=\"boolean\"),\n",
    "    ]\n",
    ")\n",
    "benchmark.set_global_rubric(global_rubric)\n",
    "\n",
    "config_both = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    rubric_enabled=True,  # Must be True\n",
    ")\n",
    "\n",
    "results_both = benchmark.run_verification(config_both)\n",
    "\n",
    "print(\"Template and rubric verification results:\")\n",
    "print(\"  - template_verification_performed = True\")\n",
    "print(\"  - verify_result = True/False (template pass/fail)\")\n",
    "print(\"  - rubric_evaluation_performed = True\")\n",
    "\n",
    "# Show rubric scores if available\n",
    "if results_both.results:\n",
    "    first_result = results_both.results[0]\n",
    "    if first_result.rubric and first_result.rubric.llm_trait_scores:\n",
    "        print(f\"\\nSample rubric scores: {first_result.rubric.llm_trait_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use:**",
    "",
    "- Production benchmarking with full metrics",
    "- Evaluate both correctness (template) and quality (rubric)",
    "- Comprehensive model assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode 3: rubric_only",
    "",
    "Evaluate rubric criteria only, skip template verification. Useful for qualitative assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_rubric_only = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"rubric_only\",\n",
    "    rubric_enabled=True,  # Must be True\n",
    ")\n",
    "\n",
    "results_rubric = benchmark.run_verification(config_rubric_only)\n",
    "\n",
    "print(\"Rubric-only verification results:\")\n",
    "print(\"  - template_verification_performed = False\")\n",
    "print(\"  - verify_result = None\")\n",
    "print(\"  - rubric_evaluation_performed = True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use:**",
    "",
    "- Qualitative evaluation without structured output requirements",
    "- Rubric development and tuning",
    "- Open-ended response evaluation",
    "- Focus on content quality over format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Advanced Configuration Options",
    "",
    "### Enable Abstention Detection",
    "",
    "Detect when models refuse to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_abstention = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    abstention_check_enabled=True,  # Detect refusals\n",
    ")\n",
    "\n",
    "print(\"Abstention detection enabled\")\n",
    "print(\"This will detect when models refuse to answer questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Deep Judgment",
    "",
    "Extract detailed feedback with verbatim excerpts and reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_deep = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.7,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt-4.1-mini-judge\",\n",
    "            model_provider=\"openai\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    deep_judgment_enabled=True,\n",
    "    deep_judgment_max_excerpts_per_attribute=3,\n",
    "    deep_judgment_fuzzy_match_threshold=0.80,\n",
    ")\n",
    "\n",
    "print(\"Deep judgment enabled\")\n",
    "print(\"This will extract detailed feedback with verbatim excerpts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Deep Judgment documentation](../advanced/deep-judgment.md) for comprehensive guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add System Prompts",
    "",
    "Customize model behavior with system prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answering model with domain expertise\n",
    "answering_model = ModelConfig(\n",
    "    id=\"gpt-genomics\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.7,\n",
    "    interface=\"langchain\",\n",
    "    system_prompt=\"You are an expert in genomics and molecular biology. Answer concisely with precise scientific terminology.\",\n",
    ")\n",
    "\n",
    "# Judge model with strict evaluation\n",
    "judge_model = ModelConfig(\n",
    "    id=\"gpt-judge-strict\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\",\n",
    "    system_prompt=\"You are a strict evaluator. Parse responses carefully and extract only explicitly stated information.\",\n",
    ")\n",
    "\n",
    "config_prompts = VerificationConfig(answering_models=[answering_model], parsing_models=[judge_model])\n",
    "\n",
    "print(\"System prompts configured:\")\n",
    "print(\"  - Answering: Expert genomics terminology\")\n",
    "print(\"  - Judge: Strict evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Temperature",
    "",
    "Control randomness and creativity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High temperature: More creative, less consistent\n",
    "creative_model = ModelConfig(\n",
    "    id=\"gpt-creative\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.9,  # More randomness\n",
    "    interface=\"langchain\",\n",
    ")\n",
    "\n",
    "# Zero temperature: Deterministic, consistent\n",
    "deterministic_model = ModelConfig(\n",
    "    id=\"gpt-deterministic\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,  # No randomness\n",
    "    interface=\"langchain\",\n",
    ")\n",
    "\n",
    "print(\"Temperature examples configured\")\n",
    "print(\"  - Creative: 0.9 (more random)\")\n",
    "print(\"  - Deterministic: 0.0 (no randomness)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperature Guidelines:**",
    "",
    "- **0.0**: Deterministic, always returns same answer (best for factual questions)",
    "- **0.3-0.5**: Slight variation, mostly consistent (good balance)",
    "- **0.7-0.9**: Creative, more diverse responses (good for open-ended questions)",
    "- **1.0+**: Very random, unpredictable (rarely useful for benchmarking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Using Different LLM Interfaces",
    "",
    "Karenina supports four interface types for connecting to LLM providers:",
    "",
    "1. **`langchain`** - Default interface for major cloud providers (OpenAI, Anthropic, Google)",
    "2. **`openrouter`** - Unified access to 200+ models through OpenRouter API",
    "3. **`openai_endpoint`** - Custom OpenAI-compatible endpoints (Ollama, vLLM, local models)",
    "4. **`manual`** - Pre-recorded traces for testing without API calls",
    "",
    "**Quick Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud provider via LangChain\n",
    "cloud_model = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\", temperature=0.0\n",
    ")\n",
    "\n",
    "# Local model via custom endpoint\n",
    "# Example configuration (not executed):\n",
    "# local_model = ModelConfig(\n",
    "#     id=\"llama-local\",\n",
    "#     model_name=\"llama3.1:70b\",\n",
    "#     interface=\"openai_endpoint\",\n",
    "#     endpoint_base_url=\"http://localhost:11434/v1\",\n",
    "#     endpoint_api_key=\"ollama\",\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "print(\"Model interface examples:\")\n",
    "print(f\"  - Cloud: {cloud_model.interface} ({cloud_model.model_provider})\")\n",
    "print(\"  - Local: openai_endpoint (Ollama example shown in comments)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comprehensive documentation on all four interfaces, including:",
    "",
    "- Detailed configuration examples",
    "- Environment variable setup",
    "- Provider-specific options",
    "- The `extra_kwargs` feature for advanced configuration",
    "- MCP tool integration",
    "- System prompts and custom parameters",
    "",
    "See the **[Model Configuration Guide](model-configuration.md)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Accessing Verification Results",
    "",
    "!!! tip \"Recommended: Use DataFrames for Result Analysis\"",
    "    For easier and more flexible result analysis, we recommend using the **DataFrame-first approach**:",
    "",
    "    - **[Analyzing Results with DataFrames](analyzing-results-dataframes.md)** - Comprehensive guide with 40+ examples",
    "    - **[DataFrame Quick Reference](dataframe-quick-reference.md)** - Cheat sheet for common operations",
    "",
    "    The DataFrame approach provides pandas-based analysis with:",
    "",
    "    - Standard pandas operations (groupby, filter, pivot)",
    "    - Helper methods for common aggregations",
    "    - Easy export to CSV, Excel, JSON",
    "    - Integration with visualization libraries",
    "",
    "    **Quick example:**",
    "    ```python",
    "    # After running verification",
    "    result_set = benchmark.run_verification(config)",
    "",
    "    # Convert to DataFrame for analysis",
    "    template_results = result_set.get_template_results()",
    "    df = template_results.to_dataframe()",
    "",
    "    # Analyze with pandas",
    "    pass_rates = df.groupby('question_id')['field_match'].mean()",
    "    ```",
    "",
    "    The sections below show how to access raw VerificationResult objects if you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Structure",
    "",
    "The `run_verification()` method returns a `VerificationResultSet` object that provides multiple ways to access results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run verification\n",
    "result_set = benchmark.run_verification(config)\n",
    "\n",
    "# Method 1: Use DataFrame API (RECOMMENDED)\n",
    "template_results = result_set.get_template_results()\n",
    "df = template_results.to_dataframe()\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Method 2: Access typed result wrappers\n",
    "rubric_results = result_set.get_rubrics_results()  # For rubric data\n",
    "judgment_results = result_set.get_judgment_results()  # For deep judgment data\n",
    "\n",
    "print(f\"Template results: {len(template_results.results)}\")\n",
    "print(f\"Rubric results available: {rubric_results is not None}\")\n",
    "print(f\"Judgment results available: {judgment_results is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Raw Results",
    "",
    "For detailed access to individual result properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Access raw VerificationResult list\n",
    "for result in result_set.results:\n",
    "    # Identification\n",
    "    print(f\"Question ID: {result.question_id}\")\n",
    "    print(f\"Answering Model: {result.answering_model}\")\n",
    "    print(f\"Parsing Model: {result.parsing_model}\")\n",
    "\n",
    "    # Raw response\n",
    "    print(f\"Raw Answer: {result.raw_llm_response}\")\n",
    "\n",
    "    # Template verification\n",
    "    print(f\"Template Passed: {result.verify_result}\")\n",
    "    print(f\"Parsed Response: {result.parsed_llm_response}\")\n",
    "\n",
    "    # Rubric evaluation (if enabled)\n",
    "    if result.rubric_evaluation_performed and result.verify_rubric:\n",
    "        print(f\"Rubric Scores: {result.verify_rubric}\")\n",
    "\n",
    "    # Abstention (if enabled)\n",
    "    if result.abstention_detected:\n",
    "        print(\"Abstention: Model refused to answer\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    break  # Just show first result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Results",
    "",
    "**Recommended: Use DataFrames for filtering** (see [DataFrame Quick Reference](dataframe-quick-reference.md#common-filters)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DataFrame\n",
    "df = result_set.get_template_results().to_dataframe()\n",
    "\n",
    "# Filter with pandas\n",
    "if \"field_match\" in df.columns:\n",
    "    passing = df[df[\"field_match\"] == True]\n",
    "    print(f\"Passing results: {len(passing)}\")\n",
    "\n",
    "if \"answering_model\" in df.columns:\n",
    "    gpt_results = df[df[\"answering_model\"] == \"gpt-4.1-mini\"]\n",
    "    print(f\"GPT results: {len(gpt_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative: Filter raw result list:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only passing results\n",
    "passing_results = [r for r in result_set.results if r.verify_result]\n",
    "print(f\"Passing: {len(passing_results)}/{len(result_set.results)}\")\n",
    "\n",
    "# Get results for specific model\n",
    "gpt_results = [r for r in result_set.results if r.answering_model == \"gpt-4.1-mini\"]\n",
    "print(f\"GPT results: {len(gpt_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Aggregate Metrics",
    "",
    "**Recommended: Use DataFrame helper methods** (see [Analyzing Results with DataFrames](analyzing-results-dataframes.md)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template metrics\n",
    "template_results = result_set.get_template_results()\n",
    "pass_rates = template_results.aggregate_pass_rate(by=\"question_id\")\n",
    "print(f\"Pass Rates: {pass_rates}\")\n",
    "\n",
    "# Or use pandas directly\n",
    "df = template_results.to_dataframe()\n",
    "if \"completed_without_errors\" in df.columns and \"field_match\" in df.columns:\n",
    "    successful = df[df[\"completed_without_errors\"] == True]\n",
    "    overall_accuracy = successful[\"field_match\"].mean()\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Automatic Database Storage",
    "",
    "Karenina can automatically save verification results to a database as they are generated. This is especially useful for production deployments and long-running verification jobs.",
    "",
    "### Configure Automatic Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Get or create temp directory\n",
    "if \"TEMP_DIR\" not in globals():\n",
    "    TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "from karenina.storage import DBConfig\n",
    "\n",
    "# Create database configuration\n",
    "db_config = DBConfig(\n",
    "    storage_url=f\"sqlite:///{temp_path('benchmarks.db')}\",\n",
    "    auto_create=True,  # Create tables if they don't exist\n",
    ")\n",
    "\n",
    "print(f\"Database configured: {temp_path('benchmarks.db')}\")\n",
    "print(\"Results will be automatically saved during verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Verification with Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_db = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"gpt-4.1-mini-judge\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    rubric_enabled=True,\n",
    "    db_config=db_config,  # Enable automatic database storage\n",
    ")\n",
    "\n",
    "print(\"Verification configured with automatic database storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How It Works",
    "",
    "1. When `db_config` is set in `VerificationConfig`, verification results are automatically saved to the specified database after completion",
    "2. The `AUTOSAVE_DATABASE` environment variable controls this behavior (defaults to `\"true\"`)",
    "3. Results are saved with metadata including run name, timestamp, and configuration details",
    "4. This happens transparently without requiring manual `save_to_db()` calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits",
    "",
    "- **No data loss**: Results are persisted immediately after verification completes",
    "- **Automatic**: No need to remember to call `save_to_db()` after verification",
    "- **Production-ready**: Ideal for automated pipelines and long-running jobs",
    "- **Queryable**: Results are immediately available for database queries and analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling Auto-Save",
    "",
    "To disable automatic database storage temporarily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Set db_config to None\n",
    "config_no_save = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"gpt-4.1-mini-judge\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    "    db_config=None,  # No automatic database storage\n",
    ")\n",
    "\n",
    "# Method 2: Use environment variable (in shell)\n",
    "# export AUTOSAVE_DATABASE=\"false\"\n",
    "\n",
    "print(\"Auto-save disabled examples shown above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed information about database storage options, see [Automatic Database Storage During Verification](saving-loading.md#automatic-database-storage-during-verification) and [Configuration](../configuration.md#database-configuration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Progress Tracking",
    "",
    "### Real-Time Progress Callback",
    "",
    "Monitor verification progress with a callback function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_callback(progress: float, message: str):\n",
    "    \"\"\"Called periodically during verification.\"\"\"\n",
    "    print(f\"Progress: {progress:.1%} - {message}\")\n",
    "\n",
    "\n",
    "# Run verification with progress tracking\n",
    "print(\"Starting verification with progress tracking...\")\n",
    "results_progress = benchmark.run_verification(config=config, progress_callback=progress_callback)\n",
    "print(\"\\nVerification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Answer Caching",
    "",
    "Karenina automatically caches answer generation to improve efficiency when multiple judge models evaluate the same answering model response.",
    "",
    "### How Answer Caching Works",
    "",
    "**Without Caching:**",
    "```",
    "1 question × 1 answering model × 3 judge models = 3 answer generations",
    "- Generate answer with Judge 1 → Parse with Judge 1",
    "- Generate answer with Judge 2 → Parse with Judge 2",
    "- Generate answer with Judge 3 → Parse with Judge 3",
    "Result: Same answer generated 3 times (wasteful, potentially inconsistent)",
    "```",
    "",
    "**With Caching (Automatic):**",
    "```",
    "1 question × 1 answering model × 3 judge models = 1 answer generation",
    "- Generate answer ONCE",
    "- Parse with Judge 1 (using cached answer)",
    "- Parse with Judge 2 (using cached answer)",
    "- Parse with Judge 3 (using cached answer)",
    "Result: Same answer reused 3 times (efficient, guaranteed consistent)",
    "```",
    "",
    "### Benefits",
    "",
    "1. **Efficiency**: Reduces LLM API calls and costs (generate once, evaluate many times)",
    "2. **Correctness**: Ensures all judges evaluate the exact same answer (important for fair comparison)",
    "3. **Speed**: Faster verification by avoiding redundant answer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Behavior",
    "",
    "The answer cache is:",
    "",
    "- **Automatic**: No configuration required, works transparently",
    "- **Thread-Safe**: Safe for parallel execution",
    "- **Per-Question**: Cache key includes question ID, answering model ID, and replicate number",
    "- **Replicate-Aware**: Each replicate gets independent answer generation",
    "",
    "**Cache Key Format:** `{question_id}_{answering_model_id}_{replicate}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching with Replication",
    "",
    "Each replicate run generates its own answer independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_cache = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "    ],\n",
    "    parsing_models=[  # 3 judges\n",
    "        ModelConfig(id=\"gpt-judge-1\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\"),\n",
    "        ModelConfig(id=\"gpt-judge-2\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\"),\n",
    "        ModelConfig(id=\"gpt-judge-3\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\"),\n",
    "    ],\n",
    "    replicate_count=2,  # 2 replicates\n",
    ")\n",
    "\n",
    "print(\"Answer caching with replication:\")\n",
    "print(\"  Total combinations: 1 question × 1 answering model × 3 judges × 2 replicates = 6 results\")\n",
    "print(\"  Answer generations: 1 question × 1 answering model × 2 replicates = 2 generations\")\n",
    "print(\"  Cache hits: 6 results - 2 generations = 4 cache reuses\")\n",
    "print(\"\")\n",
    "print(\"Result:\")\n",
    "print(\"  - Replicate 0: Answer generated once, reused by all 3 judges\")\n",
    "print(\"  - Replicate 1: New answer generated once, reused by all 3 judges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Complete Example",
    "",
    "Here's a complete end-to-end example demonstrating multi-model verification with replication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.schemas import LLMRubricTrait, ModelConfig, Rubric, VerificationConfig\n",
    "\n",
    "# Create a fresh benchmark for this example\n",
    "benchmark_complete = Benchmark.create(\n",
    "    name=\"Genomics Verification Demo\", description=\"Complete verification workflow example\", version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add questions\n",
    "demo_questions = [\n",
    "    (\"How many chromosomes are in a human somatic cell?\", \"46\"),\n",
    "    (\"What is the approved drug target of Venetoclax?\", \"BCL2\"),\n",
    "    (\"How many protein subunits does hemoglobin A have?\", \"4\"),\n",
    "]\n",
    "\n",
    "demo_qids = []\n",
    "for q, a in demo_questions:\n",
    "    qid = benchmark_complete.add_question(question=q, raw_answer=a)\n",
    "    demo_qids.append(qid)\n",
    "\n",
    "# Generate templates\n",
    "benchmark_complete.generate_all_templates(\n",
    "    model=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.1, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# Add a global rubric\n",
    "demo_rubric = Rubric(llm_traits=[LLMRubricTrait(name=\"Conciseness\", description=\"Rate conciseness 1-5\", kind=\"score\")])\n",
    "benchmark_complete.set_global_rubric(demo_rubric)\n",
    "\n",
    "print(\"Benchmark setup complete\")\n",
    "print(f\"  Questions: {len(demo_qids)}\")\n",
    "print(\"  Templates: Generated\")\n",
    "print(\"  Rubric: Global rubric with 1 trait\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure three answering models for comparison\n",
    "answering_models = [\n",
    "    ModelConfig(\n",
    "        id=\"gpt-4.1-mini\",\n",
    "        model_provider=\"openai\",\n",
    "        model_name=\"gpt-4.1-mini\",\n",
    "        temperature=0.7,\n",
    "        interface=\"langchain\",\n",
    "        system_prompt=\"You are a genomics expert. Answer concisely.\",\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"claude-sonnet\",\n",
    "        model_provider=\"anthropic\",\n",
    "        model_name=\"claude-sonnet-4.5\",\n",
    "        temperature=0.7,\n",
    "        interface=\"langchain\",\n",
    "        system_prompt=\"You are a genomics expert. Answer concisely.\",\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"gemini-flash\",\n",
    "        model_provider=\"google\",\n",
    "        model_name=\"gemini-2.5-flash\",\n",
    "        temperature=0.7,\n",
    "        interface=\"langchain\",\n",
    "        system_prompt=\"You are a genomics expert. Answer concisely.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Configure single judge model for consistent evaluation\n",
    "judge_model = ModelConfig(\n",
    "    id=\"gpt-judge\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,  # Deterministic parsing\n",
    "    interface=\"langchain\",\n",
    "    system_prompt=\"You are a strict evaluator. Parse carefully.\",\n",
    ")\n",
    "\n",
    "# Configure verification with replication\n",
    "config_complete = VerificationConfig(\n",
    "    answering_models=answering_models,\n",
    "    parsing_models=[judge_model],\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    rubric_enabled=True,\n",
    "    replicate_count=3,  # Run each combination 3 times\n",
    "    abstention_check_enabled=True,\n",
    ")\n",
    "\n",
    "print(\"Verification configured:\")\n",
    "print(f\"  Answering models: {len(answering_models)}\")\n",
    "print(\"  Judge models: 1\")\n",
    "print(f\"  Replicates: {config_complete.replicate_count}\")\n",
    "print(\n",
    "    f\"  Total results expected: {len(demo_qids)} × {len(answering_models)} × 1 × {config_complete.replicate_count} = {len(demo_qids) * len(answering_models) * config_complete.replicate_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress callback\n",
    "def show_progress(progress: float, message: str):\n",
    "    print(f\"[{progress:.0%}] {message}\")\n",
    "\n",
    "\n",
    "# Run verification\n",
    "print(\"Starting verification...\\n\")\n",
    "results_complete = benchmark_complete.run_verification(config=config_complete, progress_callback=show_progress)\n",
    "\n",
    "print(f\"\\nVerification complete: {len(results_complete.results)} results generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by model\n",
    "results_by_model = defaultdict(list)\n",
    "for result in results_complete.results:\n",
    "    results_by_model[result.answering_model].append(result)\n",
    "\n",
    "print(\"\\n=== Results by Answering Model ===\")\n",
    "for model_id, model_results in results_by_model.items():\n",
    "    passed = sum(1 for r in model_results if r.verify_result)\n",
    "    total = len(model_results)\n",
    "    accuracy = passed / total\n",
    "\n",
    "    print(f\"\\n{model_id}:\")\n",
    "    print(f\"  Template Accuracy: {accuracy:.1%} ({passed}/{total})\")\n",
    "\n",
    "    # Rubric averages\n",
    "    rubric_scores = defaultdict(list)\n",
    "    for r in model_results:\n",
    "        if r.rubric_evaluation_performed and r.verify_rubric:\n",
    "            for trait, score in r.verify_rubric.items():\n",
    "                rubric_scores[trait].append(score)\n",
    "\n",
    "    if rubric_scores:\n",
    "        print(\"  Rubric Averages:\")\n",
    "        for trait, scores in rubric_scores.items():\n",
    "            avg = sum(scores) / len(scores)\n",
    "            print(f\"    {trait}: {avg:.2f}\")\n",
    "\n",
    "    # Abstention rate\n",
    "    abstentions = sum(1 for r in model_results if r.abstention_detected)\n",
    "    abstention_rate = abstentions / total\n",
    "    print(f\"  Abstention Rate: {abstention_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Get or create temp directory\n",
    "if \"TEMP_DIR\" not in globals():\n",
    "    TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "from karenina.storage import DBConfig\n",
    "\n",
    "# Create database configuration\n",
    "db_config = DBConfig(\n",
    "    storage_url=f\"sqlite:///{temp_path('benchmarks.db')}\",\n",
    "    auto_create=True,  # Create tables if they don't exist\n",
    ")\n",
    "\n",
    "print(f\"Database configured: {temp_path('benchmarks.db')}\")\n",
    "print(\"Results will be automatically saved during verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Next Steps",
    "",
    "After running verification:",
    "",
    "- [Analyze Results](saving-loading.md#exporting-verification-results) - Export to CSV/JSON for deeper analysis",
    "- [Save Benchmark](saving-loading.md) - Persist results to database or checkpoint",
    "- [Advanced Features](../advanced/deep-judgment.md) - Use deep-judgment for detailed feedback",
    "- [Few-Shot Prompting](../advanced/few-shot.md) - Guide responses with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## Related Documentation",
    "",
    "- [Model Configuration](model-configuration.md) - Comprehensive guide to ModelConfig parameters and extra_kwargs",
    "- [Defining Benchmarks](defining-benchmark.md) - Creating and configuring benchmarks",
    "- [Templates](templates.md) - Structured answer evaluation",
    "- [Rubrics](rubrics.md) - Qualitative assessment criteria",
    "- [Saving & Loading](saving-loading.md) - Checkpoints, database, and export",
    "- [Deep Judgment](../advanced/deep-judgment.md) - Extract detailed feedback with excerpts",
    "- [Abstention Detection](../advanced/abstention-detection.md) - Handle model refusals",
    "- [Few-Shot Prompting](../advanced/few-shot.md) - Guide responses with examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
