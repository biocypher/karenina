{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "\n",
    "class MockLLMResponse:\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        self.subunits = kwargs.get(\"subunits\", 4)\n",
    "        self.diseases = kwargs.get(\"diseases\", [\"asthma\", \"bronchitis\", \"pneumonia\"])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def model_dump(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configuration\n",
    "\n",
    "This guide covers how to configure LLM models in Karenina using `ModelConfig`. You'll learn about model providers, interfaces, advanced parameters, and how to pass vendor-specific options.\n",
    "\n",
    "**Quick Navigation:**\n",
    "\n",
    "- [What is ModelConfig?](#what-is-modelconfig) - Core concepts and use cases\n",
    "- [Basic ModelConfig](#basic-modelconfig) - Minimal configuration example\n",
    "- [ModelConfig Parameters](#modelconfig-parameters) - Required and optional parameters\n",
    "- [Interfaces](#interfaces) - LangChain, OpenAI endpoint, OpenRouter, manual\n",
    "- [Model Providers](#model-providers) - OpenAI, Google, Anthropic configuration\n",
    "- [Temperature Parameter](#temperature-parameter) - Controlling randomness and determinism\n",
    "- [Extra Keyword Arguments](#extra-keyword-arguments) - Vendor-specific options and API keys\n",
    "- [System Prompts](#system-prompts) - Custom system prompt configuration\n",
    "- [MCP Tool Integration](#mcp-tool-integration) - Enable tool use during answer generation\n",
    "- [Common Configuration Patterns](#common-configuration-patterns) - Typical setup examples\n",
    "- [Best Practices](#best-practices) - Recommendations for benchmarking and API keys\n",
    "- [Troubleshooting](#troubleshooting) - Common errors and solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ModelConfig?\n",
    "\n",
    "`ModelConfig` is the configuration object that defines which LLM to use and how to interact with it. It's used in three key places:\n",
    "\n",
    "1. **Template generation**: LLMs that generate answer templates for questions\n",
    "2. **Answering models**: LLMs that generate responses to benchmark questions\n",
    "3. **Parsing models** (judges): LLMs that extract structured data from responses using templates\n",
    "\n",
    "A single `ModelConfig` can be used for all three roles, or you can use different models for each role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ModelConfig\n",
    "\n",
    "The simplest model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.schemas import ModelConfig\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    id=\"my-model\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.0, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "print(\"ModelConfig created:\")\n",
    "print(f\"  ID: {model_config.id}\")\n",
    "print(f\"  Model: {model_config.model_name}\")\n",
    "print(f\"  Provider: {model_config.model_provider}\")\n",
    "print(f\"  Interface: {model_config.interface}\")\n",
    "print(f\"  Temperature: {model_config.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelConfig Parameters\n",
    "\n",
    "### Required Parameters\n",
    "\n",
    "| Parameter | Type | Description | Example |\n",
    "|-----------|------|-------------|---------|\n",
    "| `id` | `str` | Unique identifier for this model configuration | `\"gpt-4.1-mini\"`, `\"my-custom-model\"` |\n",
    "| `model_name` | `str` | Full model name as recognized by the provider | `\"gpt-4.1-mini\"`, `\"claude-sonnet-4.5\"`, `\"gemini-2.5-flash\"` |\n",
    "| `interface` | `str` | Interface type (see [Interfaces](#interfaces)) | `\"langchain\"`, `\"openai_endpoint\"`, `\"openrouter\"`, `\"manual\"` |\n",
    "\n",
    "### Optional Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `model_provider` | `str` | Required for `langchain` | Provider name (see [Providers](#model-providers)) |\n",
    "| `temperature` | `float` | `0.1` | Sampling temperature (0.0-1.0). Use 0.0 for deterministic benchmarking |\n",
    "| `system_prompt` | `str` | `None` | Optional system prompt override |\n",
    "| `max_retries` | `int` | `2` | Maximum retry attempts for API calls |\n",
    "| `endpoint_base_url` | `str` | `None` | Custom endpoint URL (for `openai_endpoint` interface) |\n",
    "| `endpoint_api_key` | `SecretStr` | `None` | API key for custom endpoint (for `openai_endpoint` interface) |\n",
    "| `mcp_urls_dict` | `dict[str, str]` | `None` | MCP server URLs for tool use |\n",
    "| `mcp_tool_filter` | `list[str]` | `None` | Filter specific MCP tools |\n",
    "| `extra_kwargs` | `dict[str, Any]` | `None` | Additional keyword arguments (see [Extra Keyword Arguments](#extra-keyword-arguments)) |\n",
    "| `manual_traces` | `ManualTraces` | `None` | Pre-computed traces (for `manual` interface) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaces\n",
    "\n",
    "Karenina supports four interfaces for connecting to LLMs. Choose based on your use case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LangChain Interface (`langchain`)\n",
    "\n",
    "**Default and recommended interface** for most use cases. Uses LangChain's model integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", interface=\"langchain\", temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"LangChain interface configuration:\")\n",
    "print(\"  ✓ Uses LangChain's standardized model integrations\")\n",
    "print(\"  ✓ Built-in retry logic and error handling\")\n",
    "print(\"  ✓ API key from environment variable: OPENAI_API_KEY\")\n",
    "print(\"\\nSupported providers: openai, google_genai, anthropic, and more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use:**\n",
    "\n",
    "- ✅ Working with OpenAI, Google, Anthropic, or other LangChain-supported providers\n",
    "- ✅ Need standardized interface across multiple providers\n",
    "- ✅ Want built-in retry logic and error handling\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- API key must be set in environment variables (see [Configuration](../configuration.md#api-keys))\n",
    "- Or API key can be passed via `extra_kwargs` (see [Extra Keyword Arguments](#extra-keyword-arguments))\n",
    "\n",
    "**Supported providers**: `openai`, `google_genai`, `anthropic`, and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. OpenAI Endpoint Interface (`openai_endpoint`)\n",
    "\n",
    "Use this interface for **custom endpoints** that implement the OpenAI-compatible API (e.g., vLLM, Ollama, local models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration (for demonstration - endpoint would need to be running)\n",
    "# model_config = ModelConfig(\n",
    "#     id=\"local-model\",\n",
    "#     model_name=\"glm-4.6\",\n",
    "#     interface=\"openai_endpoint\",\n",
    "#     endpoint_base_url=\"http://localhost:8000/v1\",\n",
    "#     endpoint_api_key=\"dummy-key\",\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "print(\"OpenAI Endpoint interface configuration pattern:\")\n",
    "print(\"  ✓ For local models (vLLM, Ollama, etc.)\")\n",
    "print(\"  ✓ For custom inference servers\")\n",
    "print(\"  ✓ Requires endpoint_base_url to point to your server\")\n",
    "print(\"\\nExample parameters shown in commented code above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use:**\n",
    "\n",
    "- ✅ Running local models (vLLM, Ollama, etc.)\n",
    "- ✅ Using custom inference servers\n",
    "- ✅ OpenAI-compatible APIs\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- `endpoint_base_url` must point to your server\n",
    "- Some servers require `endpoint_api_key` (even if just a dummy value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. OpenRouter Interface (`openrouter`)\n",
    "\n",
    "Use OpenRouter for unified access to multiple LLM providers through a single API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration (requires OPENROUTER_API_KEY)\n",
    "# model_config = ModelConfig(\n",
    "#     id=\"claude-via-openrouter\",\n",
    "#     model_name=\"anthropic/claude-3.5-sonnet\",\n",
    "#     interface=\"openrouter\",\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "print(\"OpenRouter interface configuration pattern:\")\n",
    "print(\"  ✓ Unified billing across multiple providers\")\n",
    "print(\"  ✓ Easy switching between providers\")\n",
    "print(\"  ✓ Requires OPENROUTER_API_KEY environment variable\")\n",
    "print(\"\\nNote: model_provider not required - specified in model_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use:**\n",
    "\n",
    "- ✅ Want unified billing across multiple providers\n",
    "- ✅ Want to switch between providers easily\n",
    "\n",
    "**Requirements:**\n",
    "- `OPENROUTER_API_KEY` must be set in environment variables\n",
    "- pass the api key via `extra_kwargs`\n",
    "\n",
    "**Note**: `model_provider` is not required for OpenRouter interface since the provider is specified in the `model_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Manual Interface (`manual`)\n",
    "\n",
    "For testing and debugging with pre-computed responses (no LLM API calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.infrastructure.llm.manual_traces import ManualTraces\n",
    "\n",
    "demo_benchmark = Benchmark.create(name=\"Manual Traces Demo\", description=\"Demonstrating manual interface\")\n",
    "\n",
    "demo_benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)\n",
    "\n",
    "manual_traces = ManualTraces(demo_benchmark)\n",
    "\n",
    "manual_traces.register_trace(\"What is 2+2?\", \"The answer is 4\", map_to_id=True)\n",
    "\n",
    "model_config = ModelConfig(interface=\"manual\", manual_traces=manual_traces)\n",
    "\n",
    "print(\"Manual interface configuration:\")\n",
    "print(\"  ✓ No LLM API calls made\")\n",
    "print(\"  ✓ Uses pre-computed responses from traces\")\n",
    "print(\"  ✓ Ideal for testing and debugging\")\n",
    "print(\"  ✓ No API costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use:**\n",
    "\n",
    "- ✅ Testing workflows without API costs\n",
    "- ✅ Debugging specific scenarios\n",
    "- ✅ Evaluating pre-recorded LLM responses\n",
    "- ✅ Comparing different answer generation approaches\n",
    "\n",
    "See the **[Manual Traces Guide](../advanced/manual-traces.md)** for comprehensive documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Providers\n",
    "\n",
    "Model providers are specified with the `model_provider` parameter (required for `langchain` interface)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported Providers\n",
    "\n",
    "| Provider | Value | Example Models | API Key Required |\n",
    "|----------|-------|----------------|------------------|\n",
    "| OpenAI | `\"openai\"` | `gpt-4.1-mini`, `gpt-4.1-mini`, `gpt-4-turbo` | `OPENAI_API_KEY` |\n",
    "| Google | `\"google_genai\"` | `gemini-2.5-flash`, `gemini-2.5-pro` | `GOOGLE_API_KEY` |\n",
    "| Anthropic | `\"anthropic\"` | `claude-4-5-sonnet`, `claude-4-5-opus` | `ANTHROPIC_API_KEY` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", interface=\"langchain\", temperature=0.0\n",
    ")\n",
    "\n",
    "google_config = ModelConfig(\n",
    "    id=\"gemini-flash\",\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    model_provider=\"google_genai\",\n",
    "    interface=\"langchain\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "anthropic_config = ModelConfig(\n",
    "    id=\"claude-sonnet\",\n",
    "    model_name=\"claude-sonnet-4.5\",\n",
    "    model_provider=\"anthropic\",\n",
    "    interface=\"langchain\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"Provider configurations created:\")\n",
    "for config in [openai_config, google_config, anthropic_config]:\n",
    "    print(f\"  - {config.id}: {config.model_provider}/{config.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Parameter\n",
    "\n",
    "The `temperature` parameter controls randomness in model outputs:\n",
    "\n",
    "- **`0.0`** - Fully deterministic (recommended for benchmarking)\n",
    "- **`0.1-0.3`** - Low randomness (slight variation)\n",
    "- **`0.7-0.9`** - High randomness (creative responses)\n",
    "- **`1.0+`** - Maximum randomness\n",
    "\n",
    "**For benchmarking**: Always use `temperature=0.0` to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answering_model = ModelConfig(\n",
    "    id=\"answering\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", interface=\"langchain\", temperature=0.3\n",
    ")\n",
    "\n",
    "parsing_model = ModelConfig(\n",
    "    id=\"parsing\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", interface=\"langchain\", temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Temperature settings:\")\n",
    "print(f\"  Answering model: {answering_model.temperature} (low variation)\")\n",
    "print(f\"  Parsing model: {parsing_model.temperature} (fully deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Keyword Arguments\n",
    "\n",
    "The `extra_kwargs` field allows you to pass additional keyword arguments to the underlying model interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Passing API Key Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Passing API keys via extra_kwargs:\")\n",
    "print(\"  ✓ Alternative to environment variables\")\n",
    "print(\"  ✓ Useful for testing with multiple keys\")\n",
    "print(\"  ✓ Good for temporary key usage\")\n",
    "print(\"\\nNote: .env files still recommended for security\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Disabling Thinking Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Controlling model behavior with extra_kwargs:\")\n",
    "print(\"  ✓ Enable/disable thinking modes\")\n",
    "print(\"  ✓ Control reasoning separation\")\n",
    "print(\"  ✓ Pass vendor-specific options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Passing Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    id=\"custom-params\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    interface=\"langchain\",\n",
    "    temperature=0.0,\n",
    "    extra_kwargs={\"max_tokens\": 500, \"top_p\": 0.9, \"frequency_penalty\": 0.1, \"presence_penalty\": 0.1},\n",
    ")\n",
    "\n",
    "print(\"Generation parameters via extra_kwargs:\")\n",
    "print(f\"  max_tokens: {model_config.extra_kwargs['max_tokens']}\")\n",
    "print(f\"  top_p: {model_config.extra_kwargs['top_p']}\")\n",
    "print(f\"  frequency_penalty: {model_config.extra_kwargs['frequency_penalty']}\")\n",
    "print(f\"  presence_penalty: {model_config.extra_kwargs['presence_penalty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How extra_kwargs Works\n",
    "\n",
    "The arguments are passed to different places depending on the interface:\n",
    "\n",
    "| Interface | Where Arguments Go |\n",
    "|-----------|-------------------|\n",
    "| `langchain` | Passed to LangChain model constructor |\n",
    "| `openai_endpoint` | Passed to OpenAI client's chat completion call |\n",
    "| `openrouter` | Passed to OpenRouter API call |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts\n",
    "\n",
    "You can override the default system prompt for template generation, answering, or parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answering_model = ModelConfig(\n",
    "    id=\"biology-expert\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    interface=\"langchain\",\n",
    "    temperature=0.0,\n",
    "    system_prompt=\"You are a knowledgeable genomics expert. Provide detailed, accurate answers.\",\n",
    ")\n",
    "\n",
    "print(\"Custom system prompt configured:\")\n",
    "print(f\"  ID: {answering_model.id}\")\n",
    "print(f\"  System prompt: {answering_model.system_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Tool Integration\n",
    "\n",
    "Karenina supports Model Context Protocol (MCP) for tool access during answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MCP Tool Integration:\")\n",
    "print(\"  ✓ mcp_urls_dict: Maps tool categories to MCP server URLs\")\n",
    "print(\"  ✓ mcp_tool_filter: Optional whitelist of allowed tools\")\n",
    "print(\"\\nSupported interfaces: langchain, openai_endpoint, openrouter\")\n",
    "print(\"Not supported with: manual interface\")\n",
    "print(\"\\nFor comprehensive documentation, see MCP Integration Guide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Configuration Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Model for All Roles\n",
    "\n",
    "Use a single model configuration for template generation, answering, and parsing (simplest approach):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", interface=\"langchain\", temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Single model for all roles:\")\n",
    "print(f\"  ID: {model_config.id}\")\n",
    "print(\"  ✓ Template generation\")\n",
    "print(\"  ✓ Answering\")\n",
    "print(\"  ✓ Parsing\")\n",
    "print(\"\\nSimpler approach - same model everywhere\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Models for Different Roles\n",
    "\n",
    "Configure different models for specific tasks (optimal for cost/quality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answering_model = ModelConfig(\n",
    "    id=\"sonnet-4.5\", model_name=\"claude-4.5-sonnet\", model_provider=\"anthropic\", interface=\"langchain\", temperature=0.0\n",
    ")\n",
    "\n",
    "utility_model = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", interface=\"langchain\", temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Different models for different roles:\")\n",
    "print(f\"  Answering: {answering_model.id} ({answering_model.model_provider})\")\n",
    "print(f\"  Utility: {utility_model.id} ({utility_model.model_provider})\")\n",
    "print(\"\\nOptimal for cost/quality balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Multiple Models\n",
    "\n",
    "Create multiple model configurations for comparison testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = [\n",
    "    ModelConfig(\n",
    "        id=\"gpt-4.1-mini\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", interface=\"langchain\", temperature=0.0\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"claude-sonnet\",\n",
    "        model_name=\"claude-sonnet-4.5\",\n",
    "        model_provider=\"anthropic\",\n",
    "        interface=\"langchain\",\n",
    "        temperature=0.0,\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"gemini-flash\",\n",
    "        model_name=\"gemini-2.5-flash\",\n",
    "        model_provider=\"google_genai\",\n",
    "        interface=\"langchain\",\n",
    "        temperature=0.0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Models configured for comparison testing:\")\n",
    "for m in models_to_test:\n",
    "    print(f\"  - {m.id}: {m.model_provider}/{m.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### For Benchmarking\n",
    "\n",
    "- ✅ Always use `temperature=0.0` for reproducible results\n",
    "- ✅ Use the same parsing model across different answering models for fair comparison\n",
    "- ✅ Document your model configurations in your project README\n",
    "- ✅ Use descriptive `id` values (e.g., \"gpt-4.1-mini-biology-expert\")\n",
    "\n",
    "### For API Keys\n",
    "\n",
    "- ✅ Store API keys in `.env` files (see [Configuration](../configuration.md#api-keys))\n",
    "- ✅ Use different keys for development and production\n",
    "- ✅ Rotate keys regularly\n",
    "- ❌ Never commit API keys to version control\n",
    "- ⚠️ Only pass keys via `extra_kwargs` when necessary (testing, temporary use)\n",
    "\n",
    "### For Model Selection\n",
    "\n",
    "- ✅ Use `gpt-4.1-mini` as the default (fast, cost-effective)\n",
    "- ✅ Use `gpt-5` or `claude-4-5-sonnet` for higher quality (more expensive)\n",
    "- ✅ Use same model for all roles initially (simpler)\n",
    "- ✅ Optimize later: cheaper model for parsing/templates, expensive for answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### API Key Not Found\n",
    "\n",
    "**Solution**: Set the API key in environment variables:\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```\n",
    "\n",
    "Or pass directly via `extra_kwargs`:\n",
    "```python\n",
    "extra_kwargs={\"api_key\": \"sk-...\"}\n",
    "```\n",
    "\n",
    "### Invalid Model Name\n",
    "\n",
    "**Solution**: Check the correct model name for your provider:\n",
    "- OpenAI: `gpt-4.1-mini`, `gpt-4.1-mini`, `gpt-4-turbo`\n",
    "- Google: `gemini-2.5-flash`, `gemini-1.5-pro`\n",
    "- Anthropic: `claude-sonnet-4.5`, `claude-3-5-sonnet-20241022`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[Running Verification](verification.md)** - Learn about `VerificationConfig` and running benchmarks\n",
    "- **[Configuration Guide](../configuration.md)** - Environment variables and API key setup\n",
    "- **[Configuration Presets](../advanced/presets.md)** - Save and load model configurations\n",
    "- **[Manual Traces](../advanced/manual-traces.md)** - Detailed guide to pre-computed responses\n",
    "- **[MCP Integration](../advanced/mcp-integration.md)** - Comprehensive tool integration guide\n",
    "- **[Templates](templates.md)** - Generate and manage answer templates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}