{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"Mock setup complete\")\n",
    "print(f\"Temp directory: {TEMP_DIR}\")\n",
    "print(\"Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Benchmark\n",
    "\n",
    "The `Benchmark` class is the core component of Karenina. This page explains what it is, how to create benchmarks, what metadata can be associated with them, and how to persist them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Benchmark Class\n",
    "\n",
    "The `Benchmark` class is the central orchestrator for all benchmarking activities in Karenina. It:\n",
    "\n",
    "- **Manages collections of questions** and their associated templates\n",
    "- **Coordinates verification workflows** using LLM-as-a-judge patterns\n",
    "- **Handles serialization and persistence** through JSON-LD checkpoints\n",
    "- **Provides a unified interface** for benchmark creation, execution, and analysis\n",
    "\n",
    "Think of a benchmark as a structured container that brings together questions, evaluation templates, and execution configuration into a cohesive evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Create a Benchmark\n",
    "\n",
    "### Basic Creation\n",
    "\n",
    "Create a benchmark using the `Benchmark.create()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "# Create a basic benchmark\n",
    "benchmark = Benchmark.create(name=\"Genomics Knowledge Benchmark\")\n",
    "\n",
    "print(f\"Created: {benchmark.name}\")\n",
    "print(f\"Description: {benchmark.description}\")\n",
    "print(f\"Version: {benchmark.version}\")\n",
    "print(f\"Creator: {benchmark.creator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation with Metadata\n",
    "\n",
    "You can attach rich metadata to provide context and organization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    description=\"Testing LLM knowledge of genomics and molecular biology\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Dr. Jane Smith\",\n",
    ")\n",
    "\n",
    "print(f\"Benchmark: {benchmark.name}\")\n",
    "print(f\"Description: {benchmark.description}\")\n",
    "print(f\"Version: {benchmark.version}\")\n",
    "print(f\"Creator: {benchmark.creator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Parameters:**\n",
    "\n",
    "- **`name`** (required): Unique identifier for the benchmark\n",
    "- **`description`**: Human-readable explanation of the benchmark's purpose\n",
    "- **`version`**: Version string for tracking benchmark evolution (e.g., \"1.0.0\")\n",
    "- **`creator`**: Name or organization that created the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Metadata Attributes\n",
    "\n",
    "### Standard Metadata\n",
    "\n",
    "The following standard attributes can be set when creating a benchmark:\n",
    "\n",
    "| Attribute | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `name` | `str` | Unique identifier for the benchmark (required) |\n",
    "| `description` | `str` | Human-readable description of the benchmark's purpose |\n",
    "| `version` | `str` | Version string for tracking benchmark evolution |\n",
    "| `creator` | `str` | Creator or maintainer of the benchmark |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access standard attributes\n",
    "print(benchmark.name)  # \"Genomics Knowledge Benchmark\"\n",
    "print(benchmark.description)  # \"Testing LLM knowledge of...\"\n",
    "print(benchmark.version)  # \"1.0.0\"\n",
    "print(benchmark.creator)  # \"Dr. Jane Smith\"\n",
    "print(f\"Questions: {benchmark.question_count}\")  # Number of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Organization Patterns\n",
    "\n",
    "### Domain-Specific Benchmarks\n",
    "\n",
    "Organize benchmarks by domain to facilitate comparison and reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular biology benchmark\n",
    "molecular_bio_benchmark = Benchmark.create(\n",
    "    name=\"Molecular Biology Fundamentals\",\n",
    "    description=\"Tests understanding of core molecular biology concepts\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Biology Education Team\",\n",
    ")\n",
    "\n",
    "# Pharmacology benchmark\n",
    "pharmacology_benchmark = Benchmark.create(\n",
    "    name=\"Drug Mechanisms and Targets\",\n",
    "    description=\"Evaluates knowledge of drug targets and mechanisms of action\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Pharmacology Research Group\",\n",
    ")\n",
    "\n",
    "print(f\"Created: {molecular_bio_benchmark.name}\")\n",
    "print(f\"Created: {pharmacology_benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Version Benchmarks\n",
    "\n",
    "Track benchmark evolution by versioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1.0: Basic genomics questions\n",
    "genomics_v1 = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    version=\"1.0.0\",\n",
    "    description=\"Basic genomics questions covering chromosomes and DNA structure\",\n",
    "    creator=\"Dr. Jane Smith\",\n",
    ")\n",
    "\n",
    "# Version 2.0: Expanded with advanced topics\n",
    "genomics_v2 = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    version=\"2.0.0\",\n",
    "    description=\"Expanded genomics benchmark including epigenetics and gene regulation\",\n",
    "    creator=\"Dr. Jane Smith\",\n",
    ")\n",
    "\n",
    "print(f\"v1.0: {genomics_v1.name} - {genomics_v1.description}\")\n",
    "print(f\"v2.0: {genomics_v2.name} - {genomics_v2.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Persistence\n",
    "\n",
    "Karenina provides SQLite database storage for persistent benchmark management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Database\n",
    "\n",
    "Save your benchmark to a database with an optional checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create a test benchmark\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Test Genomics Benchmark\",\n",
    "    description=\"For demonstrating database persistence\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Documentation Example\",\n",
    ")\n",
    "\n",
    "# Add a sample question\n",
    "question_id = benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\", raw_answer=\"46\", finished=True\n",
    ")\n",
    "\n",
    "# Save to SQLite database (with checkpoint file)\n",
    "db_path = temp_path(\"benchmarks.db\")\n",
    "checkpoint_path = temp_path(\"genomics_benchmark.jsonld\")\n",
    "\n",
    "benchmark.save_to_db(storage=f\"sqlite:///{db_path}\", checkpoint_path=checkpoint_path)\n",
    "\n",
    "print(f\"Saved benchmark to database: {db_path}\")\n",
    "print(f\"Checkpoint file: {checkpoint_path}\")\n",
    "print(f\"Checkpoint exists: {checkpoint_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "\n",
    "- **`storage`**: Database connection string (e.g., `\"sqlite:///benchmarks.db\"`)\n",
    "- **`checkpoint_path`** (optional): Path to save a checkpoint file alongside the database entry\n",
    "\n",
    "**What gets stored:**\n",
    "\n",
    "- Benchmark metadata (name, description, version)\n",
    "- All questions with their metadata\n",
    "- Answer templates\n",
    "- Rubrics (global and question-specific)\n",
    "- Verification results (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Database\n",
    "\n",
    "Load a previously saved benchmark by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "# Load from database\n",
    "loaded_benchmark = Benchmark.load_from_db(benchmark_name=\"Test Genomics Benchmark\", storage=f\"sqlite:///{db_path}\")\n",
    "\n",
    "print(f\"Loaded: {loaded_benchmark.name}\")\n",
    "print(f\"Description: {loaded_benchmark.description}\")\n",
    "print(f\"Questions: {loaded_benchmark.question_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "\n",
    "- **`benchmark_name`**: Exact name of the benchmark to load\n",
    "- **`storage`**: Database connection string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Use Cases\n",
    "\n",
    "**Version Control:**\n",
    "Store multiple versions of the same benchmark with different version strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Save v1.0 and v2.0\n",
    "benchmark_v1 = Benchmark.create(name=\"My Benchmark\", version=\"1.0.0\", description=\"First version\")\n",
    "\n",
    "benchmark_v2 = Benchmark.create(name=\"My Benchmark\", version=\"2.0.0\", description=\"Updated version\")\n",
    "\n",
    "# Both can be saved to the same database\n",
    "print(\"v1.0:\", benchmark_v1.name, \"-\", benchmark_v1.version)\n",
    "print(\"v2.0:\", benchmark_v2.name, \"-\", benchmark_v2.version)\n",
    "\n",
    "# In practice:\n",
    "# benchmark_v1.save_to_db(storage=\"sqlite:///benchmarks.db\")\n",
    "# benchmark_v2.save_to_db(storage=\"sqlite:///benchmarks.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared Storage:**\n",
    "Multiple team members can access the same database to collaborate on benchmarks.\n",
    "\n",
    "**Automatic Verification Persistence:**\n",
    "When you run verification, results are automatically saved to the database if you provide a `storage` parameter in your `VerificationConfig`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Files\n",
    "\n",
    "Checkpoints are JSON-LD files that contain the complete state of a benchmark. Unlike database storage, checkpoints are portable files that can be easily shared, version-controlled, and inspected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Checkpoint\n",
    "\n",
    "Save your benchmark to a JSON-LD checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create a benchmark with a question\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Checkpoint Example Benchmark\", description=\"Demonstrating checkpoint save/load\", version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "benchmark.add_question(question=\"How many chromosomes are in a human somatic cell?\", raw_answer=\"46\", finished=True)\n",
    "\n",
    "# Save checkpoint (two equivalent methods)\n",
    "checkpoint_path = temp_path(\"genomics_benchmark.jsonld\")\n",
    "\n",
    "# Method 1: Using save()\n",
    "benchmark.save(checkpoint_path)\n",
    "\n",
    "print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
    "print(f\"File exists: {checkpoint_path.exists()}\")\n",
    "print(f\"File size: {checkpoint_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint\n",
    "\n",
    "Load a benchmark from a checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "# Load from checkpoint\n",
    "loaded = Benchmark.load(checkpoint_path)\n",
    "\n",
    "print(f\"Loaded benchmark: {loaded.name}\")\n",
    "print(f\"Description: {loaded.description}\")\n",
    "print(f\"Version: {loaded.version}\")\n",
    "print(f\"Total questions: {loaded.question_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Format\n",
    "\n",
    "Checkpoints use JSON-LD format following schema.org conventions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the raw JSON-LD structure\n",
    "\n",
    "with open(checkpoint_path) as f:\n",
    "    jsonld_data = json.load(f)\n",
    "\n",
    "# Display key fields\n",
    "print(\"JSON-LD Structure:\")\n",
    "print(f\"  @type: {jsonld_data.get('@type')}\")\n",
    "print(f\"  name: {jsonld_data.get('name')}\")\n",
    "print(f\"  version: {jsonld_data.get('version')}\")\n",
    "print(f\"  description: {jsonld_data.get('description')}\")\n",
    "print(f\"  creator: {jsonld_data.get('creator')}\")\n",
    "print(f\"  hasPart (questions): {len(jsonld_data.get('hasPart', []))} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Use Cases\n",
    "\n",
    "**Sharing Benchmarks:**\n",
    "Send checkpoint files to collaborators or publish them in repositories.\n",
    "\n",
    "**Version Control:**\n",
    "Track checkpoint files in Git to monitor benchmark evolution over time.\n",
    "\n",
    "**Portability:**\n",
    "Move benchmarks between systems without database dependencies.\n",
    "\n",
    "**Inspection:**\n",
    "Open checkpoint files in text editors to review benchmark structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this guide, you learned:\n",
    "\n",
    "- How to create benchmarks with `Benchmark.create()`\n",
    "- How to attach metadata (name, description, version, creator)\n",
    "- How to organize benchmarks by domain and version\n",
    "- How to save/load benchmarks using SQLite database storage\n",
    "- How to save/load checkpoints as portable JSON-LD files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once you have a benchmark defined, you can:\n",
    "\n",
    "- **Add questions** to populate it with evaluation content\n",
    "- **Set up templates** for structured evaluation\n",
    "- **Configure verification** to run assessments\n",
    "- **Save and load** benchmarks using checkpoints or database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
