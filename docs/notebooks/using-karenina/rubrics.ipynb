{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mock-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:18.146557Z",
     "iopub.status.busy": "2026-01-04T09:33:18.146470Z",
     "iopub.status.idle": "2026-01-04T09:33:20.297511Z",
     "shell.execute_reply": "2026-01-04T09:33:20.297102Z"
    },
    "jupyter": {
     "source_hidden": false
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mock setup complete\n",
      "✓ Temp directory: /var/folders/34/129m5tdd04vf10ptyj12w6f80000gp/T/karenina_docs_opgi7rlv\n",
      "✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\n"
     ]
    }
   ],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"BCL2\"):\n",
    "        self.content = content\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output for template parsing.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        self.subunits = kwargs.get(\"subunits\", 4)\n",
    "        self.diseases = kwargs.get(\"diseases\", [\"asthma\", \"bronchitis\", \"pneumonia\"])\n",
    "        self.mentions_bcl2_protein = kwargs.get(\"mentions_bcl2_protein\", True)\n",
    "        self.mentions_apoptosis_regulation = kwargs.get(\"mentions_apoptosis_regulation\", False)\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model(default_response: str = \"BCL2\"):\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(default_response)\n",
    "    mock.ainvoke.return_value = MockLLMResponse(default_response)\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "def create_mock_benchmark():\n",
    "    \"\"\"Create a mock Benchmark object for demonstrations.\"\"\"\n",
    "    from karenina import Benchmark\n",
    "\n",
    "    benchmark = Benchmark.create(\n",
    "        name=\"Demo Benchmark\", description=\"Mock benchmark for documentation\", version=\"1.0.0\", creator=\"Documentation\"\n",
    "    )\n",
    "    return benchmark\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "\n",
    "# Helper to replace file paths in examples\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Get a temporary file path for documentation examples.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup on kernel shutdown\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Rubrics\n\nRubrics provide qualitative evaluation criteria beyond the basic template verification. They enable assessment of answer traits like clarity, conciseness, safety, and domain-specific requirements.\n\n**Quick Navigation:**\n\n- [What Are Rubrics?](#what-are-rubrics) - Core concepts and capabilities\n- [Why Use Rubrics?](#why-use-rubrics) - Quality assessment, domain validation, compliance\n- [Rubric Scope](#rubric-scope) - Global vs question-specific rubrics\n- [Four Types of Rubric Traits](#four-types) - LLM-based, regex-based, callable-based, metric-based\n- [Creating a Global Rubric](#creating-global) - Apply traits to all questions\n- [Creating Question-Specific Rubrics](#question-specific) - Apply traits to specific questions\n- [Working with Rubric Results](#working-with-results) - Access and analyze evaluation results\n- [Complete Example](#complete-example) - End-to-end workflow with all trait types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "four-types",
   "metadata": {},
   "source": [
    "## Four Types of Rubric Traits\n\nKarenina supports four types of evaluation traits:\n\n| Trait Type | Description | Best For |\n|------------|-------------|----------|\n| **LLMRubricTrait** | AI-evaluated traits using LLM judgment | Subjective qualities (clarity, tone, completeness) |\n| **RegexTrait** | Pattern-matching traits using regular expressions | Format compliance, keyword presence/absence |\n| **CallableTrait** | Custom Python function-based traits | Complex validation logic (word counts, structure) |\n| **MetricRubricTrait** | Confusion matrix-based metrics for classification | Precision, recall, F1 for item identification |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-score-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.299120Z",
     "iopub.status.busy": "2026-01-04T09:33:20.298965Z",
     "iopub.status.idle": "2026-01-04T09:33:20.313186Z",
     "shell.execute_reply": "2026-01-04T09:33:20.312820Z"
    }
   },
   "outputs": [],
   "source": [
    "from karenina.schemas import LLMRubricTrait\n",
    "\n",
    "# Score-based trait: Measures conciseness on a 1-5 scale\n",
    "conciseness_trait = LLMRubricTrait(\n",
    "    name=\"Conciseness\",\n",
    "    description=\"Rate the conciseness of the answer on a scale from 1 (very verbose) to 5 (extremely concise).\",\n",
    "    kind=\"score\",\n",
    ")\n",
    "\n",
    "# Score-based trait: Measures clarity\n",
    "clarity_trait = LLMRubricTrait(\n",
    "    name=\"Clarity\",\n",
    "    description=\"Rate how clear and understandable the answer is, from 1 (confusing) to 5 (crystal clear).\",\n",
    "    kind=\"score\",\n",
    ")\n",
    "\n",
    "print(f\"✓ Created score-based traits: {conciseness_trait.name}, {clarity_trait.name}\")\n",
    "print(f\"  Conciseness range: {conciseness_trait.min_score}-{conciseness_trait.max_score}\")\n",
    "print(f\"  Clarity range: {clarity_trait.min_score}-{clarity_trait.max_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-mode",
   "metadata": {},
   "source": [
    "**Boolean Mode (pass/fail):**\n\n- Provides **yes/no judgment** on whether criteria are met\n- Returns: `true` (pass) or `false` (fail)\n- Best for: Questions with clear criteria that either are or aren't satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-code",
   "metadata": {},
   "source": [
    "# Binary trait: Checks for safety concerns\nsafety_trait = LLMRubricTrait(\n    name=\"Safety Concerns\",\n    description=\"Does the answer include any mention of safety concerns or warnings?\",\n    kind=\"boolean\"  # Note: use \"boolean\", not \"binary\"\n)\n\nprint(f\"✓ Created boolean trait: {safety_trait.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regex-traits",
   "metadata": {},
   "source": [
    "### 2. Regex-Based Traits (`RegexTrait`)\n\n**What they are:** Deterministic pattern-matching traits that check if answers match specific regex patterns. These provide **100% reproducible** validation without any LLM judgment.\n\n**When to use:**\n\n- Checking for **required terminology** or keywords\n- Validating **format compliance** (dates, gene symbols, IDs)\n- Detecting **prohibited content** (profanity, inappropriate terms)\n- Ensuring **specific patterns** are present or absent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regex-code",
   "metadata": {},
   "source": [
    "from karenina.schemas import RegexTrait\n\n# Answer must mention \"BH3 proteins\"\nbh3_trait = RegexTrait(\n    name=\"Mentions BH3 Proteins\",\n    description=\"Answer must mention BH3 proteins (the mechanism of BCL2 inhibition)\",\n    pattern=r\"\\bBH3\\b\",\n    case_sensitive=False,\n    invert_result=False  # Pass if pattern found\n)\n\nprint(f\"✓ Created regex trait: {bh3_trait.name}\")\n\n# Test evaluation\ntest_answer = \"The answer involves BH3 proteins binding to BCL2.\"\nresult = bh3_trait.evaluate(test_answer)\nprint(f\"  Test answer: {test_answer}\")\nprint(f\"  BH3 pattern found: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invert-table",
   "metadata": {},
   "source": [
    "**Using `invert_result` Parameter:**\n\nThe `invert_result` parameter changes the pass/fail logic:\n\n| `invert_result` | Pattern Matches | Result |\n|----------------|----------------|--------|\n| `False` (default) | Pattern **found** in answer | ✅ Pass |\n| `False` (default) | Pattern **NOT found** in answer | ❌ Fail |\n| `True` | Pattern **found** in answer | ❌ Fail |\n| `True` | Pattern **NOT found** in answer | ✅ Pass |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "callable-traits",
   "metadata": {},
   "source": [
    "### 3. Callable Traits (`CallableTrait`)\n\n**What they are:** Custom Python function-based traits for complex validation logic that cannot be expressed with regex patterns.\n\n**When to use:**\n\n- Complex validation requiring **Python logic** (word counts, sentence structure analysis)\n- **Custom scoring algorithms** beyond simple pattern matching\n- When regex patterns are too limited but you want deterministic evaluation\n\n**⚠️ SECURITY WARNING**: CallableTrait uses cloudpickle serialization. Only load from trusted sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "callable-code",
   "metadata": {},
   "source": [
    "# Define evaluation function\ndef check_word_count(text: str) -> bool:\n    \"\"\"Check if answer has at least 50 words.\"\"\"\n    return len(text.split()) >= 50\n\n# Create callable trait\nfrom karenina.schemas import CallableTrait\nword_count_trait = CallableTrait.from_callable(\n    name=\"Minimum Word Count\",\n    description=\"Answer must have at least 50 words\",\n    func=check_word_count,\n    kind=\"boolean\"\n)\n\nprint(f\"✓ Created callable trait: {word_count_trait.name}\")\n\n# Evaluate\nshort_answer = \"This is a short answer.\"\nlong_answer = \"This is a much longer answer with many more words. \" * 10\n\nprint(f\"  Short answer (fewer than 50 words): {word_count_trait.evaluate(short_answer)}\")\nprint(f\"  Long answer (more than 50 words): {word_count_trait.evaluate(long_answer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-traits",
   "metadata": {},
   "source": [
    "### 4. Metric-Based Traits (`MetricRubricTrait`)\n\nConfusion matrix-based traits for quantitative classification evaluation.\n\n**Available Metrics:**\n\n- **Precision**: TP / (TP + FP)\n- **Recall**: TP / (TP + FN)\n- **F1 Score**: 2 × (Precision × Recall) / (Precision + Recall)\n- **Accuracy**: (TP + TN) / (TP + TN + FP + FN)\n- **Specificity**: TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-code",
   "metadata": {},
   "source": [
    "from karenina.schemas import MetricRubricTrait\n\n# TP-Only Mode: Identifying inflammatory diseases\ninflammatory_trait = MetricRubricTrait(\n    name=\"Inflammatory Disease Identification\",\n    description=\"Evaluate accuracy of identifying inflammatory lung diseases\",\n    metrics=[\"precision\", \"recall\", \"f1\"],\n    tp_instructions=[\n        \"asthma\",\n        \"bronchitis\",\n        \"pneumonia\",\n        \"pleurisy\"\n    ],\n    fp_instructions=[\n        \"emphysema\",\n        \"pulmonary fibrosis\",\n        \"sarcoidosis\"\n    ]\n)\n\nprint(f\"✓ Created metric trait: {inflammatory_trait.name}\")\nprint(f\"  Evaluation mode: {inflammatory_trait.evaluation_mode}\")\nprint(f\"  Metrics: {inflammatory_trait.metrics}\")\nprint(f\"  TP instructions: {inflammatory_trait.tp_instructions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trait-comparison",
   "metadata": {},
   "source": [
    "### When to Use Each Trait Type\n\n| Need | Use This Trait Type |\n|------|---------------------|\n| Subjective quality assessment | LLM-Based (`LLMRubricTrait`) |\n| Exact keyword or format validation | Regex-Based (`RegexTrait`) |\n| Complex validation logic | Callable-Based (`CallableTrait`) |\n| Classification accuracy metrics | Metric-Based (`MetricRubricTrait`) |\n| Nuanced scoring (1-5) | LLM-Based (`LLMRubricTrait`, kind=\"score\") |\n| Yes/no judgment | LLM-Based (`LLMRubricTrait`, kind=\"boolean\") |\n| Deterministic pattern matching | Regex-Based (`RegexTrait`) |\n| Custom scoring algorithms | Callable-Based (`CallableTrait`, kind=\"score\") |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creating-global",
   "metadata": {},
   "source": [
    "## Creating a Global Rubric\n\nGlobal rubrics apply to **all questions** in your benchmark. They're perfect for general quality traits like clarity and conciseness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-rubric-code",
   "metadata": {},
   "source": [
    "from karenina import Benchmark\nfrom karenina.schemas import Rubric, LLMRubricTrait\n\n# Create benchmark\nbenchmark = Benchmark.create(name=\"Genomics Knowledge Benchmark\")\n\n# Add questions\nbenchmark.add_question(\n    question=\"How many chromosomes are in a human somatic cell?\",\n    raw_answer=\"46\"\n)\n\nbenchmark.add_question(\n    question=\"What is the approved drug target of Venetoclax?\",\n    raw_answer=\"BCL2\"\n)\n\n# Create global rubric with LLM-based traits\n# Note: use llm_traits, not traits\nglobal_rubric = Rubric(\n    name=\"Answer Quality Assessment\",\n    llm_traits=[\n        LLMRubricTrait(\n            name=\"Conciseness\",\n            description=\"Rate the conciseness of the answer on a scale from 1 (very verbose) to 5 (extremely concise).\",\n            kind=\"score\"\n        ),\n        LLMRubricTrait(\n            name=\"Clarity\",\n            description=\"Rate how clear and understandable the answer is, from 1 (confusing) to 5 (crystal clear).\",\n            kind=\"score\"\n        )\n    ]\n)\n\n# Set as global rubric - applies to ALL questions\nbenchmark.set_global_rubric(global_rubric)\n\nprint(f\"✓ Created benchmark with {len(benchmark.get_all_questions())} questions\")\nprint(f\"✓ Set global rubric with traits: {global_rubric.get_llm_trait_names()}\")\nprint(f\"  These traits will be evaluated for EVERY question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "question-specific",
   "metadata": {},
   "source": [
    "## Creating Question-Specific Rubrics\n\nQuestion-specific rubrics apply to **a single question only**. They're perfect for domain validation and specialized requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "question-specific-code",
   "metadata": {},
   "source": [
    "from karenina.schemas import Rubric, RegexTrait\n\n# This rubric is ONLY for the Venetoclax question\nvenetoclax_rubric = Rubric(\n    name=\"Drug Mechanism Validation\",\n    regex_traits=[  # Note: use regex_traits, not traits\n        RegexTrait(\n            name=\"Mentions BH3 Proteins\",\n            description=\"Answer must mention BH3 proteins\",\n            pattern=r\"\\bBH3\\b\",\n            case_sensitive=False\n        )\n    ]\n)\n\nbenchmark2 = Benchmark.create(name=\"Question-Specific Rubric Example\")\n\n# Add question with specific rubric\nbenchmark2.add_question(\n    question=\"What is the approved drug target of Venetoclax?\",\n    raw_answer=\"BCL2\",\n    rubric=venetoclax_rubric,\n)\n\nbenchmark2.add_question(\n    question=\"How many chromosomes in a human cell?\",\n    raw_answer=\"46\"\n)\n\nprint(f\"✓ Created benchmark with question-specific rubric\")\nprint(f\"  Venetoclax question has rubric: {venetoclax_rubric.get_regex_trait_names()}\")\nprint(f\"  Chromosomes question has no question-specific rubric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-with-results",
   "metadata": {},
   "source": [
    "## Working with Rubric Results\n\nAfter running verification with rubrics, you can access the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-code",
   "metadata": {},
   "source": [
    "from karenina.schemas import ModelConfig, VerificationConfig\n\n# Configure verification\nmodel_config = ModelConfig(\n    id=\"gpt-4.1-mini\",\n    model_provider=\"openai\",\n    model_name=\"gpt-4.1-mini\",\n    temperature=0.0,\n    interface=\"langchain\"\n)\n\nconfig = VerificationConfig(\n    answering_models=[model_config],\n    parsing_models=[model_config]\n)\n\n# Run verification (using our mock)\nresults = benchmark.run_verification(config)\n\n# Access rubric results for each question\nfor result in results:\n    print(f\"\\nQuestion: {result.metadata.question_text[:50]}...\")\n    \n    # LLM-based trait scores\n    if result.rubric.llm_trait_scores:\n        print(\"  LLM Trait Scores:\")\n        for trait_name, score in result.rubric.llm_trait_scores.items():\n            print(f\"    {trait_name}: {score}/5\")\n    \n    # Regex trait results\n    if result.rubric.regex_trait_results:\n        print(\"  Regex Trait Results:\")\n        for trait_name, passed in result.rubric.regex_trait_results.items():\n            status = \"✓ Pass\" if passed else \"✗ Fail\"\n            print(f\"    {trait_name}: {status}\")\n    \n    # Metric trait results\n    if result.rubric.metric_trait_metrics:\n        print(\"  Metric Trait Results:\")\n        for trait_name, metrics in result.rubric.metric_trait_metrics.items():\n            print(f\"    {trait_name}:\")\n            for metric_name, value in metrics.items():\n                print(f\"      {metric_name}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-example",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.318175Z",
     "iopub.status.busy": "2026-01-04T09:33:20.318101Z",
     "iopub.status.idle": "2026-01-04T09:33:20.320084Z",
     "shell.execute_reply": "2026-01-04T09:33:20.319620Z"
    }
   },
   "source": [
    "## Complete Example\n\nHere is a complete workflow showing both global and question-specific rubrics with all trait types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-code",
   "metadata": {},
   "source": [
    "from karenina import Benchmark\nfrom karenina.schemas import (\n    Rubric, LLMRubricTrait, RegexTrait, MetricRubricTrait,\n    ModelConfig, VerificationConfig\n)\n\n# 1. Create benchmark\nbenchmark = Benchmark.create(\n    name=\"Genomics Knowledge Benchmark\",\n    description=\"Testing LLM knowledge of genomics\",\n    version=\"1.0.0\"\n)\n\n# 2. Create global rubric (applies to ALL questions)\nglobal_rubric = Rubric(\n    name=\"General Quality Assessment\",\n    llm_traits=[\n        LLMRubricTrait(\n            name=\"Conciseness\",\n            description=\"Rate conciseness from 1 (verbose) to 5 (concise).\",\n            kind=\"score\"\n        ),\n        LLMRubricTrait(\n            name=\"Clarity\",\n            description=\"Rate clarity from 1 (confusing) to 5 (clear).\",\n            kind=\"score\"\n        )\n    ]\n)\n\nbenchmark.set_global_rubric(global_rubric)\n\n# 3. Add questions with different rubrics\nbenchmark.add_question(\n    question=\"How many chromosomes are in a human somatic cell?\",\n    raw_answer=\"46\"\n)\n\n# Question with regex-based question-specific rubric\nvenetoclax_rubric = Rubric(\n    name=\"Drug Mechanism Validation\",\n    regex_traits=[\n        RegexTrait(\n            name=\"Mentions BH3 Proteins\",\n            description=\"Answer must mention BH3 proteins\",\n            pattern=r\"\\bBH3\\b\",\n            case_sensitive=False\n        )\n    ]\n)\n\nbenchmark.add_question(\n    question=\"What is the approved drug target of Venetoclax?\",\n    raw_answer=\"BCL2\",\n    rubric=venetoclax_rubric,\n)\n\n# 4. Generate templates\nmodel_config = ModelConfig(\n    id=\"gpt-4.1-mini\",\n    model_provider=\"openai\",\n    model_name=\"gpt-4.1-mini\",\n    temperature=0.1,\n    interface=\"langchain\"\n)\n\nresults = benchmark.generate_all_templates(\n    model=\"gpt-4.1-mini\",\n    model_provider=\"openai\",\n    temperature=0.1,\n    interface=\"langchain\"\n)\n\nprint(f\"✓ Generated {len(results)} templates\")\n\n# 5. Save benchmark\nsave_path = temp_path(\"genomics_benchmark_with_rubrics.jsonld\")\nbenchmark.save(save_path)\n\nprint(f\"✓ Benchmark saved to {save_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n\nOnce you have rubrics configured:\n\n- [Run verification](verification.md) to apply both template and rubric evaluation\n- [Analyze results](verification.md#accessing-verification-results) to understand performance across different criteria\n- [Save and load benchmarks](saving-loading.md) to preserve your rubric configurations\n- [Export results](saving-loading.md#exporting-verification-results) to CSV or JSON for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-docs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T09:33:20.321084Z",
     "iopub.status.busy": "2026-01-04T09:33:20.321020Z",
     "iopub.status.idle": "2026-01-04T09:33:20.322875Z",
     "shell.execute_reply": "2026-01-04T09:33:20.322518Z"
    }
   },
   "source": [
    "## Related Documentation\n\n- [Adding Questions](adding-questions.md) - Populate your benchmark with questions\n- [Templates](templates.md) - Structured answer evaluation for factual correctness\n- [Verification](verification.md) - Run evaluations with multiple models\n- [Quick Start](../quickstart.md) - End-to-end workflow example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}