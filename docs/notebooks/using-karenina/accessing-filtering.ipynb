{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mock-setup",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from unittest.mock import Mock, MagicMock, patch\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import karenina\n",
    "from karenina import Benchmark\n",
    "\n",
    "# Create a sample benchmark for demonstration\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Access and Filtering Demo\",\n",
    "    description=\"Sample benchmark for demonstrating data access patterns\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Documentation\"\n",
    ")\n",
    "\n",
    "# Define sample questions with various metadata\n",
    "questions_data = [\n",
    "    {\n",
    "        \"question\": \"How many chromosomes are in the human genome?\",\n",
    "        \"raw_answer\": \"46\",\n",
    "        \"answer_template\": '''class Answer(BaseAnswer):\n",
    "    count: int = Field(description=\"The number of chromosomes\")\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"count\": 46}\n",
    "    def verify(self) -> bool:\n",
    "        return self.count == 46''',\n",
    "        \"finished\": True,\n",
    "        \"author\": {\"name\": \"Dr. Smith\", \"email\": \"smith@example.com\"},\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"biology\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"tags\": [\"genetics\", \"basics\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the target of the drug venetoclax?\",\n",
    "        \"raw_answer\": \"BCL2\",\n",
    "        \"answer_template\": '''class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"Target protein\")\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCL2\"}\n",
    "    def verify(self) -> bool:\n",
    "        return self.target.upper() == \"BCL2\"''',\n",
    "        \"finished\": True,\n",
    "        \"author\": {\"name\": \"Dr. Jones\", \"email\": \"jones@example.com\"},\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"pharmacology\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"tags\": [\"cancer\", \"target\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many subunits does hemoglobin have?\",\n",
    "        \"raw_answer\": \"4\",\n",
    "        \"answer_template\": '''class Answer(BaseAnswer):\n",
    "    subunits: int = Field(description=\"Number of subunits\")\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"subunits\": 4}\n",
    "    def verify(self) -> bool:\n",
    "        return self.subunits == 4''',\n",
    "        \"finished\": True,\n",
    "        \"author\": {\"name\": \"Dr. Smith\", \"email\": \"smith@example.com\"},\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"biology\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"tags\": [\"proteins\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"raw_answer\": \"A subset of artificial intelligence\",\n",
    "        \"finished\": False,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"computer science\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"tags\": [\"AI\", \"algorithms\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain quantum mechanics in simple terms\",\n",
    "        \"raw_answer\": \"The study of matter and energy at the smallest scales\",\n",
    "        \"finished\": False,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"physics\",\n",
    "            \"difficulty\": \"hard\",\n",
    "            \"tags\": [\"quantum\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Describe DNA replication and RNA synthesis processes\",\n",
    "        \"raw_answer\": \"DNA replication copies genetic material; RNA synthesis transcribes it\",\n",
    "        \"answer_template\": '''class Answer(BaseAnswer):\n",
    "    processes: list = Field(description=\"List of processes described\")\n",
    "    def verify(self) -> bool:\n",
    "        return \"replication\" in str(self.processes).lower() and \"synthesis\" in str(self.processes).lower()''',\n",
    "        \"finished\": True,\n",
    "        \"author\": {\"name\": \"Dr. Smith\", \"email\": \"smith@example.com\"},\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"biology\",\n",
    "            \"difficulty\": \"hard\",\n",
    "            \"tags\": [\"molecular biology\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is Python used for in data science?\",\n",
    "        \"raw_answer\": \"Data analysis, visualization, and machine learning\",\n",
    "        \"finished\": False,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"computer science\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"tags\": [\"Python\", \"data science\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Solve the calculus equation for the derivative\",\n",
    "        \"raw_answer\": \"The derivative is computed using chain rule\",\n",
    "        \"finished\": False,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"mathematics\",\n",
    "            \"difficulty\": \"hard\",\n",
    "            \"tags\": [\"calculus\", \"derivatives\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add questions to benchmark\n",
    "for q_data in questions_data:\n",
    "    benchmark.add_question(\n",
    "        question=q_data[\"question\"],\n",
    "        raw_answer=q_data[\"raw_answer\"],\n",
    "        answer_template=q_data.get(\"answer_template\"),\n",
    "        finished=q_data.get(\"finished\", False),\n",
    "        author=q_data.get(\"author\"),\n",
    "        custom_metadata=q_data.get(\"custom_metadata\", {})\n",
    "    )\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "def _cleanup():\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(f\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(f\"✓ Created benchmark with {len(benchmark)} questions\")\n",
    "print(f\"✓ Sample data ready for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing and Filtering Questions\n",
    "\n",
    "This guide covers how to access, filter, and search through questions in your benchmark for analysis and management.\n",
    "\n",
    "**Quick Navigation:**\n",
    "\n",
    "- [Accessing Questions](#accessing-questions) - Basic access patterns and iteration\n",
    "- [Filtering by Status](#filtering-by-status) - Finished vs unfinished, template status\n",
    "- [Searching Questions by Content](#searching-questions-by-content) - Text search, regex, advanced search\n",
    "- [Filtering by Metadata](#filtering-by-metadata) - Category, difficulty, multi-criteria filtering\n",
    "- [Sorting Questions](#sorting-questions) - Sort by metadata, content length\n",
    "- [Advanced Query Patterns](#advanced-query-patterns) - Complex filtering and statistics\n",
    "- [Bulk Operations](#bulk-operations-on-filtered-questions) - Update metadata, generate templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding Question Metadata\n",
    "\n",
    "Each question in a Karenina benchmark has two types of metadata:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Metadata (Built-in Fields)\n",
    "\n",
    "These are standard fields managed by Karenina:\n",
    "\n",
    "- `id` - Unique question identifier\n",
    "- `question` - The question text\n",
    "- `raw_answer` - The expected answer\n",
    "- `finished` - Boolean flag for template completion status\n",
    "- `answer_template` - The Answer class code for verification\n",
    "- `date_created` - Creation timestamp\n",
    "- `date_modified` - Last modification timestamp\n",
    "- `author` - Author information (optional dict)\n",
    "- `sources` - Source documents (optional list)\n",
    "- `question_rubric` - Question-specific rubric traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access system metadata directly from question dictionary\n",
    "question_ids = benchmark.get_question_ids()\n",
    "question = benchmark.get_question(question_ids[0])\n",
    "print(f\"Question ID: {question['id']}\")\n",
    "print(f\"Question text: {question['question']}\")\n",
    "print(f\"Finished status: {question.get('finished', False)}\")\n",
    "print(f\"Author: {question.get('author', {})}\")\n",
    "print(f\"Date created: {question.get('date_created', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metadata (User-defined Fields)\n",
    "\n",
    "The `custom_metadata` field is a **dictionary** where you can store any arbitrary key-value pairs specific to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access custom metadata\n",
    "question = benchmark.get_question(question_ids[0])\n",
    "custom = question.get(\"custom_metadata\", {})\n",
    "print(f\"Category: {custom.get('category')}\")\n",
    "print(f\"Difficulty: {custom.get('difficulty')}\")\n",
    "print(f\"Tags: {custom.get('tags', [])}\")\n",
    "\n",
    "# The custom_metadata structure is completely flexible\n",
    "# You can add any fields you need for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Built-in filtering methods (`filter_questions`) work with system metadata. For custom metadata, use the generic filtering methods described below.\n",
    "\n",
    "---\n",
    "\n",
    "## Built-in Methods Overview\n",
    "\n",
    "The Benchmark class provides several built-in methods for accessing and filtering questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Methods\n",
    "- `get_all_questions(ids_only)` - Get all questions (objects by default, IDs if `ids_only=True`)\n",
    "- `get_question(question_id)` - Get a specific question by ID\n",
    "- `get_question_ids()` - Get list of all question IDs\n",
    "\n",
    "### System Metadata Filtering\n",
    "- `filter_questions(finished, has_template, has_rubric, author, custom_filter)` - Filter by system fields or custom lambda\n",
    "- `get_unfinished_questions(ids_only)` - Get unfinished questions\n",
    "- `get_finished_questions(ids_only)` - Get finished questions\n",
    "- `get_questions_by_author(author)` - Filter by author name\n",
    "- `get_questions_with_rubric()` - Get questions with rubrics\n",
    "\n",
    "### Custom Metadata Filtering\n",
    "- `filter_by_custom_metadata(**criteria)` - Filter by custom fields with AND/OR logic\n",
    "- `filter_by_metadata(field_path, value, match_mode)` - Generic field filtering with dot notation\n",
    "- `count_by_field(field_path)` - Count questions by any field value\n",
    "\n",
    "### Search Methods\n",
    "- `search_questions(query, match_all, fields, case_sensitive, regex)` - Unified search supporting single/multi-term, regex, case-sensitive\n",
    "\n",
    "### Template Methods\n",
    "- `has_template(question_id)` - Check if question has a template\n",
    "- `get_missing_templates(ids_only)` - Get questions without templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Accessing Questions\n",
    "\n",
    "### Basic Access Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all questions as dictionaries\n",
    "all_questions = benchmark.get_all_questions()\n",
    "print(f\"Total questions: {len(all_questions)}\")\n",
    "\n",
    "# Get question count (using len)\n",
    "question_count = len(benchmark)\n",
    "print(f\"Question count via len(): {question_count}\")\n",
    "\n",
    "# Get list of question IDs\n",
    "question_ids = benchmark.get_question_ids()\n",
    "print(f\"Question IDs: {question_ids}\")\n",
    "\n",
    "# Get a specific question by ID\n",
    "question = benchmark.get_question(question_ids[0])\n",
    "print(f\"\\nFirst question: {question['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through questions\n",
    "print(\"All questions in the benchmark:\")\n",
    "for question in benchmark.get_all_questions():\n",
    "    status = \"✓\" if question.get('finished', False) else \"○\"\n",
    "    print(f\"  {status} {question['id'][:30]}...: {question['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square Bracket Access\n",
    "\n",
    "Karenina supports convenient square bracket notation for accessing questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Access by index - returns SchemaOrgQuestion object\\nquestion_obj = benchmark[0]\\nprint(f\\\"First question via index: {question_obj.text}\\\")\\n\\n# For dictionary access, use get_question() instead\\nquestion_dict = benchmark.get_question(benchmark.get_question_ids()[0])\\nprint(f\\\"\\\\nVia get_question(): {question_dict['question']}\\\")\\n\\n# Slice access - returns list of SchemaOrgQuestion objects\\nfirst_three = benchmark[0:3]\\nprint(f\\\"\\\\nFirst 3 questions: {[q.id[:30] for q in first_three]}\\\")\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Filtering by Status\n",
    "\n",
    "### Finished vs Unfinished Questions\n",
    "\n",
    "Questions are considered \"finished\" when they have both a template and verification results:\n",
    "\n",
    "> **Note:** When adding questions through the backend API, questions are marked as \"finished\" by default. The frontend GUI behaves differently and marks questions as \"unfinished\" until templates are generated. This distinction is important when programmatically creating benchmarks versus using the web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unfinished questions (returns list of question objects by default)\n",
    "unfinished_questions = benchmark.get_unfinished_questions()\n",
    "print(f\"Unfinished questions: {len(unfinished_questions)}\")\n",
    "\n",
    "# Iterate directly over the question objects\n",
    "print(\"\\nUnfinished questions:\")\n",
    "for question in unfinished_questions:\n",
    "    print(f\"  ○ {question['id'][:30]}...: {question['question'][:50]}...\")\n",
    "    print(f\"    Answer: {question.get('raw_answer', 'N/A')[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only question IDs if needed\n",
    "unfinished_ids = benchmark.get_unfinished_questions(ids_only=True)\n",
    "print(f\"Unfinished question IDs: {unfinished_ids}\")\n",
    "\n",
    "# Get finished questions\n",
    "finished_questions = benchmark.get_finished_questions()\n",
    "print(f\"\\nFinished questions: {len(finished_questions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status for all questions\n",
    "print(\"Status check for all questions:\")\n",
    "for question in benchmark.get_all_questions():\n",
    "    is_finished = question.get(\"finished\", False)\n",
    "    has_template = benchmark.has_template(question[\"id\"])\n",
    "    status = \"finished\" if is_finished and has_template else \"unfinished\"\n",
    "    print(f\"  {question['id'][:30]}...: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Status Filtering\n",
    "\n",
    "Use the built-in `filter_questions` method for template-based filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions with generated templates\n",
    "templated = benchmark.filter_questions(has_template=True)\n",
    "print(f\"Questions with templates: {len(templated)}\")\n",
    "\n",
    "# Questions needing templates\n",
    "needs_templates = benchmark.filter_questions(has_template=False)\n",
    "print(f\"Questions needing templates: {len(needs_templates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Status Filtering\n",
    "\n",
    "The `filter_questions` method supports multiple criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by finished status only\n",
    "finished = benchmark.filter_questions(finished=True)\n",
    "print(f\"Finished questions: {len(finished)}\")\n",
    "\n",
    "# Filter by multiple criteria (finished, has template)\n",
    "ready = benchmark.filter_questions(finished=True, has_template=True)\n",
    "print(f\"Finished with templates: {len(ready)}\")\n",
    "\n",
    "# Get all finished questions without templates\n",
    "needs_work = benchmark.filter_questions(finished=True, has_template=False)\n",
    "print(f\"Finished but needs templates: {len(needs_work)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Searching Questions by Content\n",
    "\n",
    "The `search_questions()` method provides flexible text search with support for single/multi-term queries, regex, and case-sensitive matching.\n",
    "\n",
    "### Simple Text Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search in question text (default)\n",
    "ml_questions = benchmark.search_questions(\"machine learning\")\n",
    "print(f\"Questions matching 'machine learning': {len(ml_questions)}\")\n",
    "for q in ml_questions:\n",
    "    print(f\"  - {q['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-term Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND logic: question must contain all terms\n",
    "quantum_mechanics = benchmark.search_questions([\"quantum\", \"mechanics\"], match_all=True)\n",
    "print(f\"Questions with 'quantum' AND 'mechanics': {len(quantum_mechanics)}\")\n",
    "for q in quantum_mechanics:\n",
    "    print(f\"  - {q['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR logic: question contains any term\n",
    "stem_terms = benchmark.search_questions([\"DNA\", \"RNA\", \"protein\"], match_all=False)\n",
    "print(f\"\\nQuestions with DNA, RNA, OR protein: {len(stem_terms)}\")\n",
    "for q in stem_terms:\n",
    "    print(f\"  - {q['id'][:30]}...: {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in Multiple Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search in both question and answer\n",
    "algorithm_content = benchmark.search_questions(\n",
    "    \"data\",\n",
    "    fields=[\"question\", \"raw_answer\"]\n",
    ")\n",
    "print(f\"Questions with 'data' in question or answer: {len(algorithm_content)}\")\n",
    "for q in algorithm_content:\n",
    "    print(f\"  - {q['id'][:30]}...: {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Search Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case-sensitive search\n",
    "python_qs = benchmark.search_questions(\"Python\", case_sensitive=True)\n",
    "print(f\"Questions with capital 'Python': {len(python_qs)}\")\n",
    "for q in python_qs:\n",
    "    print(f\"  - {q['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex search\n",
    "explanation_qs = benchmark.search_questions(r\"\\b(explain|describe|what is)\\b\", regex=True)\n",
    "print(f\"Questions asking for explanation: {len(explanation_qs)}\")\n",
    "for q in explanation_qs:\n",
    "    print(f\"  - {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Filtering by Metadata\n",
    "\n",
    "### Filtering by System Metadata\n",
    "\n",
    "Filter by built-in Karenina fields using `filter_questions()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by finished status\n",
    "finished = benchmark.filter_questions(finished=True)\n",
    "unfinished = benchmark.filter_questions(finished=False)\n",
    "print(f\"Finished: {len(finished)}, Unfinished: {len(unfinished)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by author\n",
    "johns_questions = benchmark.filter_questions(author=\"Dr. Smith\")\n",
    "print(f\"\\nQuestions by Dr. Smith: {len(johns_questions)}\")\n",
    "for q in johns_questions:\n",
    "    print(f\"  - {q['question'][:50]}...\")\n",
    "\n",
    "# Or use the convenience method\n",
    "johns_questions_alt = benchmark.get_questions_by_author(\"Dr. Smith\")\n",
    "print(f\"(via convenience method: {len(johns_questions_alt)} questions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by Custom Metadata\n",
    "\n",
    "Use built-in methods to filter by your custom metadata fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by single custom metadata field (AND logic)\n",
    "bio_easy = benchmark.filter_by_custom_metadata(category=\"biology\", difficulty=\"easy\")\n",
    "print(f\"Biology + Easy questions: {len(bio_easy)}\")\n",
    "for q in bio_easy:\n",
    "    print(f\"  - {q['id'][:30]}...: {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR logic for custom metadata (match any criterion)\n",
    "stem_subjects = benchmark.filter_by_custom_metadata(\n",
    "    match_all=False,\n",
    "    category=\"mathematics\",\n",
    "    category2=\"physics\"\n",
    ")\n",
    "print(f\"Math OR Physics questions: {len(stem_subjects)}\")\n",
    "for q in stem_subjects:\n",
    "    print(f\"  - {q['id'][:30]}...: {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using generic field path filtering with dot notation\n",
    "bio_qs = benchmark.filter_by_metadata(\"custom_metadata.category\", \"biology\")\n",
    "print(f\"Biology questions: {len(bio_qs)}\")\n",
    "\n",
    "# Filter by value in a list (for tags/arrays)\n",
    "genetics_tagged = benchmark.filter_by_metadata(\"custom_metadata.tags\", \"genetics\", match_mode=\"in\")\n",
    "print(f\"\\nQuestions with 'genetics' tag: {len(genetics_tagged)}\")\n",
    "for q in genetics_tagged:\n",
    "    print(f\"  - {q['id'][:30]}...: {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substring matching\n",
    "bio_qs = benchmark.filter_by_metadata(\"custom_metadata.category\", \"bio\", match_mode=\"contains\")\n",
    "print(f\"Category containing 'bio': {len(bio_qs)}\")\n",
    "\n",
    "# Regex matching on custom fields\n",
    "hard_qs = benchmark.filter_by_metadata(\"custom_metadata.difficulty\", r\"(hard|advanced)\", match_mode=\"regex\")\n",
    "print(f\"Hard/Advanced questions: {len(hard_qs)}\")\n",
    "for q in hard_qs:\n",
    "    print(f\"  - {q['id'][:30]}...: {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Custom Filtering with Lambda\n",
    "\n",
    "For complex logic, use the `custom_filter` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex logic on custom metadata\n",
    "bio_hard = benchmark.filter_questions(\n",
    "    custom_filter=lambda q: (\n",
    "        q.get(\"custom_metadata\", {}).get(\"category\") == \"biology\" and\n",
    "        q.get(\"custom_metadata\", {}).get(\"difficulty\") == \"hard\"\n",
    "    )\n",
    ")\n",
    "print(f\"Biology + Hard questions: {len(bio_hard)}\")\n",
    "for q in bio_hard:\n",
    "    print(f\"  - {q['id'][:30]}...: {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine system and custom metadata filtering\n",
    "hard_finished = benchmark.filter_questions(\n",
    "    finished=True,\n",
    "    custom_filter=lambda q: q.get(\"custom_metadata\", {}).get(\"difficulty\") == \"easy\"\n",
    ")\n",
    "print(f\"\\nFinished + Easy questions: {len(hard_finished)}\")\n",
    "for q in hard_finished:\n",
    "    print(f\"  - {q['id'][:30]}...: {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics with Custom Metadata\n",
    "\n",
    "Use `count_by_field()` for statistics on any field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by custom metadata field\n",
    "category_counts = benchmark.count_by_field(\"custom_metadata.category\")\n",
    "print(\"Category distribution:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"  {category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count finished vs unfinished\n",
    "status_counts = benchmark.count_by_field(\"finished\")\n",
    "print(f\"\\nStatus distribution: {status_counts}\")\n",
    "\n",
    "# Count difficulty distribution\n",
    "difficulty_counts = benchmark.count_by_field(\"custom_metadata.difficulty\")\n",
    "print(f\"\\nDifficulty distribution: {difficulty_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count on filtered subset\n",
    "bio_qs = benchmark.filter_by_custom_metadata(category=\"biology\")\n",
    "bio_difficulty_counts = benchmark.count_by_field(\"custom_metadata.difficulty\", questions=bio_qs)\n",
    "print(\"Biology questions by difficulty:\")\n",
    "for difficulty, count in bio_difficulty_counts.items():\n",
    "    print(f\"  {difficulty}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sorting Questions\n",
    "\n",
    "You can sort questions using Python's `sorted()` function with custom key functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all questions first\n",
    "questions = benchmark.get_all_questions()\n",
    "\n",
    "# Sort by custom metadata with custom order\n",
    "difficulty_order = {\"easy\": 1, \"medium\": 2, \"hard\": 3}\n",
    "sorted_by_difficulty = sorted(\n",
    "    questions,\n",
    "    key=lambda q: difficulty_order.get(\n",
    "        q.get(\"custom_metadata\", {}).get(\"difficulty\", \"medium\"), 2\n",
    "    )\n",
    ")\n",
    "print(\"Questions sorted by difficulty:\")\n",
    "for q in sorted_by_difficulty:\n",
    "    difficulty = q.get(\"custom_metadata\", {}).get(\"difficulty\", \"unknown\")\n",
    "    print(f\"  [{difficulty}] {q['question'][:40]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by category alphabetically\n",
    "sorted_by_category = sorted(\n",
    "    questions,\n",
    "    key=lambda q: q.get(\"custom_metadata\", {}).get(\"category\", \"\")\n",
    ")\n",
    "print(\"\\nQuestions sorted by category:\")\n",
    "for q in sorted_by_category:\n",
    "    category = q.get(\"custom_metadata\", {}).get(\"category\", \"unknown\")\n",
    "    print(f\"  [{category}] {q['id'][:30]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by question length\n",
    "sorted_by_length = sorted(questions, key=lambda q: len(q.get(\"question\", \"\")))\n",
    "print(\"\\nQuestions sorted by length (shortest first):\")\n",
    "for q in sorted_by_length:\n",
    "    print(f\"  [{len(q['question'])} chars] {q['question'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced Query Patterns\n",
    "\n",
    "### Combining Filters and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First filter, then search within results\n",
    "bio_questions = benchmark.filter_by_custom_metadata(category=\"biology\")\n",
    "bio_with_genetics = [\n",
    "    q for q in bio_questions\n",
    "    if \"genetics\" in str(q.get(\"custom_metadata\", {}).get(\"tags\", [])).lower()\n",
    "]\n",
    "print(f\"Biology questions with genetics tag: {len(bio_with_genetics)}\")\n",
    "for q in bio_with_genetics:\n",
    "    print(f\"  - {q['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use lambda for the same thing\n",
    "bio_genetics_lambda = benchmark.filter_questions(\n",
    "    custom_filter=lambda q: (\n",
    "        q.get(\"custom_metadata\", {}).get(\"category\") == \"biology\" and\n",
    "        \"genetics\" in str(q.get(\"custom_metadata\", {}).get(\"tags\", [])).lower()\n",
    "    )\n",
    ")\n",
    "print(f\"\\nVia lambda: {len(bio_genetics_lambda)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of any field\n",
    "category_dist = benchmark.count_by_field(\"custom_metadata.category\")\n",
    "print(\"Category distribution:\")\n",
    "for cat, count in sorted(category_dist.items()):\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "difficulty_dist = benchmark.count_by_field(\"custom_metadata.difficulty\")\n",
    "print(\"\\nDifficulty distribution:\")\n",
    "for diff, count in sorted(difficulty_dist.items()):\n",
    "    print(f\"  {diff}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bulk Operations on Filtered Questions\n",
    "\n",
    "### Update System Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark all finished questions as unfinished\n",
    "# Use ids_only=True since mark_unfinished_batch expects IDs\n",
    "finished_ids = benchmark.get_finished_questions(ids_only=True)\n",
    "print(f\"Finished question IDs: {finished_ids}\")\n",
    "\n",
    "# In practice, you would call:\n",
    "# benchmark.mark_unfinished_batch(finished_ids)\n",
    "print(\"\\n(Note: mark_unfinished_batch would be called here in practice)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update author for specific questions\n",
    "bio_qs = benchmark.filter_by_custom_metadata(category=\"biology\")\n",
    "print(f\"Biology questions to update author for: {len(bio_qs)}\")\n",
    "\n",
    "# In practice, you would iterate and update:\n",
    "# for q in bio_qs:\n",
    "#     benchmark.set_question_author(q[\"id\"], {\"name\": \"Bio Team\", \"email\": \"bio@example.com\"})\n",
    "print(\"(Author update would be performed here in practice)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Custom Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tags to all biology questions\n",
    "bio_qs = benchmark.filter_by_custom_metadata(category=\"biology\")\n",
    "print(f\"Biology questions to tag: {len(bio_qs)}\")\n",
    "\n",
    "for question in bio_qs:\n",
    "    question_id = question[\"id\"]\n",
    "    # Get current custom metadata\n",
    "    custom_meta = benchmark.get_question_metadata(question_id).get(\"custom_metadata\", {})\n",
    "    print(f\"  {question_id[:30]}...: current tags = {custom_meta.get('tags', [])}\")\n",
    "    \n",
    "    # In practice, you would add the tag:\n",
    "    # if \"tags\" not in custom_meta:\n",
    "    #     custom_meta[\"tags\"] = []\n",
    "    # if \"reviewed\" not in custom_meta[\"tags\"]:\n",
    "    #     custom_meta[\"tags\"].append(\"reviewed\")\n",
    "    # benchmark.update_question_metadata(question_id, custom_metadata=custom_meta)\n",
    "\n",
    "print(\"\\n(Tag update would be performed here in practice)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use the convenience method for single properties\n",
    "for q in bio_qs:\n",
    "    # In practice:\n",
    "    # benchmark.set_question_custom_property(q[\"id\"], \"reviewed\", True)\n",
    "    pass\n",
    "\n",
    "print(\"Convenience method example: set_question_custom_property(question_id, 'reviewed', True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Templates for Filtered Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate templates only for unfinished questions\n",
    "# Use ids_only=True since generate_templates expects a list of IDs\n",
    "unfinished_ids = benchmark.get_unfinished_questions(ids_only=True)\n",
    "print(f\"Unfinished question IDs needing templates: {unfinished_ids}\")\n",
    "\n",
    "# In practice, use the bulk generation method:\n",
    "# results = benchmark.generate_templates(\n",
    "#     question_ids=unfinished_ids,\n",
    "#     model=\"gemini-2.0-flash\",\n",
    "#     model_provider=\"google_genai\",\n",
    "#     temperature=0\n",
    "# )\n",
    "\n",
    "# Check results\n",
    "# successful = sum(1 for r in results.values() if r[\"success\"])\n",
    "# print(f\"Generated {successful}/{len(unfinished_ids)} templates\")\n",
    "\n",
    "print(\"\\n(Template generation would be performed here in practice)\")\n",
    "print(\"Requires LLM API credentials to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once you can effectively access and filter questions:\n",
    "\n",
    "- [Set up templates](templates.md) for evaluation structure\n",
    "- [Configure rubrics](rubrics.md) for assessment criteria\n",
    "- [Run verification](verification.md) to evaluate responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}