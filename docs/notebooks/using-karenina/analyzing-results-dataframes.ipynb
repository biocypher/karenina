{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hide-cell": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mock setup complete\n",
      "✓ Loaded 10 mock verification results\n",
      "✓ Results from 2 models: ['claude-3-5-sonnet', 'gpt-4o-mini']\n",
      "✓ 5 questions available for analysis\n"
     ]
    }
   ],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unittest.mock import Mock, MagicMock, patch, PropertyMock\n",
    "from typing import Any, Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultDeepJudgment,\n",
    "    VerificationResultDeepJudgmentRubric,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "from karenina.schemas.workflow.rubric_results import RubricResults\n",
    "from karenina.schemas.workflow.judgment_results import JudgmentResults\n",
    "\n",
    "# Import pandas for DataFrame operations\n",
    "import pandas as pd\n",
    "\n",
    "def create_mock_results():\n",
    "    \"\"\"Create realistic mock verification results for DataFrame analysis examples.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Mock data for different questions\n",
    "    mock_scenarios = [\n",
    "        {\n",
    "            \"question_id\": \"q_chromosomes\",\n",
    "            \"question_text\": \"How many chromosomes do humans have?\",\n",
    "            \"raw_response\": \"Humans have 46 chromosomes in total, arranged in 23 pairs.\",\n",
    "            \"parsed_gt\": {\"count\": 46, \"unit\": \"chromosomes\"},\n",
    "            \"parsed_llm\": {\"count\": 46, \"unit\": \"chromosomes\"},\n",
    "            \"verify_result\": True,\n",
    "            \"llm_traits\": {\"Accuracy\": 5, \"Clarity\": 4, \"Completeness\": 4},\n",
    "            \"embedding_score\": 0.95,\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"q_drug_target\",\n",
    "            \"question_text\": \"What is the target of venetoclax?\",\n",
    "            \"raw_response\": \"Venetoclax targets the BCL2 protein, which regulates apoptosis.\",\n",
    "            \"parsed_gt\": {\"target\": \"BCL2\", \"class\": \"BCL-2 inhibitor\"},\n",
    "            \"parsed_llm\": {\"target\": \"BCL2\", \"class\": \"BCL-2 inhibitor\"},\n",
    "            \"verify_result\": True,\n",
    "            \"llm_traits\": {\"Accuracy\": 5, \"Clarity\": 5, \"Completeness\": 5},\n",
    "            \"embedding_score\": 0.92,\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"q_hemoglobin\",\n",
    "            \"question_text\": \"How many subunits does hemoglobin have?\",\n",
    "            \"raw_response\": \"Hemoglobin has 4 subunits consisting of 2 alpha and 2 beta chains.\",\n",
    "            \"parsed_gt\": {\"subunits\": 4, \"composition\": \"2 alpha, 2 beta\"},\n",
    "            \"parsed_llm\": {\"subunits\": 4, \"composition\": \"2 alpha, 2 beta\"},\n",
    "            \"verify_result\": True,\n",
    "            \"llm_traits\": {\"Accuracy\": 5, \"Clarity\": 4, \"Completeness\": 5},\n",
    "            \"embedding_score\": 0.88,\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"q_inflammatory_lung\",\n",
    "            \"question_text\": \"Name three inflammatory lung diseases.\",\n",
    "            \"raw_response\": \"Three inflammatory lung diseases are asthma, bronchitis, and pneumonia.\",\n",
    "            \"parsed_gt\": {\"diseases\": [\"asthma\", \"bronchitis\", \"pneumonia\"]},\n",
    "            \"parsed_llm\": {\"diseases\": [\"asthma\", \"bronchitis\", \"pneumonia\"]},\n",
    "            \"verify_result\": True,\n",
    "            \"llm_traits\": {\"Accuracy\": 4, \"Clarity\": 5, \"Completeness\": 4},\n",
    "            \"embedding_score\": 0.85,\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"q_partial_fail\",\n",
    "            \"question_text\": \"What is the molecular weight of insulin?\",\n",
    "            \"raw_response\": \"Insulin has a molecular weight of approximately 5800 Daltons.\",\n",
    "            \"parsed_gt\": {\"weight\": 5808, \"unit\": \"Daltons\"},\n",
    "            \"parsed_llm\": {\"weight\": 5800, \"unit\": \"Daltons\"},\n",
    "            \"verify_result\": False,  # Slight mismatch\n",
    "            \"llm_traits\": {\"Accuracy\": 3, \"Clarity\": 5, \"Completeness\": 4},\n",
    "            \"embedding_score\": 0.90,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Create second model results for comparison\n",
    "    for model in [\"gpt-4o-mini\", \"claude-3-5-sonnet\"]:\n",
    "        for scenario in mock_scenarios:\n",
    "            # Vary results slightly by model\n",
    "            is_gpt = model == \"gpt-4o-mini\"\n",
    "            \n",
    "            timestamp = datetime.now().isoformat()\n",
    "            template_id = hashlib.md5(f\"{scenario['question_id']}_{model}\".encode()).hexdigest()[:32]\n",
    "            \n",
    "            # Compute result_id\n",
    "            result_data = {\n",
    "                \"answering_mcp_servers\": [],\n",
    "                \"answering_model\": model,\n",
    "                \"parsing_model\": model,\n",
    "                \"question_id\": scenario['question_id'],\n",
    "                \"replicate\": None,\n",
    "                \"timestamp\": timestamp,\n",
    "            }\n",
    "            json_str = json.dumps(result_data, sort_keys=True, ensure_ascii=True)\n",
    "            result_id = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()[:16]\n",
    "            \n",
    "            # Create template result\n",
    "            template = VerificationResultTemplate(\n",
    "                raw_llm_response=scenario['raw_response'],\n",
    "                parsed_llm_response=scenario['parsed_llm'],\n",
    "                parsed_gt_response=scenario['parsed_gt'],\n",
    "                verify_result=scenario['verify_result'] if is_gpt else True,\n",
    "                template_verification_performed=True,\n",
    "                usage_metadata={\n",
    "                    \"answer_generation\": {\"total_tokens\": 50, \"input_tokens\": 30, \"output_tokens\": 20},\n",
    "                    \"parsing\": {\"total_tokens\": 30, \"input_tokens\": 15, \"output_tokens\": 15},\n",
    "                    \"total\": {\"total_tokens\": 80, \"input_tokens\": 45, \"output_tokens\": 35}\n",
    "                },\n",
    "                abstention_check_performed=True,\n",
    "                abstention_detected=False,\n",
    "                embedding_check_performed=True,\n",
    "                embedding_similarity_score=scenario['embedding_score'] - (0.05 if not is_gpt else 0),\n",
    "                embedding_model_used=\"text-embedding-3-small\",\n",
    "                regex_validations_performed=False,\n",
    "                recursion_limit_reached=False,\n",
    "                answering_mcp_servers=[],\n",
    "            )\n",
    "            \n",
    "            # Create rubric result\n",
    "            rubric = VerificationResultRubric(\n",
    "                rubric_evaluation_performed=True,\n",
    "                llm_trait_scores=scenario['llm_traits'],\n",
    "                regex_trait_scores={},\n",
    "                callable_trait_scores={},\n",
    "                metric_trait_scores={},\n",
    "                metric_trait_confusion_lists={},\n",
    "            )\n",
    "            \n",
    "            # Create metadata\n",
    "            metadata = VerificationResultMetadata(\n",
    "                question_id=scenario['question_id'],\n",
    "                template_id=template_id,\n",
    "                completed_without_errors=True,\n",
    "                question_text=scenario['question_text'],\n",
    "                raw_answer=scenario['raw_response'],\n",
    "                answering_model=model,\n",
    "                parsing_model=model,\n",
    "                execution_time=1.5 if is_gpt else 2.0,\n",
    "                timestamp=timestamp,\n",
    "                result_id=result_id,\n",
    "                keywords=[],\n",
    "                replicate=None,\n",
    "                answering_system_prompt=\"You are a helpful assistant.\",\n",
    "                parsing_system_prompt=\"Extract structured information.\",\n",
    "                error=None,\n",
    "                run_name=\"demo_run\",\n",
    "            )\n",
    "            \n",
    "            results.append(VerificationResult(\n",
    "                metadata=metadata,\n",
    "                template=template,\n",
    "                rubric=rubric,\n",
    "                deep_judgment=None,\n",
    "                deep_judgment_rubric=None,\n",
    "            ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create mock result set\n",
    "mock_results = create_mock_results()\n",
    "result_set = VerificationResultSet(results=mock_results)\n",
    "\n",
    "# Extract result wrappers\n",
    "template_results = result_set.get_template_results()\n",
    "rubric_results = result_set.get_rubrics_results()\n",
    "judgment_results = result_set.get_judgment_results()\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "def _cleanup():\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Loaded {len(mock_results)} mock verification results\")\n",
    "print(f\"✓ Results from {len(result_set.get_model_names())} models: {result_set.get_model_names()}\")\n",
    "print(f\"✓ {len(result_set.get_question_ids())} questions available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Verification Results with DataFrames\n",
    "\n",
    "This guide covers how to analyze verification results using the DataFrame-first approach, providing flexible and powerful data analysis with pandas.\n",
    "\n",
    "## Overview\n",
    "\n",
    "### What This Guide Covers\n",
    "\n",
    "This guide focuses on **analyzing verification results** after you've run verification. The DataFrame-first approach provides a modern, flexible way to wrangle and analyze verification output using pandas DataFrames.\n",
    "\n",
    "**Typical Workflow**:\n",
    "\n",
    "1. Run verification (see verification.md for details)\n",
    "2. Extract results using `result_set.get_templates()`, `get_rubrics()`, or `get_judgments()`\n",
    "3. Convert to DataFrame for analysis\n",
    "4. Analyze with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "### Basic Workflow\n",
    "\n",
    "After running verification, you can analyze results with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template results: 10\n",
      "Rubric results: 10\n",
      "Judgment results: 10\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Extract result type wrappers from verification output\n",
    "template_results = result_set.get_template_results()\n",
    "rubric_results = result_set.get_rubrics_results()\n",
    "judgment_results = result_set.get_judgment_results()\n",
    "\n",
    "print(f\"Template results: {len(template_results)}\")\n",
    "print(f\"Rubric results: {len(rubric_results)}\")\n",
    "print(f\"Judgment results: {len(judgment_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template DataFrame shape: (18, 34)\n",
      "Rubric DataFrame shape: (30, 17)\n",
      "Judgment DataFrame shape: (10, 40)\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Convert verification results to DataFrames\n",
    "template_df = template_results.to_dataframe()\n",
    "rubric_df = rubric_results.to_dataframe()\n",
    "judgment_df = judgment_results.to_dataframe()\n",
    "\n",
    "print(f\"Template DataFrame shape: {template_df.shape}\")\n",
    "print(f\"Rubric DataFrame shape: {rubric_df.shape}\")\n",
    "print(f\"Judgment DataFrame shape: {judgment_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rate by question:\n",
      "question_id\n",
      "q_chromosomes          1.0\n",
      "q_drug_target          1.0\n",
      "q_hemoglobin           1.0\n",
      "q_inflammatory_lung    1.0\n",
      "q_partial_fail         0.5\n",
      "Name: field_match, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Analyze with pandas\n",
    "pass_rate = template_df.groupby('question_id')['field_match'].mean()\n",
    "print(\"Pass rate by question:\")\n",
    "print(pass_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Example: Template Verification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns:\n",
      "['completed_without_errors', 'error', 'recursion_limit_reached', 'question_id', 'template_id', 'question_text', 'keywords', 'replicate', 'answering_mcp_servers', 'answering_model', 'parsing_model', 'answering_system_prompt', 'parsing_system_prompt', 'raw_llm_response', 'field_name', 'gt_value', 'llm_value', 'field_match', 'field_type', 'verify_result', 'embedding_check_performed', 'embedding_similarity_score', 'embedding_model_used', 'embedding_override_applied', 'abstention_check_performed', 'abstention_detected', 'abstention_reasoning', 'abstention_override_applied', 'regex_validations_performed', 'regex_overall_success', 'execution_time', 'timestamp', 'run_name', 'result_index']\n"
     ]
    }
   ],
   "source": [
    "# Get template results DataFrame\n",
    "df = template_results.to_dataframe()\n",
    "\n",
    "# View available columns\n",
    "print(\"Available columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful verifications: 18 out of 18\n"
     ]
    }
   ],
   "source": [
    "# Filter to successful verifications only\n",
    "successful = df[df['completed_without_errors'] == True]\n",
    "print(f\"Successful verifications: {len(successful)} out of {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass rates by question:\n",
      "question_id\n",
      "q_chromosomes          1.0\n",
      "q_drug_target          1.0\n",
      "q_hemoglobin           1.0\n",
      "q_inflammatory_lung    1.0\n",
      "q_partial_fail         0.5\n",
      "Name: field_match, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate pass rate by question\n",
    "pass_rates = successful.groupby('question_id')['field_match'].mean()\n",
    "print(\"\\nPass rates by question:\")\n",
    "print(pass_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions with <100% pass rate: 1\n",
      "question_id\n",
      "q_partial_fail    0.5\n",
      "Name: field_match, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find questions with low pass rates\n",
    "low_performers = pass_rates[pass_rates < 1.0]\n",
    "print(f\"\\nQuestions with <100% pass rate: {len(low_performers)}\")\n",
    "print(low_performers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Field performance:\n",
      "            field_match      \n",
      "                   mean count\n",
      "field_name                   \n",
      "class               1.0     2\n",
      "composition         1.0     2\n",
      "count               1.0     2\n",
      "diseases            1.0     2\n",
      "subunits            1.0     2\n",
      "target              1.0     2\n",
      "unit                1.0     4\n",
      "weight              0.0     2\n"
     ]
    }
   ],
   "source": [
    "# Analyze by field\n",
    "field_performance = successful.groupby('field_name').agg({\n",
    "    'field_match': ['mean', 'count']\n",
    "})\n",
    "print(\"\\nField performance:\")\n",
    "print(field_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Methods Reference\n",
    "\n",
    "### TemplateResults\n",
    "\n",
    "#### `to_dataframe()`\n",
    "\n",
    "Convert template verification results to pandas DataFrame with field-level explosion.\n",
    "\n",
    "**Key Columns**:\n",
    "- **Status**: `completed_without_errors`, `error`, `recursion_limit_reached`\n",
    "- **Field Comparison**: `field_name`, `gt_value`, `llm_value`, `field_match`, `field_type`\n",
    "- **Verification Checks**: `embedding_check_performed`, `embedding_similarity_score`, `abstention_detected`, `regex_validations_performed`\n",
    "- **Identification**: `question_id`, `template_id`, `answering_model`, `parsing_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows (exploded by field): 18\n",
      "Unique questions: 5\n"
     ]
    }
   ],
   "source": [
    "df = template_results.to_dataframe()\n",
    "\n",
    "# Each field gets its own row\n",
    "# For example, a question with 3 fields → 3 rows\n",
    "print(f\"Total rows (exploded by field): {len(df)}\")\n",
    "print(f\"Unique questions: {df['question_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight field match rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Analyze specific fields\n",
    "weight_fields = df[df['field_name'] == 'weight']\n",
    "if len(weight_fields) > 0:\n",
    "    match_rate = weight_fields['field_match'].mean()\n",
    "    print(f\"Weight field match rate: {match_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `to_regex_dataframe()`\n",
    "\n",
    "Convert regex validation results to DataFrame with pattern explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex DataFrame rows: 0\n",
      "No regex validations performed in this example.\n"
     ]
    }
   ],
   "source": [
    "# For regex validation results\n",
    "regex_df = template_results.to_regex_dataframe()\n",
    "print(f\"Regex DataFrame rows: {len(regex_df)}\")\n",
    "\n",
    "if len(regex_df) > 0:\n",
    "    # Analyze pattern success rates\n",
    "    pattern_stats = regex_df.groupby('pattern_name').agg({\n",
    "        'matched': 'mean'\n",
    "    })\n",
    "    print(\"\\nPattern success rates:\")\n",
    "    print(pattern_stats)\n",
    "else:\n",
    "    print(\"No regex validations performed in this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `to_usage_dataframe(totals_only=False)`\n",
    "\n",
    "Convert token usage data to DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token usage by stage:\n",
      "usage_stage\n",
      "answer_generation    500\n",
      "parsing              300\n",
      "Name: total_tokens, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Detailed usage by stage\n",
    "usage_df = template_results.to_usage_dataframe(totals_only=False)\n",
    "stage_costs = usage_df.groupby('usage_stage')['total_tokens'].sum()\n",
    "print(\"Token usage by stage:\")\n",
    "print(stage_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tokens used: 800\n"
     ]
    }
   ],
   "source": [
    "# Total usage only\n",
    "totals_df = template_results.to_usage_dataframe(totals_only=True)\n",
    "total_cost = totals_df['total_tokens'].sum()\n",
    "print(f\"\\nTotal tokens used: {total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RubricResults\n",
    "\n",
    "#### `to_dataframe(trait_type=\"all\")`\n",
    "\n",
    "Convert rubric evaluation results to DataFrame with trait explosion.\n",
    "\n",
    "**Parameters**:\n",
    "- `trait_type`: Filter trait type - \"llm_score\", \"llm_binary\", \"llm\", \"regex\", \"callable\", \"metric\", or \"all\"\n",
    "\n",
    "**Key Columns**:\n",
    "- `trait_name`: Name of the rubric trait\n",
    "- `trait_type`: Type (llm_score, llm_binary, regex, callable, metric)\n",
    "- `trait_score`: Score value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM trait DataFrame shape: (30, 17)\n",
      "\n",
      "Sample LLM trait scores:\n",
      "           question_id    trait_name  trait_score\n",
      "0        q_chromosomes      Accuracy            5\n",
      "1        q_chromosomes       Clarity            4\n",
      "2        q_chromosomes  Completeness            4\n",
      "3        q_drug_target      Accuracy            5\n",
      "4        q_drug_target       Clarity            5\n",
      "5        q_drug_target  Completeness            5\n",
      "6         q_hemoglobin      Accuracy            5\n",
      "7         q_hemoglobin       Clarity            4\n",
      "8         q_hemoglobin  Completeness            5\n",
      "9  q_inflammatory_lung      Accuracy            4\n"
     ]
    }
   ],
   "source": [
    "# Get all LLM-scored traits\n",
    "llm_df = rubric_results.to_dataframe(trait_type=\"llm_score\")\n",
    "print(f\"LLM trait DataFrame shape: {llm_df.shape}\")\n",
    "print(\"\\nSample LLM trait scores:\")\n",
    "print(llm_df[['question_id', 'trait_name', 'trait_score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trait score summary:\n",
      "              mean       std  count\n",
      "trait_name                         \n",
      "Accuracy       4.4  0.843274     10\n",
      "Clarity        4.6  0.516398     10\n",
      "Completeness   4.4  0.516398     10\n"
     ]
    }
   ],
   "source": [
    "# Analyze trait performance\n",
    "trait_scores = llm_df.groupby('trait_name')['trait_score'].agg(['mean', 'std', 'count'])\n",
    "print(\"\\nTrait score summary:\")\n",
    "print(trait_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traits with average score < 4.0:\n",
      "Empty DataFrame\n",
      "Columns: [mean, std, count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find low-scoring traits\n",
    "low_scores = trait_scores[trait_scores['mean'] < 4.0]\n",
    "print(\"\\nTraits with average score < 4.0:\")\n",
    "print(low_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model comparison by question:\n",
      "answering_model      claude-3-5-sonnet  gpt-4o-mini\n",
      "question_id                                        \n",
      "q_chromosomes                 4.333333     4.333333\n",
      "q_drug_target                 5.000000     5.000000\n",
      "q_hemoglobin                  4.666667     4.666667\n",
      "q_inflammatory_lung           4.333333     4.333333\n",
      "q_partial_fail                4.000000     4.000000\n"
     ]
    }
   ],
   "source": [
    "# Compare models\n",
    "model_comparison = llm_df.pivot_table(\n",
    "    values='trait_score',\n",
    "    index='question_id',\n",
    "    columns='answering_model',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nModel comparison by question:\")\n",
    "print(model_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Patterns\n",
    "\n",
    "### Pattern 1: Calculate Pass Rates\n",
    "\n",
    "**Template verification pass rate by question:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rates by question:\n",
      "question_id\n",
      "q_chromosomes          1.0\n",
      "q_drug_target          1.0\n",
      "q_hemoglobin           1.0\n",
      "q_inflammatory_lung    1.0\n",
      "q_partial_fail         0.5\n",
      "Name: field_match, dtype: float64\n",
      "\n",
      "Questions with <100% pass rate: ['q_partial_fail']\n"
     ]
    }
   ],
   "source": [
    "df = template_results.to_dataframe()\n",
    "\n",
    "# Filter to successful verifications\n",
    "successful = df[df['completed_without_errors'] == True]\n",
    "\n",
    "# Calculate pass rate by question\n",
    "pass_rates = successful.groupby('question_id')['field_match'].mean()\n",
    "print(\"Pass rates by question:\")\n",
    "print(pass_rates)\n",
    "\n",
    "# Get questions below threshold\n",
    "failing = pass_rates[pass_rates < 1.0]\n",
    "print(f\"\\nQuestions with <100% pass rate: {list(failing.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rubric trait scores by model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average trait scores by model:\n",
      "answering_model\n",
      "claude-3-5-sonnet    4.466667\n",
      "gpt-4o-mini          4.466667\n",
      "Name: trait_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rubric_df = rubric_results.to_dataframe(trait_type=\"llm_score\")\n",
    "\n",
    "# Average score by model\n",
    "model_scores = rubric_df.groupby('answering_model')['trait_score'].mean()\n",
    "print(\"Average trait scores by model:\")\n",
    "print(model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model scores by trait:\n",
      "trait_name         Accuracy  Clarity  Completeness\n",
      "answering_model                                   \n",
      "claude-3-5-sonnet       4.4      4.6           4.4\n",
      "gpt-4o-mini             4.4      4.6           4.4\n"
     ]
    }
   ],
   "source": [
    "# Detailed breakdown by trait and model\n",
    "model_trait_scores = rubric_df.pivot_table(\n",
    "    values='trait_score',\n",
    "    index='answering_model',\n",
    "    columns='trait_name',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nModel scores by trait:\")\n",
    "print(model_trait_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Multi-Dimensional Analysis\n",
    "\n",
    "**Template + Rubric combined analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined template and rubric metrics:\n",
      "                     template_pass_rate  rubric_avg_score\n",
      "question_id                                              \n",
      "q_chromosomes                       1.0          4.333333\n",
      "q_drug_target                       1.0          5.000000\n",
      "q_hemoglobin                        1.0          4.666667\n",
      "q_inflammatory_lung                 1.0          4.333333\n",
      "q_partial_fail                      0.5          4.000000\n"
     ]
    }
   ],
   "source": [
    "# Get DataFrames\n",
    "template_df = template_results.to_dataframe()\n",
    "rubric_df = rubric_results.to_dataframe(trait_type=\"llm_score\")\n",
    "\n",
    "# Aggregate to question level\n",
    "template_agg = template_df.groupby('question_id')['field_match'].mean()\n",
    "rubric_agg = rubric_df.groupby('question_id')['trait_score'].mean()\n",
    "\n",
    "# Merge\n",
    "combined = pd.DataFrame({\n",
    "    'template_pass_rate': template_agg,\n",
    "    'rubric_avg_score': rubric_agg\n",
    "})\n",
    "\n",
    "print(\"Combined template and rubric metrics:\")\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Field-Level Analysis\n",
    "\n",
    "**Identify problematic fields:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field performance (worst to best):\n",
      "            field_match          \n",
      "                   mean count sum\n",
      "field_name                       \n",
      "weight              0.0     2   0\n",
      "class               1.0     2   2\n",
      "composition         1.0     2   2\n",
      "count               1.0     2   2\n",
      "diseases            1.0     2   2\n",
      "subunits            1.0     2   2\n",
      "target              1.0     2   2\n",
      "unit                1.0     4   4\n"
     ]
    }
   ],
   "source": [
    "df = template_results.to_dataframe()\n",
    "successful = df[df['completed_without_errors'] == True]\n",
    "\n",
    "# Calculate match rate by field\n",
    "field_performance = successful.groupby('field_name').agg({\n",
    "    'field_match': ['mean', 'count', 'sum']\n",
    "})\n",
    "\n",
    "# Sort by match rate\n",
    "field_performance = field_performance.sort_values(('field_match', 'mean'))\n",
    "\n",
    "print(\"Field performance (worst to best):\")\n",
    "print(field_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 4: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model comparison by question:\n",
      "answering_model      claude-3-5-sonnet  gpt-4o-mini\n",
      "question_id                                        \n",
      "q_chromosomes                      1.0          1.0\n",
      "q_drug_target                      1.0          1.0\n",
      "q_hemoglobin                       1.0          1.0\n",
      "q_inflammatory_lung                1.0          1.0\n",
      "q_partial_fail                     0.5          0.5\n"
     ]
    }
   ],
   "source": [
    "df = template_results.to_dataframe()\n",
    "\n",
    "# Pivot: questions × models\n",
    "model_comparison = df.pivot_table(\n",
    "    values='field_match',\n",
    "    index='question_id',\n",
    "    columns='answering_model',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(\"Model comparison by question:\")\n",
    "print(model_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With best model and spread:\n",
      "answering_model      claude-3-5-sonnet  gpt-4o-mini  performance_spread  \\\n",
      "question_id                                                               \n",
      "q_chromosomes                      1.0          1.0                 0.0   \n",
      "q_drug_target                      1.0          1.0                 0.0   \n",
      "q_hemoglobin                       1.0          1.0                 0.0   \n",
      "q_inflammatory_lung                1.0          1.0                 0.0   \n",
      "q_partial_fail                     0.5          0.5                 0.0   \n",
      "\n",
      "answering_model             best_model  \n",
      "question_id                             \n",
      "q_chromosomes        claude-3-5-sonnet  \n",
      "q_drug_target        claude-3-5-sonnet  \n",
      "q_hemoglobin         claude-3-5-sonnet  \n",
      "q_inflammatory_lung  claude-3-5-sonnet  \n",
      "q_partial_fail       claude-3-5-sonnet  \n"
     ]
    }
   ],
   "source": [
    "# Calculate relative performance\n",
    "# First compute the spread before adding string columns\n",
    "model_comparison = model_comparison.copy()\n",
    "model_comparison['performance_spread'] = model_comparison.max(axis=1) - model_comparison.min(axis=1)\n",
    "model_comparison['best_model'] = model_comparison.idxmax(axis=1)\n",
    "\n",
    "print(\"\\nWith best model and spread:\")\n",
    "print(model_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions with model variance: 0\n",
      "Empty DataFrame\n",
      "Columns: [best_model, performance_spread]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find questions with high model variance\n",
    "high_variance = model_comparison[model_comparison['performance_spread'] > 0]\n",
    "print(f\"\\nQuestions with model variance: {len(high_variance)}\")\n",
    "print(high_variance[['best_model', 'performance_spread']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 5: Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed field comparisons:\n",
      "       question_id field_name gt_value llm_value\n",
      "8   q_partial_fail     weight     5808      5800\n",
      "17  q_partial_fail     weight     5808      5800\n"
     ]
    }
   ],
   "source": [
    "df = template_results.to_dataframe()\n",
    "\n",
    "# Get failed verifications (where field_match is False)\n",
    "failed_fields = df[df['field_match'] == False]\n",
    "\n",
    "if len(failed_fields) > 0:\n",
    "    print(\"Failed field comparisons:\")\n",
    "    print(failed_fields[['question_id', 'field_name', 'gt_value', 'llm_value']].to_string())\n",
    "else:\n",
    "    print(\"No failed field comparisons in this dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 7: Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost by model:\n",
      "  claude-3-5-sonnet: $0.0001\n",
      "  gpt-4o-mini: $0.0001\n",
      "\n",
      "Total cost: $0.0001\n"
     ]
    }
   ],
   "source": [
    "usage_df = template_results.to_usage_dataframe(totals_only=True)\n",
    "\n",
    "# Assuming cost per 1K tokens\n",
    "INPUT_COST_PER_1K = 0.0001\n",
    "OUTPUT_COST_PER_1K = 0.0003\n",
    "\n",
    "usage_df['input_cost'] = usage_df['input_tokens'] / 1000 * INPUT_COST_PER_1K\n",
    "usage_df['output_cost'] = usage_df['output_tokens'] / 1000 * OUTPUT_COST_PER_1K\n",
    "usage_df['total_cost'] = usage_df['input_cost'] + usage_df['output_cost']\n",
    "\n",
    "# Cost by model\n",
    "model_costs = usage_df.groupby('answering_model')['total_cost'].sum()\n",
    "print(\"Cost by model:\")\n",
    "for model, cost in model_costs.items():\n",
    "    print(f\"  {model}: ${cost:.4f}\")\n",
    "\n",
    "# Total cost\n",
    "print(f\"\\nTotal cost: ${usage_df['total_cost'].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods\n",
    "\n",
    "Helper methods provide convenient aggregations for common operations. They are implemented using the DataFrame API internally.\n",
    "\n",
    "### TemplateResults Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `aggregate_pass_rate(by=\"question_id\", strategy=\"mean\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rates by question:\n",
      "  q_chromosomes: 100.00%\n",
      "  q_drug_target: 100.00%\n",
      "  q_hemoglobin: 100.00%\n",
      "  q_inflammatory_lung: 100.00%\n",
      "  q_partial_fail: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Pass rate by question\n",
    "question_rates = template_results.aggregate_pass_rate(by=\"question_id\")\n",
    "print(\"Pass rates by question:\")\n",
    "for qid, rate in question_rates.items():\n",
    "    print(f\"  {qid}: {rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass rates by model:\n",
      "  claude-3-5-sonnet: 100.00%\n",
      "  gpt-4o-mini: 80.00%\n"
     ]
    }
   ],
   "source": [
    "# Pass rate by model\n",
    "model_rates = template_results.aggregate_pass_rate(by=\"answering_model\")\n",
    "print(\"\\nPass rates by model:\")\n",
    "for model, rate in model_rates.items():\n",
    "    print(f\"  {model}: {rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `aggregate_embedding_scores(by=\"question_id\", strategy=\"mean\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding similarity scores by question:\n",
      "  q_chromosomes: 0.925\n",
      "  q_drug_target: 0.895\n",
      "  q_hemoglobin: 0.855\n",
      "  q_inflammatory_lung: 0.825\n",
      "  q_partial_fail: 0.875\n"
     ]
    }
   ],
   "source": [
    "# Average embedding score by question\n",
    "embedding_scores = template_results.aggregate_embedding_scores(by=\"question_id\")\n",
    "print(\"Embedding similarity scores by question:\")\n",
    "for qid, score in embedding_scores.items():\n",
    "    print(f\"  {qid}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RubricResults Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `aggregate_llm_traits(by=\"question_id\", strategy=\"mean\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM trait scores by question:\n",
      "\n",
      "q_chromosomes:\n",
      "  Accuracy: 5.00\n",
      "  Clarity: 4.00\n",
      "  Completeness: 4.00\n",
      "\n",
      "q_drug_target:\n",
      "  Accuracy: 5.00\n",
      "  Clarity: 5.00\n",
      "  Completeness: 5.00\n",
      "\n",
      "q_hemoglobin:\n",
      "  Accuracy: 5.00\n",
      "  Clarity: 4.00\n",
      "  Completeness: 5.00\n",
      "\n",
      "q_inflammatory_lung:\n",
      "  Accuracy: 4.00\n",
      "  Clarity: 5.00\n",
      "  Completeness: 4.00\n",
      "\n",
      "q_partial_fail:\n",
      "  Accuracy: 3.00\n",
      "  Clarity: 5.00\n",
      "  Completeness: 4.00\n"
     ]
    }
   ],
   "source": [
    "# Get average scores by question\n",
    "question_traits = rubric_results.aggregate_llm_traits(by=\"question_id\")\n",
    "\n",
    "print(\"LLM trait scores by question:\")\n",
    "for qid, traits in question_traits.items():\n",
    "    print(f\"\\n{qid}:\")\n",
    "    for trait, score in traits.items():\n",
    "        print(f\"  {trait}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM trait scores by model:\n",
      "\n",
      "claude-3-5-sonnet:\n",
      "  Accuracy: 4.40\n",
      "  Clarity: 4.60\n",
      "  Completeness: 4.40\n",
      "\n",
      "gpt-4o-mini:\n",
      "  Accuracy: 4.40\n",
      "  Clarity: 4.60\n",
      "  Completeness: 4.40\n"
     ]
    }
   ],
   "source": [
    "# Get average scores by model\n",
    "model_traits = rubric_results.aggregate_llm_traits(by=\"answering_model\")\n",
    "\n",
    "print(\"\\nLLM trait scores by model:\")\n",
    "for model, traits in model_traits.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    for trait, score in traits.items():\n",
    "        print(f\"  {trait}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips\n",
    "\n",
    "### 1. Filter Early\n",
    "\n",
    "Filter DataFrames before aggregation to reduce computational load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rates (filtered first):\n",
      "question_id\n",
      "q_chromosomes          1.0\n",
      "q_drug_target          1.0\n",
      "q_hemoglobin           1.0\n",
      "q_inflammatory_lung    1.0\n",
      "q_partial_fail         0.5\n",
      "Name: field_match, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Good: Filter first\n",
    "df = template_results.to_dataframe()\n",
    "successful = df[df['completed_without_errors'] == True]\n",
    "pass_rates = successful.groupby('question_id')['field_match'].mean()\n",
    "print(\"Pass rates (filtered first):\")\n",
    "print(pass_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use Helper Methods for Simple Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass rates using helper method:\n",
      "{'q_chromosomes': 1.0, 'q_drug_target': 1.0, 'q_hemoglobin': 1.0, 'q_inflammatory_lung': 1.0, 'q_partial_fail': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Prefer helper for simple aggregation\n",
    "pass_rates = template_results.aggregate_pass_rate(by=\"question_id\")\n",
    "print(\"Pass rates using helper method:\")\n",
    "print(pass_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Avoid Repeated Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field statistics:\n",
      "             mean  count\n",
      "field_name              \n",
      "class         1.0      2\n",
      "composition   1.0      2\n",
      "count         1.0      2\n",
      "diseases      1.0      2\n",
      "subunits      1.0      2\n",
      "target        1.0      2\n",
      "unit          1.0      4\n",
      "weight        0.0      2\n"
     ]
    }
   ],
   "source": [
    "# Good: Convert once\n",
    "template_df = template_results.to_dataframe()\n",
    "pass_rates = template_df.groupby('question_id')['field_match'].mean()\n",
    "field_stats = template_df.groupby('field_name')['field_match'].agg(['mean', 'count'])\n",
    "\n",
    "print(\"Field statistics:\")\n",
    "print(field_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The DataFrame-first approach provides:\n",
    "\n",
    "- **Familiar API**: Use standard pandas operations\n",
    "- **Flexibility**: Combine multiple verification aspects in custom ways\n",
    "- **Performance**: Leverage pandas' optimized operations\n",
    "- **Helper Methods**: Convenience methods for common operations\n",
    "\n",
    "For more information:\n",
    "- See the [DataFrame Quick Reference](dataframe-quick-reference.md)\n",
    "- Check integration tests for real-world usage examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
