{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# MCP-Enabled Verification\n",
    "\n",
    "This notebook demonstrates how to configure and run verification with MCP\n",
    "(Model Context Protocol) tools. MCP turns the answering model into an **agent**\n",
    "that can use external tools — web search, database queries, file operations —\n",
    "before producing its final response.\n",
    "\n",
    "| Step | What You'll Do |\n",
    "|------|---------------|\n",
    "| 1 | Configure MCP tools on a model |\n",
    "| 2 | Set up agent middleware (limits, retry, summarization) |\n",
    "| 3 | Run MCP-enabled verification |\n",
    "| 4 | Inspect agent traces and tool usage |\n",
    "| 5 | Handle recursion limit hits |\n",
    "| 6 | Control trace input for evaluation |\n",
    "\n",
    "See [MCP-Enabled Verification](../06-running-verification/mcp-verification.md)\n",
    "for full documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-mock",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:10:21.504644Z",
     "iopub.status.busy": "2026-02-06T07:10:21.504170Z",
     "iopub.status.idle": "2026-02-06T07:10:21.834773Z",
     "shell.execute_reply": "2026-02-06T07:10:21.834415Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: patches run_verification so examples execute without live API keys\n",
    "# or running MCP servers. This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationConfig, VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "\n",
    "def _mock_run_verification(self, config, question_ids=None, **_kwargs):\n",
    "    \"\"\"Return realistic mock results simulating MCP agent verification.\"\"\"\n",
    "    qids = question_ids or self.get_question_ids()\n",
    "    mock_results = []\n",
    "\n",
    "    # Simulate agent performance: most pass, one hits recursion limit\n",
    "    for qid in qids:\n",
    "        q = self.get_question(qid)\n",
    "        q_text = q.get(\"question\", \"\")\n",
    "        has_template = self.has_template(qid)\n",
    "\n",
    "        for ans_model in config.answering_models:\n",
    "            for parse_model in config.parsing_models:\n",
    "                # Simulate: prime number question hits recursion limit\n",
    "                hit_recursion = \"prime number\" in q_text\n",
    "                passed = not hit_recursion if has_template else None\n",
    "\n",
    "                answering = ModelIdentity(\n",
    "                    interface=ans_model.interface,\n",
    "                    model_name=ans_model.model_name,\n",
    "                )\n",
    "                parsing = ModelIdentity(\n",
    "                    interface=parse_model.interface,\n",
    "                    model_name=parse_model.model_name,\n",
    "                )\n",
    "\n",
    "                # Build a mock agent trace with tool calls\n",
    "                if ans_model.mcp_urls_dict:\n",
    "                    tool_names = list(ans_model.mcp_urls_dict.keys())\n",
    "                    trace_lines = [\n",
    "                        f\"[Agent] Received question: {q_text}\",\n",
    "                        f\"[Agent] Using tools from: {', '.join(tool_names)}\",\n",
    "                        \"[Tool Call] web_search('relevant query')\",\n",
    "                        \"[Tool Result] Found 3 results...\",\n",
    "                        \"[Agent] Based on the search results, the answer is...\",\n",
    "                    ]\n",
    "                    if hit_recursion:\n",
    "                        trace_lines.append(\"[Agent] RECURSION LIMIT REACHED — returning partial response\")\n",
    "                    raw_response = \"\\n\".join(trace_lines)\n",
    "                else:\n",
    "                    raw_response = f\"Mock answer for: {q_text}\"\n",
    "\n",
    "                template_result = None\n",
    "                if has_template:\n",
    "                    template_result = VerificationResultTemplate(\n",
    "                        raw_llm_response=raw_response,\n",
    "                        verify_result=passed,\n",
    "                        template_verification_performed=True,\n",
    "                        recursion_limit_reached=hit_recursion,\n",
    "                    )\n",
    "\n",
    "                metadata = VerificationResultMetadata(\n",
    "                    question_id=qid,\n",
    "                    template_id=(self.get_template(qid)[:10] + \"...\" if has_template else \"no_template\"),\n",
    "                    completed_without_errors=True,\n",
    "                    question_text=q_text,\n",
    "                    answering=answering,\n",
    "                    parsing=parsing,\n",
    "                    execution_time=3.5 if ans_model.mcp_urls_dict else 1.2,\n",
    "                    timestamp=datetime.datetime.now(datetime.UTC).isoformat(),\n",
    "                    result_id=f\"mock-mcp-{qid[:8]}\",\n",
    "                )\n",
    "\n",
    "                mock_results.append(\n",
    "                    VerificationResult(\n",
    "                        metadata=metadata,\n",
    "                        template=template_result,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    return VerificationResultSet(results=mock_results)\n",
    "\n",
    "\n",
    "_patcher1 = patch(\n",
    "    \"karenina.benchmark.benchmark.Benchmark.run_verification\",\n",
    "    _mock_run_verification,\n",
    ")\n",
    "_patcher2 = patch(\n",
    "    \"karenina.schemas.verification.config.VerificationConfig._validate_config\",\n",
    "    lambda _self: None,\n",
    ")\n",
    "_ = _patcher1.start()\n",
    "_ = _patcher2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-md",
   "metadata": {},
   "source": [
    "## Step 1: Configure MCP Tools\n",
    "\n",
    "MCP tools are configured on `ModelConfig` via `mcp_urls_dict`. Each key is a\n",
    "server name, each value is the MCP endpoint URL. Setting this field activates\n",
    "agent mode for that model.\n",
    "\n",
    "Use `mcp_tool_filter` to restrict which tools are available, and\n",
    "`mcp_tool_description_overrides` to improve tool descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "step1-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:10:21.836046Z",
     "iopub.status.busy": "2026-02-06T07:10:21.835967Z",
     "iopub.status.idle": "2026-02-06T07:10:21.837958Z",
     "shell.execute_reply": "2026-02-06T07:10:21.837737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: claude-sonnet-4-5-20250929\n",
      "MCP servers: ['search', 'database']\n",
      "Tool filter: ['web_search', 'query_db']\n",
      "Agent mode: True\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import ModelConfig\n",
    "\n",
    "# Configure an answering model with MCP tools\n",
    "answering_model = ModelConfig(\n",
    "    id=\"agent-claude\",\n",
    "    model_name=\"claude-sonnet-4-5-20250929\",\n",
    "    model_provider=\"anthropic\",\n",
    "    interface=\"langchain\",\n",
    "    mcp_urls_dict={\n",
    "        \"search\": \"http://localhost:3000/mcp\",\n",
    "        \"database\": \"http://localhost:3001/mcp\",\n",
    "    },\n",
    "    mcp_tool_filter=[\"web_search\", \"query_db\"],\n",
    "    mcp_tool_description_overrides={\n",
    "        \"web_search\": \"Search the web for current biomedical research.\",\n",
    "        \"query_db\": \"Query the genomics database for gene information.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Model: {answering_model.model_name}\")\n",
    "print(f\"MCP servers: {list(answering_model.mcp_urls_dict.keys())}\")\n",
    "print(f\"Tool filter: {answering_model.mcp_tool_filter}\")\n",
    "print(f\"Agent mode: {answering_model.mcp_urls_dict is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-md",
   "metadata": {},
   "source": [
    "## Step 2: Agent Middleware Configuration\n",
    "\n",
    "When MCP is enabled, the model runs as an agent — making multiple LLM calls\n",
    "and invoking tools between calls. `AgentMiddlewareConfig` controls safety\n",
    "limits, retry behavior, summarization, and prompt caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "step2-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:10:21.838930Z",
     "iopub.status.busy": "2026-02-06T07:10:21.838872Z",
     "iopub.status.idle": "2026-02-06T07:10:21.840980Z",
     "shell.execute_reply": "2026-02-06T07:10:21.840795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model call limit: 30\n",
      "Tool call limit: 60\n",
      "Model retries: 3\n",
      "Tool retries: 3\n",
      "Summarization: True\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas.config.models import (\n",
    "    AgentLimitConfig,\n",
    "    AgentMiddlewareConfig,\n",
    "    ModelRetryConfig,\n",
    "    SummarizationConfig,\n",
    "    ToolRetryConfig,\n",
    ")\n",
    "\n",
    "# Configure agent middleware with custom settings\n",
    "middleware = AgentMiddlewareConfig(\n",
    "    limits=AgentLimitConfig(\n",
    "        model_call_limit=30,  # Max LLM calls per question\n",
    "        tool_call_limit=60,  # Max tool calls per question\n",
    "        exit_behavior=\"end\",  # Return partial response on limit\n",
    "    ),\n",
    "    model_retry=ModelRetryConfig(\n",
    "        max_retries=3,  # Retry failed LLM calls\n",
    "        backoff_factor=2.0,\n",
    "        initial_delay=2.0,\n",
    "    ),\n",
    "    tool_retry=ToolRetryConfig(\n",
    "        max_retries=3,  # Retry failed tool calls\n",
    "        on_failure=\"return_message\",  # Return error as message to model\n",
    "    ),\n",
    "    summarization=SummarizationConfig(\n",
    "        enabled=True,  # Summarize when context gets long\n",
    "        trigger_fraction=0.8,  # Trigger at 80% of context window\n",
    "        keep_messages=20,  # Keep 20 recent messages after summarization\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Model call limit: {middleware.limits.model_call_limit}\")\n",
    "print(f\"Tool call limit: {middleware.limits.tool_call_limit}\")\n",
    "print(f\"Model retries: {middleware.model_retry.max_retries}\")\n",
    "print(f\"Tool retries: {middleware.tool_retry.max_retries}\")\n",
    "print(f\"Summarization: {middleware.summarization.enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-md",
   "metadata": {},
   "source": [
    "## Step 3: Run MCP-Enabled Verification\n",
    "\n",
    "Combine the MCP-configured model with middleware and run verification.\n",
    "The answering model will use MCP tools to gather information before\n",
    "responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:10:21.841887Z",
     "iopub.status.busy": "2026-02-06T07:10:21.841830Z",
     "iopub.status.idle": "2026-02-06T07:10:21.844332Z",
     "shell.execute_reply": "2026-02-06T07:10:21.844143Z"
    }
   },
   "outputs": [],
   "source": [
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "benchmark = Benchmark.load(\"test_checkpoint.jsonld\")\n",
    "\n",
    "# Attach middleware to the answering model\n",
    "agent_model = ModelConfig(\n",
    "    id=\"agent-claude\",\n",
    "    model_name=\"claude-sonnet-4-5-20250929\",\n",
    "    model_provider=\"anthropic\",\n",
    "    interface=\"langchain\",\n",
    "    mcp_urls_dict={\"search\": \"http://localhost:3000/mcp\"},\n",
    "    mcp_tool_filter=[\"web_search\"],\n",
    "    agent_middleware=middleware,\n",
    ")\n",
    "\n",
    "# Parsing model does not need MCP\n",
    "parsing_model = ModelConfig(\n",
    "    id=\"parser-gpt4o\",\n",
    "    model_name=\"gpt-4o\",\n",
    "    model_provider=\"openai\",\n",
    "    interface=\"langchain\",\n",
    ")\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[agent_model],\n",
    "    parsing_models=[parsing_model],\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "summary = results.get_summary()\n",
    "pass_info = summary.get(\"template_pass_overall\", {})\n",
    "print(f\"Total results: {len(results.results)}\")\n",
    "print(f\"Passed: {pass_info.get('passed', 0)}/{pass_info.get('total', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-md",
   "metadata": {},
   "source": [
    "## Step 4: Inspect Agent Traces\n",
    "\n",
    "MCP-enabled models produce **agent traces** — sequences of messages including\n",
    "LLM responses, tool calls, and tool results. The full trace is always captured\n",
    "in `raw_llm_response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "step4-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:10:21.845247Z",
     "iopub.status.busy": "2026-02-06T07:10:21.845195Z",
     "iopub.status.idle": "2026-02-06T07:10:21.846914Z",
     "shell.execute_reply": "2026-02-06T07:10:21.846727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- What is the capital of France?... [PASS] ---\n",
      "  [Agent] Received question: What is the capital of France?\n",
      "  [Agent] Using tools from: search\n",
      "  [Tool Call] web_search('relevant query')\n",
      "\n",
      "--- What is 6 multiplied by 7?... [PASS] ---\n",
      "  [Agent] Received question: What is 6 multiplied by 7?\n",
      "  [Agent] Using tools from: search\n",
      "  [Tool Call] web_search('relevant query')\n",
      "\n",
      "--- What element has the atomic number 8? Provide both... [PASS] ---\n",
      "  [Agent] Received question: What element has the atomic number 8? Provide both the name and chemical symbol.\n",
      "  [Agent] Using tools from: search\n",
      "  [Tool Call] web_search('relevant query')\n",
      "\n",
      "--- Is 17 a prime number?... [FAIL] ---\n",
      "  [Agent] Received question: Is 17 a prime number?\n",
      "  [Agent] Using tools from: search\n",
      "  [Tool Call] web_search('relevant query')\n",
      "\n",
      "--- Explain the concept of machine learning in simple ... [NO TEMPLATE] ---\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    q_text = result.metadata.question_text\n",
    "    if result.template:\n",
    "        trace = result.template.raw_llm_response\n",
    "        passed = result.template.verify_result\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        print(f\"\\n--- {q_text[:50]}... [{status}] ---\")\n",
    "        # Show first 3 lines of the agent trace\n",
    "        for line in trace.split(\"\\n\")[:3]:\n",
    "            print(f\"  {line}\")\n",
    "    else:\n",
    "        print(f\"\\n--- {q_text[:50]}... [NO TEMPLATE] ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-md",
   "metadata": {},
   "source": [
    "## Step 5: Handle Recursion Limit Hits\n",
    "\n",
    "When an agent exceeds `model_call_limit` or `tool_call_limit`, the pipeline\n",
    "auto-fails the question. Check `recursion_limit_reached` on the template\n",
    "result to identify these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "step5-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:10:21.847778Z",
     "iopub.status.busy": "2026-02-06T07:10:21.847725Z",
     "iopub.status.idle": "2026-02-06T07:10:21.849329Z",
     "shell.execute_reply": "2026-02-06T07:10:21.849156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion limit hit: Is 17 a prime number?\n",
      "  verify_result: False\n",
      "  completed_without_errors: True\n",
      "  trace lines: 6\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    if result.template and result.template.recursion_limit_reached:\n",
    "        print(f\"Recursion limit hit: {result.metadata.question_text[:60]}\")\n",
    "        print(f\"  verify_result: {result.template.verify_result}\")\n",
    "        print(f\"  completed_without_errors: {result.metadata.completed_without_errors}\")\n",
    "        # The trace is still available for analysis\n",
    "        print(f\"  trace lines: {len(result.template.raw_llm_response.split(chr(10)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-md",
   "metadata": {},
   "source": [
    "## Step 6: Trace Input for Evaluation\n",
    "\n",
    "Two flags on `VerificationConfig` control what evaluation models see:\n",
    "\n",
    "| Flag | Default | What the Evaluator Sees |\n",
    "|------|---------|------------------------|\n",
    "| `use_full_trace_for_template` | `False` | Only the final answer (less noise, lower cost) |\n",
    "| `use_full_trace_for_rubric` | `True` | Full agent trace (tool usage, reasoning process) |\n",
    "\n",
    "The full trace is **always** captured regardless of these flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "step6-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:10:21.850259Z",
     "iopub.status.busy": "2026-02-06T07:10:21.850194Z",
     "iopub.status.idle": "2026-02-06T07:10:21.851897Z",
     "shell.execute_reply": "2026-02-06T07:10:21.851727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template sees full trace: False\n",
      "Rubric sees full trace: True\n",
      "\n",
      "Results: 5\n"
     ]
    }
   ],
   "source": [
    "# Configure trace input separately for template and rubric evaluation\n",
    "config_trace = VerificationConfig(\n",
    "    answering_models=[agent_model],\n",
    "    parsing_models=[parsing_model],\n",
    "    # Template parsing: only final answer (default)\n",
    "    use_full_trace_for_template=False,\n",
    "    # Rubric evaluation: full agent trace (default)\n",
    "    use_full_trace_for_rubric=True,\n",
    ")\n",
    "\n",
    "print(f\"Template sees full trace: {config_trace.use_full_trace_for_template}\")\n",
    "print(f\"Rubric sees full trace: {config_trace.use_full_trace_for_rubric}\")\n",
    "\n",
    "# Run with trace settings\n",
    "trace_results = benchmark.run_verification(config_trace)\n",
    "print(f\"\\nResults: {len(trace_results.results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Feature | Configuration |\n",
    "|---------|---------------|\n",
    "| Enable MCP | Set `mcp_urls_dict` on `ModelConfig` |\n",
    "| Filter tools | Set `mcp_tool_filter` on `ModelConfig` |\n",
    "| Execution limits | `AgentLimitConfig` in `AgentMiddlewareConfig` |\n",
    "| Retry behavior | `ModelRetryConfig` and `ToolRetryConfig` |\n",
    "| Summarization | `SummarizationConfig` (LangChain adapter only) |\n",
    "| Trace input | `use_full_trace_for_template/rubric` on `VerificationConfig` |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [MCP-Enabled Verification](../06-running-verification/mcp-verification.md) — full documentation\n",
    "- [Adapters Overview](../04-core-concepts/adapters.md) — adapter-specific MCP behavior\n",
    "- [Python API Verification](../06-running-verification/python-api.md) — standard verification workflow\n",
    "- [Multi-Model Evaluation](../06-running-verification/multi-model.md) — comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cleanup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:10:21.852714Z",
     "iopub.status.busy": "2026-02-06T07:10:21.852668Z",
     "iopub.status.idle": "2026-02-06T07:10:21.854017Z",
     "shell.execute_reply": "2026-02-06T07:10:21.853815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup mocks\n",
    "_ = _patcher1.stop()\n",
    "_ = _patcher2.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
