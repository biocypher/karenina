{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09748873",
   "metadata": {},
   "source": [
    "# Answer Templates\n",
    "\n",
    "Answer templates are the primary mechanism for evaluating **correctness** in Karenina. They define what information to extract from an LLM's response and how to verify it against ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f56eb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.939466Z",
     "iopub.status.busy": "2026-02-06T07:22:34.939219Z",
     "iopub.status.idle": "2026-02-06T07:22:34.943209Z",
     "shell.execute_reply": "2026-02-06T07:22:34.942823Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: ensures examples execute without live API keys.\n",
    "# This cell is hidden in rendered documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e1cae",
   "metadata": {},
   "source": [
    "## What Are Templates?\n",
    "\n",
    "An answer template is a **Pydantic model** that serves two purposes:\n",
    "\n",
    "1. **Parsing instructions** — Field names and descriptions tell the Judge LLM what to extract from a free-text response\n",
    "2. **Verification logic** — The `verify()` method programmatically checks extracted values against expected answers\n",
    "\n",
    "This is the core of Karenina's **LLM-as-judge** approach: the answering model produces natural text, the Judge LLM fills in the template's schema, and the template's code decides if the answer is correct.\n",
    "\n",
    "```\n",
    "Response (free text)  →  Judge LLM  →  Filled template  →  verify()  →  True/False\n",
    "```\n",
    "\n",
    "## Template Structure\n",
    "\n",
    "Every template inherits from `BaseAnswer` and must be named `Answer`. A template has three required components:\n",
    "\n",
    "1. **Fields** with descriptions that guide the Judge LLM\n",
    "2. **`model_post_init`** to set ground truth values\n",
    "3. **`verify`** to compare extracted values against ground truth\n",
    "\n",
    "Here is a simple template that checks whether an LLM correctly identified a drug target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d87226c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.945191Z",
     "iopub.status.busy": "2026-02-06T07:22:34.945074Z",
     "iopub.status.idle": "2026-02-06T07:22:35.265514Z",
     "shell.execute_reply": "2026-02-06T07:22:35.265224Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "from karenina.schemas.entities import BaseAnswer\n",
    "\n",
    "\n",
    "class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target of the drug mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCL2\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.target.strip().upper() == self.correct[\"target\"].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad4bfd",
   "metadata": {},
   "source": [
    "Let's walk through what happens when this template is used:\n",
    "\n",
    "1. The Judge LLM reads the answering model's response and extracts the `target` field based on the description\n",
    "2. Pydantic creates an `Answer` instance, then `model_post_init` sets the expected value\n",
    "3. `verify()` compares the extracted target to \"BCL2\" (case-insensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8acc78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:35.266702Z",
     "iopub.status.busy": "2026-02-06T07:22:35.266628Z",
     "iopub.status.idle": "2026-02-06T07:22:35.268289Z",
     "shell.execute_reply": "2026-02-06T07:22:35.268037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted target: 'Bcl-2'\n",
      "Ground truth:     'BCL2'\n",
      "Verified:         False\n"
     ]
    }
   ],
   "source": [
    "# Simulate what the Judge LLM produces after parsing a response\n",
    "parsed = Answer(target=\"Bcl-2\")\n",
    "print(f\"Extracted target: {parsed.target!r}\")\n",
    "print(f\"Ground truth:     {parsed.correct['target']!r}\")\n",
    "print(f\"Verified:         {parsed.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f104b",
   "metadata": {},
   "source": [
    "## Naming Requirement\n",
    "\n",
    "All answer template classes **must be named `Answer`**. The pipeline looks for this exact class name when executing template code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f62bb28c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:35.269228Z",
     "iopub.status.busy": "2026-02-06T07:22:35.269176Z",
     "iopub.status.idle": "2026-02-06T07:22:35.270944Z",
     "shell.execute_reply": "2026-02-06T07:22:35.270769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify(): True\n"
     ]
    }
   ],
   "source": [
    "# Correct\n",
    "class Answer(BaseAnswer):\n",
    "    value: str = Field(description=\"The answer value\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"value\": \"42\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.value.strip() == self.correct[\"value\"]\n",
    "\n",
    "\n",
    "# This works:\n",
    "a = Answer(value=\"42\")\n",
    "print(f\"verify(): {a.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78de9d",
   "metadata": {},
   "source": [
    "## Field Types\n",
    "\n",
    "Template fields can use any type that Pydantic supports. The field type guides both the Judge LLM's parsing and the verification logic.\n",
    "\n",
    "| Type | Use Case | Example |\n",
    "|------|----------|---------|\n",
    "| `str` | Names, terms, identifiers | Drug target, gene symbol |\n",
    "| `int` | Counts, quantities | Number of chromosomes |\n",
    "| `float` | Measurements, scores | Temperature, percentage |\n",
    "| `bool` | Yes/no judgments | \"Does the response mention X?\" |\n",
    "| `list[str]` | Multiple items | List of proteins, symptoms |\n",
    "\n",
    "Here is a multi-field example that extracts and verifies two pieces of information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899c4e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:35.271865Z",
     "iopub.status.busy": "2026-02-06T07:22:35.271794Z",
     "iopub.status.idle": "2026-02-06T07:22:35.273892Z",
     "shell.execute_reply": "2026-02-06T07:22:35.273726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element correct:  True\n",
      "Number correct:   True\n",
      "Overall verify(): True\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    element: str = Field(description=\"The chemical element name mentioned in the response\")\n",
    "    atomic_number: int = Field(description=\"The atomic number stated in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"element\": \"oxygen\", \"atomic_number\": 8}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        name_ok = self.element.strip().lower() == self.correct[\"element\"]\n",
    "        number_ok = self.atomic_number == self.correct[\"atomic_number\"]\n",
    "        return name_ok and number_ok\n",
    "\n",
    "\n",
    "# Both fields must match for verification to pass\n",
    "parsed = Answer(element=\"Oxygen\", atomic_number=8)\n",
    "print(f\"Element correct:  {parsed.element.strip().lower() == parsed.correct['element']}\")\n",
    "print(f\"Number correct:   {parsed.atomic_number == parsed.correct['atomic_number']}\")\n",
    "print(f\"Overall verify(): {parsed.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2283d9b",
   "metadata": {},
   "source": [
    "## Partial Credit with verify_granular\n",
    "\n",
    "For multi-field templates, you can implement `verify_granular()` to return a score between 0.0 and 1.0 representing the fraction of correct fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a5bcbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:35.274782Z",
     "iopub.status.busy": "2026-02-06T07:22:35.274732Z",
     "iopub.status.idle": "2026-02-06T07:22:35.277216Z",
     "shell.execute_reply": "2026-02-06T07:22:35.277013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify():         False\n",
      "verify_granular(): 0.67\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    capital: str = Field(description=\"The capital city mentioned\")\n",
    "    population: int = Field(description=\"The population figure stated\")\n",
    "    continent: str = Field(description=\"The continent mentioned\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\n",
    "            \"capital\": \"paris\",\n",
    "            \"population\": 2161000,\n",
    "            \"continent\": \"europe\",\n",
    "        }\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return (\n",
    "            self.capital.strip().lower() == self.correct[\"capital\"]\n",
    "            and self.population == self.correct[\"population\"]\n",
    "            and self.continent.strip().lower() == self.correct[\"continent\"]\n",
    "        )\n",
    "\n",
    "    def verify_granular(self) -> float:\n",
    "        correct_count = 0\n",
    "        if self.capital.strip().lower() == self.correct[\"capital\"]:\n",
    "            correct_count += 1\n",
    "        if self.population == self.correct[\"population\"]:\n",
    "            correct_count += 1\n",
    "        if self.continent.strip().lower() == self.correct[\"continent\"]:\n",
    "            correct_count += 1\n",
    "        return correct_count / 3\n",
    "\n",
    "\n",
    "# 2 out of 3 fields correct\n",
    "parsed = Answer(capital=\"Paris\", population=999, continent=\"Europe\")\n",
    "print(f\"verify():         {parsed.verify()}\")\n",
    "print(f\"verify_granular(): {parsed.verify_granular():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4b9e7",
   "metadata": {},
   "source": "## Embedding Check\n\nWhen `embedding_check_enabled` is set in `VerificationConfig`, the pipeline runs a **semantic similarity check** (stage 9) after `verify()`. This uses a SentenceTransformer model to compare the raw LLM response against the expected answer, providing a secondary signal when string-based verification is too strict.\n\nThe embedding check is configured via three settings:\n\n| Setting | Default | Description |\n|---------|---------|-------------|\n| `embedding_check_enabled` | `False` | Enable the embedding similarity check |\n| `embedding_check_model` | `all-MiniLM-L6-v2` | SentenceTransformer model name |\n| `embedding_check_threshold` | `0.85` | Similarity threshold (0.0-1.0) |\n\nThe embedding check result is stored alongside the template result — it does not override `verify()` but provides additional context for analysis.\n\n## Writing Good Field Descriptions\n\nField descriptions are the **instructions the Judge LLM follows** when parsing a response. Clear descriptions lead to accurate extraction.\n\n**Good descriptions** are specific and unambiguous:\n\n    # Good: tells the judge exactly what to look for\n    target: str = Field(description=\"The protein target of the drug mentioned in the response\")\n\n    # Good: specifies format expectations\n    count: int = Field(description=\"The total count of items listed in the response, as an integer\")\n\n**Weak descriptions** are vague or redundant:\n\n    # Weak: too vague\n    target: str = Field(description=\"The target\")\n\n    # Weak: repeats the field name without adding guidance\n    count: int = Field(description=\"The count\")\n\n## Next Steps\n\n- [Rubrics](rubrics/index.md) — Assess response quality beyond correctness\n- [Evaluation Modes](evaluation-modes.md) — Choose between template-only, rubric-only, or both\n- [Creating Benchmarks](../05-creating-benchmarks/index.md) — Build benchmarks with templates and questions\n- [Philosophy](../home/philosophy.md) — Why the LLM-as-judge approach works"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
