{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa33df8",
   "metadata": {},
   "source": [
    "# Multi-Model Evaluation\n",
    "\n",
    "This notebook demonstrates how to compare LLM performance across multiple\n",
    "answering models using a single benchmark. You'll learn to configure models,\n",
    "run verification, and analyze per-model results.\n",
    "\n",
    "| Step | What You'll Do |\n",
    "|------|---------------|\n",
    "| 1 | Configure multiple answering models |\n",
    "| 2 | Run verification across all models |\n",
    "| 3 | Compare models by pass rate |\n",
    "| 4 | Drill into per-question performance |\n",
    "| 5 | Use `from_overrides` for separate model runs |\n",
    "| 6 | Measure variance with replicates |\n",
    "\n",
    "See [Multi-Model Evaluation](../06-running-verification/multi-model.md) for\n",
    "full documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e7898c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.495658Z",
     "iopub.status.busy": "2026-02-06T07:00:52.495438Z",
     "iopub.status.idle": "2026-02-06T07:00:52.822980Z",
     "shell.execute_reply": "2026-02-06T07:00:52.822674Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: patches run_verification so examples execute without live API keys.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationConfig, VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "\n",
    "def _mock_run_verification(self, config, question_ids=None, **kwargs):\n",
    "    \"\"\"Return realistic mock results for multi-model documentation.\"\"\"\n",
    "    qids = question_ids or self.get_question_ids()\n",
    "    mock_results = []\n",
    "\n",
    "    # Model performance profiles — different models have different strengths\n",
    "    # Keys are substrings that match questions in test_checkpoint.jsonld\n",
    "    model_profiles = {\n",
    "        \"gpt-4o\": {\n",
    "            \"capital of France\": True,\n",
    "            \"6 multiplied by 7\": True,\n",
    "            \"atomic number 8\": True,\n",
    "            \"prime number\": True,\n",
    "        },\n",
    "        \"claude-sonnet-4-5-20250514\": {\n",
    "            \"capital of France\": True,\n",
    "            \"6 multiplied by 7\": True,\n",
    "            \"atomic number 8\": True,\n",
    "            \"prime number\": True,\n",
    "        },\n",
    "        \"gemini-2.0-flash\": {\n",
    "            \"capital of France\": True,\n",
    "            \"6 multiplied by 7\": True,\n",
    "            \"atomic number 8\": False,\n",
    "            \"prime number\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for qid in qids:\n",
    "        q = self.get_question(qid)\n",
    "        q_text = q.get(\"question\", \"\")\n",
    "        has_template = self.has_template(qid)\n",
    "\n",
    "        for ans_model in config.answering_models:\n",
    "            model_name = ans_model.model_name\n",
    "            profile = model_profiles.get(model_name, {})\n",
    "\n",
    "            for rep in range(config.replicate_count):\n",
    "                for parse_model in config.parsing_models:\n",
    "                    passed = any(\n",
    "                        key in q_text and profile.get(key, False)\n",
    "                        for key in profile\n",
    "                    )\n",
    "                    if not has_template:\n",
    "                        passed = None\n",
    "\n",
    "                    answering = ModelIdentity(\n",
    "                        interface=ans_model.interface,\n",
    "                        model_name=ans_model.model_name,\n",
    "                    )\n",
    "                    parsing = ModelIdentity(\n",
    "                        interface=parse_model.interface,\n",
    "                        model_name=parse_model.model_name,\n",
    "                    )\n",
    "\n",
    "                    template_result = None\n",
    "                    if has_template:\n",
    "                        template_result = VerificationResultTemplate(\n",
    "                            raw_llm_response=f\"Mock answer from {model_name}\",\n",
    "                            verify_result=passed,\n",
    "                            template_verification_performed=True,\n",
    "                        )\n",
    "\n",
    "                    metadata = VerificationResultMetadata(\n",
    "                        question_id=qid,\n",
    "                        template_id=(\n",
    "                            self.get_template(qid)[:10] + \"...\"\n",
    "                            if has_template\n",
    "                            else \"no_template\"\n",
    "                        ),\n",
    "                        completed_without_errors=True,\n",
    "                        question_text=q_text,\n",
    "                        answering=answering,\n",
    "                        parsing=parsing,\n",
    "                        execution_time=1.2,\n",
    "                        timestamp=datetime.datetime.now(\n",
    "                            datetime.UTC\n",
    "                        ).isoformat(),\n",
    "                        result_id=f\"mock-{qid[:8]}-{model_name[:4]}-r{rep}\",\n",
    "                        run_name=kwargs.get(\"run_name\"),\n",
    "                        replicate=rep,\n",
    "                    )\n",
    "\n",
    "                    mock_results.append(\n",
    "                        VerificationResult(\n",
    "                            metadata=metadata, template=template_result\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    return VerificationResultSet(results=mock_results)\n",
    "\n",
    "\n",
    "_patcher1 = patch(\n",
    "    \"karenina.benchmark.benchmark.Benchmark.run_verification\",\n",
    "    _mock_run_verification,\n",
    ")\n",
    "_patcher2 = patch(\n",
    "    \"karenina.schemas.verification.config.VerificationConfig._validate_config\",\n",
    "    lambda self: None,\n",
    ")\n",
    "_ = _patcher1.start()\n",
    "_ = _patcher2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f45c0",
   "metadata": {},
   "source": [
    "## Step 1: Configure Multiple Models\n",
    "\n",
    "Provide a list of answering models to compare. Each model is a `ModelConfig`\n",
    "with its own provider and interface settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc5c57f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.824333Z",
     "iopub.status.busy": "2026-02-06T07:00:52.824228Z",
     "iopub.status.idle": "2026-02-06T07:00:52.827025Z",
     "shell.execute_reply": "2026-02-06T07:00:52.826785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions: 5\n",
      "Answering models: 3\n",
      "Parsing models: 1\n",
      "Total tasks: 15\n"
     ]
    }
   ],
   "source": [
    "from karenina.benchmark import Benchmark\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "benchmark = Benchmark.load(\"test_checkpoint.jsonld\")\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt4o\",\n",
    "            model_name=\"gpt-4o\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"claude-sonnet\",\n",
    "            model_name=\"claude-sonnet-4-5-20250514\",\n",
    "            model_provider=\"anthropic\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"gemini-flash\",\n",
    "            model_name=\"gemini-2.0-flash\",\n",
    "            model_provider=\"google_genai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "total_tasks = (\n",
    "    benchmark.question_count\n",
    "    * len(config.answering_models)\n",
    "    * len(config.parsing_models)\n",
    "    * config.replicate_count\n",
    ")\n",
    "print(f\"Questions: {benchmark.question_count}\")\n",
    "print(f\"Answering models: {len(config.answering_models)}\")\n",
    "print(f\"Parsing models: {len(config.parsing_models)}\")\n",
    "print(f\"Total tasks: {total_tasks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43b73f",
   "metadata": {},
   "source": [
    "## Step 2: Run Verification\n",
    "\n",
    "A single call runs all model combinations and returns one `VerificationResultSet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635e1760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.828040Z",
     "iopub.status.busy": "2026-02-06T07:00:52.827966Z",
     "iopub.status.idle": "2026-02-06T07:00:52.829569Z",
     "shell.execute_reply": "2026-02-06T07:00:52.829357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 15\n"
     ]
    }
   ],
   "source": [
    "results = benchmark.run_verification(config)\n",
    "print(f\"Total results: {len(results.results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c46d3",
   "metadata": {},
   "source": [
    "## Step 3: Compare Models by Pass Rate\n",
    "\n",
    "Group results by answering model to see overall performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037b87d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.830539Z",
     "iopub.status.busy": "2026-02-06T07:00:52.830480Z",
     "iopub.status.idle": "2026-02-06T07:00:52.832142Z",
     "shell.execute_reply": "2026-02-06T07:00:52.831958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain:gpt-4o: 4/4 passed (100%)\n",
      "langchain:claude-sonnet-4-5-20250514: 4/4 passed (100%)\n",
      "langchain:gemini-2.0-flash: 3/4 passed (75%)\n"
     ]
    }
   ],
   "source": [
    "by_model = results.group_by_model(by=\"answering\")\n",
    "\n",
    "for model_name, model_results in by_model.items():\n",
    "    summary = model_results.get_summary()\n",
    "    pass_info = summary.get(\"template_pass_overall\", {})\n",
    "    passed = pass_info.get(\"passed\", 0)\n",
    "    total = pass_info.get(\"total\", 0)\n",
    "    pct = pass_info.get(\"pass_pct\", 0)\n",
    "    print(f\"{model_name}: {passed}/{total} passed ({pct:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae38d8",
   "metadata": {},
   "source": [
    "Filter to a specific model using the `interface:model_name` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b566bed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.833065Z",
     "iopub.status.busy": "2026-02-06T07:00:52.832990Z",
     "iopub.status.idle": "2026-02-06T07:00:52.834341Z",
     "shell.execute_reply": "2026-02-06T07:00:52.834151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o results: 5\n"
     ]
    }
   ],
   "source": [
    "gpt4_results = results.filter(answering_models=[\"langchain:gpt-4o\"])\n",
    "print(f\"GPT-4o results: {len(gpt4_results.results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21a767",
   "metadata": {},
   "source": [
    "## Step 4: Per-Question Comparison\n",
    "\n",
    "See how each model performed on the same question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71e93bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.835269Z",
     "iopub.status.busy": "2026-02-06T07:00:52.835214Z",
     "iopub.status.idle": "2026-02-06T07:00:52.836741Z",
     "shell.execute_reply": "2026-02-06T07:00:52.836546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is the capital of France?...\n",
      "  gpt-4o: PASS\n",
      "  claude-sonnet-4-5-20250514: PASS\n",
      "  gemini-2.0-flash: PASS\n",
      "\n",
      "What is 6 multiplied by 7?...\n",
      "  gpt-4o: PASS\n",
      "  claude-sonnet-4-5-20250514: PASS\n",
      "  gemini-2.0-flash: PASS\n",
      "\n",
      "What element has the atomic number 8? Provide both...\n",
      "  gpt-4o: PASS\n",
      "  claude-sonnet-4-5-20250514: PASS\n",
      "  gemini-2.0-flash: FAIL\n"
     ]
    }
   ],
   "source": [
    "by_question = results.group_by_question()\n",
    "\n",
    "for qid, q_results in list(by_question.items())[:3]:\n",
    "    q_text = q_results.results[0].metadata.question_text\n",
    "    print(f\"\\n{q_text[:50]}...\")\n",
    "    for r in q_results.results:\n",
    "        model = r.metadata.answering.model_name\n",
    "        passed = r.template.verify_result if r.template else \"N/A\"\n",
    "        print(f\"  {model}: {'PASS' if passed is True else 'FAIL' if passed is False else passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27b594",
   "metadata": {},
   "source": [
    "## Step 5: Separate Runs with `from_overrides`\n",
    "\n",
    "For independent result sets per model, loop with `from_overrides`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b46d833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.837684Z",
     "iopub.status.busy": "2026-02-06T07:00:52.837634Z",
     "iopub.status.idle": "2026-02-06T07:00:52.839793Z",
     "shell.execute_reply": "2026-02-06T07:00:52.839586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o: 4/4 passed\n",
      "claude-sonnet-4-5-20250514: 4/4 passed\n",
      "gemini-2.0-flash: 3/4 passed\n"
     ]
    }
   ],
   "source": [
    "models_to_compare = [\n",
    "    (\"gpt-4o\", \"openai\"),\n",
    "    (\"claude-sonnet-4-5-20250514\", \"anthropic\"),\n",
    "    (\"gemini-2.0-flash\", \"google_genai\"),\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "for model_name, provider in models_to_compare:\n",
    "    run_config = VerificationConfig.from_overrides(\n",
    "        answering_model=model_name,\n",
    "        answering_provider=provider,\n",
    "        answering_id=f\"ans-{model_name}\",\n",
    "        parsing_model=\"gpt-4o-mini\",\n",
    "        parsing_provider=\"openai\",\n",
    "        parsing_id=\"parser\",\n",
    "    )\n",
    "    run_results = benchmark.run_verification(run_config, run_name=model_name)\n",
    "    all_results[model_name] = run_results\n",
    "\n",
    "for model_name, model_results in all_results.items():\n",
    "    summary = model_results.get_summary()\n",
    "    pass_info = summary.get(\"template_pass_overall\", {})\n",
    "    print(f\"{model_name}: {pass_info.get('passed', 0)}/{pass_info.get('total', 0)} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bdc99",
   "metadata": {},
   "source": [
    "## Step 6: Replicates for Variance Analysis\n",
    "\n",
    "Run each model combination multiple times to measure consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420133ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.840704Z",
     "iopub.status.busy": "2026-02-06T07:00:52.840650Z",
     "iopub.status.idle": "2026-02-06T07:00:52.842672Z",
     "shell.execute_reply": "2026-02-06T07:00:52.842506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 15 (5 questions x 3 replicates)\n",
      "Replicate 0: 4/4 passed\n",
      "Replicate 1: 4/4 passed\n",
      "Replicate 2: 4/4 passed\n"
     ]
    }
   ],
   "source": [
    "config_with_reps = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"gpt4o\",\n",
    "            model_name=\"gpt-4o\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        ),\n",
    "    ],\n",
    "    replicate_count=3,\n",
    ")\n",
    "\n",
    "rep_results = benchmark.run_verification(config_with_reps)\n",
    "print(f\"Total results: {len(rep_results.results)} (5 questions x 3 replicates)\")\n",
    "\n",
    "by_replicate = rep_results.group_by_replicate()\n",
    "for rep_num, rep_group in sorted(by_replicate.items()):\n",
    "    summary = rep_group.get_summary()\n",
    "    pass_info = summary.get(\"template_pass_overall\", {})\n",
    "    print(f\"Replicate {rep_num}: {pass_info.get('passed', 0)}/{pass_info.get('total', 0)} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b25f8c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Approach | When to Use |\n",
    "|----------|------------|\n",
    "| Multiple `answering_models` list | Single run, answer caching, combined result set |\n",
    "| `from_overrides` loop | Separate result sets, per-model export, independent analysis |\n",
    "| Replicates | Variance analysis, consistency measurement |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Verification Result structure](../07-analyzing-results/verification-result.md) — full result hierarchy\n",
    "- [DataFrame analysis](../07-analyzing-results/dataframe-analysis.md) — convert results to pandas DataFrames\n",
    "- [Python API verification](../06-running-verification/python-api.md) — single-model workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d394f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:00:52.843542Z",
     "iopub.status.busy": "2026-02-06T07:00:52.843488Z",
     "iopub.status.idle": "2026-02-06T07:00:52.844834Z",
     "shell.execute_reply": "2026-02-06T07:00:52.844663Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Cleanup mocks\n",
    "_ = _patcher1.stop()\n",
    "_ = _patcher2.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
