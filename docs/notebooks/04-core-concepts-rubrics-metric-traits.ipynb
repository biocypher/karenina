{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca610b3",
   "metadata": {},
   "source": [
    "# Metric Traits\n",
    "\n",
    "Metric traits measure **extraction completeness** using a confusion-matrix approach. You define a set of instructions describing what the response should (or should not) contain, and the parsing model checks each one. The result is a set of precision, recall, and F1 metrics computed from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930bf267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:51:09.536770Z",
     "iopub.status.busy": "2026-02-06T01:51:09.536557Z",
     "iopub.status.idle": "2026-02-06T01:51:09.541584Z",
     "shell.execute_reply": "2026-02-06T01:51:09.541142Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: ensures examples execute without live API keys.\n",
    "# This cell is hidden in rendered documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc3d75",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A `MetricRubricTrait` evaluates how well a response covers a set of expected items. Unlike other trait types that return a single boolean or score, metric traits return **multiple metrics** (precision, recall, F1, and optionally specificity and accuracy) computed from a confusion matrix.\n",
    "\n",
    "| Field | Type | Default | Description |\n",
    "|-------|------|---------|-------------|\n",
    "| `name` | `str` | *(required)* | Human-readable identifier |\n",
    "| `description` | `str \\| None` | `None` | What this trait evaluates |\n",
    "| `evaluation_mode` | `Literal[\"tp_only\", \"full_matrix\"]` | `\"tp_only\"` | Evaluation approach |\n",
    "| `metrics` | `list[str]` | *(required)* | Metrics to compute (mode-dependent) |\n",
    "| `tp_instructions` | `list[str]` | *(required)* | What SHOULD be present in the answer |\n",
    "| `tn_instructions` | `list[str]` | `[]` | What should NOT be present (required in `full_matrix` mode) |\n",
    "| `repeated_extraction` | `bool` | `True` | Deduplicate repeated excerpts |\n",
    "\n",
    "**Key characteristics:**\n",
    "\n",
    "- Returns **multiple metrics** (not a single value) as a dictionary\n",
    "- Requires an **LLM call** -- the parsing model categorizes instructions into confusion matrix buckets\n",
    "- Two evaluation modes: `tp_only` (precision/recall/F1) and `full_matrix` (adds specificity/accuracy)\n",
    "- Instructions are natural-language descriptions, not regex patterns\n",
    "\n",
    "## TP-Only Mode\n",
    "\n",
    "In `tp_only` mode, you define instructions for what **should be present** in the response. The parsing model then categorizes each instruction:\n",
    "\n",
    "- **TP (True Positive)**: Instruction found in the answer\n",
    "- **FN (False Negative)**: Instruction missing from the answer\n",
    "- **FP (False Positive)**: Extra content not matching any instruction\n",
    "\n",
    "Available metrics: `precision`, `recall`, `f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d73446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:51:09.543587Z",
     "iopub.status.busy": "2026-02-06T01:51:09.543446Z",
     "iopub.status.idle": "2026-02-06T01:51:09.865042Z",
     "shell.execute_reply": "2026-02-06T01:51:09.864823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Reference Coverage\n",
      "Mode: tp_only\n",
      "Metrics: ['precision', 'recall', 'f1']\n",
      "TP instructions: 3\n",
      "Required buckets: {'tp', 'fp', 'fn'}\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import MetricRubricTrait\n",
    "\n",
    "# Check if a response covers key references\n",
    "reference_trait = MetricRubricTrait(\n",
    "    name=\"Reference Coverage\",\n",
    "    description=\"Check if response covers key references\",\n",
    "    evaluation_mode=\"tp_only\",\n",
    "    metrics=[\"precision\", \"recall\", \"f1\"],\n",
    "    tp_instructions=[\n",
    "        \"Mentions Tsujimoto et al., Science, 1985\",\n",
    "        \"Mentions Hockenbery et al., Nature, 1990\",\n",
    "        \"Mentions Adams & Cory, Science, 1998\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Trait: {reference_trait.name}\")\n",
    "print(f\"Mode: {reference_trait.evaluation_mode}\")\n",
    "print(f\"Metrics: {reference_trait.metrics}\")\n",
    "print(f\"TP instructions: {len(reference_trait.tp_instructions)}\")\n",
    "print(f\"Required buckets: {reference_trait.get_required_buckets()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fe2f4",
   "metadata": {},
   "source": [
    "### Metric Formulas\n",
    "\n",
    "The metrics are computed from the confusion matrix counts:\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|---------------|\n",
    "| Precision | TP / (TP + FP) | Of items found, how many were expected? |\n",
    "| Recall | TP / (TP + FN) | Of expected items, how many were found? |\n",
    "| F1 | 2 * P * R / (P + R) | Harmonic mean of precision and recall |\n",
    "\n",
    "For example, if you define 3 TP instructions and the response contains 2 of them plus 1 extra item:\n",
    "\n",
    "- TP = 2, FN = 1, FP = 1\n",
    "- Precision = 2/3 = 0.67, Recall = 2/3 = 0.67, F1 = 0.67\n",
    "\n",
    "## Full Matrix Mode\n",
    "\n",
    "In `full_matrix` mode, you define both what **should** and what **should not** be present. This adds specificity and accuracy to the available metrics:\n",
    "\n",
    "- **TP (True Positive)**: TP instruction found in the answer\n",
    "- **FN (False Negative)**: TP instruction missing from the answer\n",
    "- **TN (True Negative)**: TN instruction correctly absent\n",
    "- **FP (False Positive)**: TN instruction incorrectly present\n",
    "\n",
    "Available metrics: `precision`, `recall`, `specificity`, `accuracy`, `f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c66491e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:51:09.866136Z",
     "iopub.status.busy": "2026-02-06T01:51:09.866053Z",
     "iopub.status.idle": "2026-02-06T01:51:09.868242Z",
     "shell.execute_reply": "2026-02-06T01:51:09.868008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait: Content Accuracy\n",
      "Mode: full_matrix\n",
      "Metrics: ['precision', 'recall', 'specificity', 'accuracy', 'f1']\n",
      "TP instructions: 3\n",
      "TN instructions: 2\n",
      "Required buckets: {'tp', 'fp', 'tn', 'fn'}\n"
     ]
    }
   ],
   "source": [
    "# Check content accuracy with both positive and negative assertions\n",
    "accuracy_trait = MetricRubricTrait(\n",
    "    name=\"Content Accuracy\",\n",
    "    description=\"Check content accuracy for BCL2 gene information\",\n",
    "    evaluation_mode=\"full_matrix\",\n",
    "    metrics=[\"precision\", \"recall\", \"specificity\", \"accuracy\", \"f1\"],\n",
    "    tp_instructions=[\n",
    "        \"Mentions BCL2 gene\",\n",
    "        \"Discusses apoptosis regulation\",\n",
    "        \"References cancer research\",\n",
    "    ],\n",
    "    tn_instructions=[\n",
    "        \"Claims BCL2 is pro-apoptotic\",       # Should NOT be present (it's anti-apoptotic)\n",
    "        \"States BCL2 is on chromosome 1\",      # Should NOT be present (it's on chromosome 18)\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Trait: {accuracy_trait.name}\")\n",
    "print(f\"Mode: {accuracy_trait.evaluation_mode}\")\n",
    "print(f\"Metrics: {accuracy_trait.metrics}\")\n",
    "print(f\"TP instructions: {len(accuracy_trait.tp_instructions)}\")\n",
    "print(f\"TN instructions: {len(accuracy_trait.tn_instructions)}\")\n",
    "print(f\"Required buckets: {accuracy_trait.get_required_buckets()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcf89b",
   "metadata": {},
   "source": [
    "### Additional Metrics\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|---------------|\n",
    "| Specificity | TN / (TN + FP) | Of things that should be absent, how many actually are? |\n",
    "| Accuracy | (TP + TN) / (TP + TN + FP + FN) | Overall correctness across all instructions |\n",
    "\n",
    "## Choosing a Mode\n",
    "\n",
    "| Scenario | Mode | Why |\n",
    "|----------|------|-----|\n",
    "| Check if key facts are mentioned | `tp_only` | Only care about presence of expected items |\n",
    "| Verify entity extraction coverage | `tp_only` | Measure what was found vs. missed |\n",
    "| Check both correct and incorrect claims | `full_matrix` | Need to verify absence of wrong information |\n",
    "| Evaluate factual accuracy | `full_matrix` | Important to catch both missing truths and present falsehoods |\n",
    "\n",
    "## The `repeated_extraction` Option\n",
    "\n",
    "By default, `repeated_extraction=True` deduplicates repeated excerpts using case-insensitive exact matching. This prevents the same instruction from being counted multiple times in the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7bb9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:51:09.869206Z",
     "iopub.status.busy": "2026-02-06T01:51:09.869151Z",
     "iopub.status.idle": "2026-02-06T01:51:09.870898Z",
     "shell.execute_reply": "2026-02-06T01:51:09.870707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated extraction: True\n",
      "Repeated extraction: False\n"
     ]
    }
   ],
   "source": [
    "# Default: deduplicate repeated extractions\n",
    "dedup_trait = MetricRubricTrait(\n",
    "    name=\"Entity Check\",\n",
    "    evaluation_mode=\"tp_only\",\n",
    "    metrics=[\"precision\", \"recall\"],\n",
    "    tp_instructions=[\"Mentions mitochondria\", \"Mentions apoptosis\"],\n",
    "    repeated_extraction=True,  # default\n",
    ")\n",
    "print(f\"Repeated extraction: {dedup_trait.repeated_extraction}\")\n",
    "\n",
    "# Disable deduplication if the same instruction can legitimately appear multiple times\n",
    "no_dedup_trait = MetricRubricTrait(\n",
    "    name=\"Keyword Frequency\",\n",
    "    evaluation_mode=\"tp_only\",\n",
    "    metrics=[\"precision\", \"recall\"],\n",
    "    tp_instructions=[\"Mentions mitochondria\", \"Mentions apoptosis\"],\n",
    "    repeated_extraction=False,\n",
    ")\n",
    "print(f\"Repeated extraction: {no_dedup_trait.repeated_extraction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ffbc1",
   "metadata": {},
   "source": [
    "## Confusion Matrix Output\n",
    "\n",
    "When a metric trait is evaluated during verification, the results include both the computed metrics and the raw confusion matrix lists. This lets you inspect exactly which instructions were found, missed, or incorrectly present.\n",
    "\n",
    "The result structure in `VerificationResult.rubric_results` includes:\n",
    "\n",
    "    metric_trait_confusion_lists: dict[str, dict[str, list[str]]]\n",
    "\n",
    "For example, a TP-only trait might produce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bfcef9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:51:09.871823Z",
     "iopub.status.busy": "2026-02-06T01:51:09.871771Z",
     "iopub.status.idle": "2026-02-06T01:51:09.873195Z",
     "shell.execute_reply": "2026-02-06T01:51:09.873013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example output structure (from VerificationResult)\n",
    "confusion_lists = {\n",
    "    \"Reference Coverage\": {\n",
    "        \"tp\": [\"Mentions Tsujimoto et al., Science, 1985\", \"Mentions Adams & Cory, Science, 1998\"],\n",
    "        \"fn\": [\"Mentions Hockenbery et al., Nature, 1990\"],\n",
    "        \"fp\": [\"Discusses BCL2 protein structure\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343278d8",
   "metadata": {},
   "source": [
    "And a full-matrix trait would additionally include a `\"tn\"` key with instructions that were correctly absent.\n",
    "\n",
    "## Validation\n",
    "\n",
    "Metric traits enforce strict validation at construction time:\n",
    "\n",
    "- `tp_instructions` must be non-empty (always required)\n",
    "- `tn_instructions` must be non-empty in `full_matrix` mode\n",
    "- Metrics must be valid for the chosen mode\n",
    "- You cannot request `specificity` or `accuracy` in `tp_only` mode (they require TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcee9fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:51:09.874069Z",
     "iopub.status.busy": "2026-02-06T01:51:09.874019Z",
     "iopub.status.idle": "2026-02-06T01:51:09.875720Z",
     "shell.execute_reply": "2026-02-06T01:51:09.875543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: specificity requires full_matrix mode\n",
      "Validation error: full_matrix requires tn_instructions\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "# Trying to use specificity in tp_only mode raises an error\n",
    "try:\n",
    "    MetricRubricTrait(\n",
    "        name=\"invalid\",\n",
    "        evaluation_mode=\"tp_only\",\n",
    "        metrics=[\"specificity\"],  # Not available in tp_only mode\n",
    "        tp_instructions=[\"test\"],\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"Validation error: specificity requires full_matrix mode\")\n",
    "\n",
    "# Missing tn_instructions in full_matrix mode\n",
    "try:\n",
    "    MetricRubricTrait(\n",
    "        name=\"invalid\",\n",
    "        evaluation_mode=\"full_matrix\",\n",
    "        metrics=[\"precision\"],\n",
    "        tp_instructions=[\"test\"],\n",
    "        # tn_instructions missing!\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"Validation error: full_matrix requires tn_instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eebf470",
   "metadata": {},
   "source": [
    "## Using Metric Traits in a Rubric\n",
    "\n",
    "Metric traits are added to a `Rubric` via the `metric_traits` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe453e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:51:09.876681Z",
     "iopub.status.busy": "2026-02-06T01:51:09.876622Z",
     "iopub.status.idle": "2026-02-06T01:51:09.878462Z",
     "shell.execute_reply": "2026-02-06T01:51:09.878283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric traits: ['Topic Coverage', 'Factual Accuracy']\n",
      "Total traits: 2\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import Rubric\n",
    "\n",
    "# Create traits for different aspects\n",
    "coverage_trait = MetricRubricTrait(\n",
    "    name=\"Topic Coverage\",\n",
    "    evaluation_mode=\"tp_only\",\n",
    "    metrics=[\"recall\", \"f1\"],\n",
    "    tp_instructions=[\n",
    "        \"Discusses the mechanism of action\",\n",
    "        \"Mentions clinical applications\",\n",
    "        \"References recent studies\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "accuracy_trait = MetricRubricTrait(\n",
    "    name=\"Factual Accuracy\",\n",
    "    evaluation_mode=\"full_matrix\",\n",
    "    metrics=[\"precision\", \"recall\", \"accuracy\"],\n",
    "    tp_instructions=[\n",
    "        \"States the drug targets HER2\",\n",
    "        \"Mentions breast cancer indication\",\n",
    "    ],\n",
    "    tn_instructions=[\n",
    "        \"Claims the drug is a small molecule\",  # It's a monoclonal antibody\n",
    "    ],\n",
    ")\n",
    "\n",
    "rubric = Rubric(metric_traits=[coverage_trait, accuracy_trait])\n",
    "print(f\"Metric traits: {rubric.get_metric_trait_names()}\")\n",
    "print(f\"Total traits: {len(rubric.metric_traits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40586a9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [LLM Rubric Traits](llm-traits.md) -- Boolean and score evaluation via LLM judgment\n",
    "- [Regex Traits](regex-traits.md) -- Pattern matching on raw response text\n",
    "- [Callable Traits](callable-traits.md) -- Custom Python function evaluation\n",
    "- [Rubrics Overview](index.md) -- When to use each trait type\n",
    "- [Defining Rubrics](../../05-creating-benchmarks/defining-rubrics.md) -- Adding traits to benchmarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
