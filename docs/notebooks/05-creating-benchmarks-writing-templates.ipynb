{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18770e4",
   "metadata": {},
   "source": [
    "# Writing Custom Templates\n",
    "\n",
    "Custom templates give you full control over evaluation logic. While [automatic generation](generating-templates.md) works well for straightforward questions, custom templates are essential when you need complex comparison logic, domain-specific tolerance, or multi-step verification.\n",
    "\n",
    "This page covers practical patterns for writing templates by hand. For the conceptual foundation (what templates are, field types, `verify()` basics), see [Answer Templates](../04-core-concepts/answer-templates.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72b18e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:33.962050Z",
     "iopub.status.busy": "2026-02-06T07:22:33.961993Z",
     "iopub.status.idle": "2026-02-06T07:22:34.308804Z",
     "shell.execute_reply": "2026-02-06T07:22:34.308515Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell: ensures examples execute without live API keys.\n",
    "# This cell is hidden in rendered documentation.\n",
    "from pydantic import Field\n",
    "\n",
    "from karenina.schemas.entities import BaseAnswer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ac9f4",
   "metadata": {},
   "source": [
    "## When to Write Custom Templates\n",
    "\n",
    "Write templates manually when:\n",
    "\n",
    "- **Fuzzy matching** is needed (e.g., \"BCL2\" should match \"Bcl-2\" and \"BCL-2\")\n",
    "- **Numeric tolerance** applies (e.g., within 5% of the expected value)\n",
    "- **List comparison** requires set semantics, not exact ordering\n",
    "- **Conditional logic** depends on which fields are populated\n",
    "- **Domain-specific normalization** is needed before comparison\n",
    "\n",
    "## Case-Insensitive String Matching\n",
    "\n",
    "The most common pattern normalizes strings before comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e80630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.310274Z",
     "iopub.status.busy": "2026-02-06T07:22:34.310161Z",
     "iopub.status.idle": "2026-02-06T07:22:34.313129Z",
     "shell.execute_reply": "2026-02-06T07:22:34.312832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tp53'       → verify(): True\n",
      "' TP53 '     → verify(): True\n",
      "'Tp53'       → verify(): True\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    gene_symbol: str = Field(description=\"The gene symbol mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"gene_symbol\": \"TP53\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.gene_symbol.strip().upper() == self.correct[\"gene_symbol\"].upper()\n",
    "\n",
    "\n",
    "# Handles variations: \"tp53\", \" TP53 \", \"Tp53\"\n",
    "for variant in [\"tp53\", \" TP53 \", \"Tp53\"]:\n",
    "    parsed = Answer(gene_symbol=variant)\n",
    "    print(f\"{variant!r:12s} → verify(): {parsed.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397e416",
   "metadata": {},
   "source": [
    "## Numeric Tolerance\n",
    "\n",
    "For questions involving measurements or estimates, exact comparison is often too strict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab111f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.314327Z",
     "iopub.status.busy": "2026-02-06T07:22:34.314257Z",
     "iopub.status.idle": "2026-02-06T07:22:34.316674Z",
     "shell.execute_reply": "2026-02-06T07:22:34.316406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.0°C → verify(): True\n",
      "  99.8°C → verify(): True\n",
      " 100.3°C → verify(): True\n",
      " 101.0°C → verify(): False\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    temperature: float = Field(description=\"The boiling point temperature in degrees Celsius\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"temperature\": 100.0}\n",
    "        self.tolerance = 0.5  # Accept within ±0.5°C\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return abs(self.temperature - self.correct[\"temperature\"]) <= self.tolerance\n",
    "\n",
    "\n",
    "# Exact and approximate values both pass\n",
    "for temp in [100.0, 99.8, 100.3, 101.0]:\n",
    "    parsed = Answer(temperature=temp)\n",
    "    print(f\"{temp:6.1f}°C → verify(): {parsed.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c2dd6",
   "metadata": {},
   "source": [
    "## Percentage-Based Tolerance\n",
    "\n",
    "For values that span wide ranges, use relative tolerance instead of absolute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561c9000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.318622Z",
     "iopub.status.busy": "2026-02-06T07:22:34.318506Z",
     "iopub.status.idle": "2026-02-06T07:22:34.321221Z",
     "shell.execute_reply": "2026-02-06T07:22:34.320856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8,336,817 → verify(): True\n",
      " 8,000,000 → verify(): True\n",
      " 9,000,000 → verify(): True\n",
      " 7,000,000 → verify(): False\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    population: int = Field(description=\"The estimated population of the city\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"population\": 8_336_817}\n",
    "        self.tolerance_pct = 10  # Accept within 10%\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        expected = self.correct[\"population\"]\n",
    "        threshold = expected * (self.tolerance_pct / 100)\n",
    "        return abs(self.population - expected) <= threshold\n",
    "\n",
    "\n",
    "# Values within 10% of 8,336,817 pass\n",
    "for pop in [8_336_817, 8_000_000, 9_000_000, 7_000_000]:\n",
    "    parsed = Answer(population=pop)\n",
    "    print(f\"{pop:>10,} → verify(): {parsed.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da369e0d",
   "metadata": {},
   "source": [
    "## Set-Based List Comparison\n",
    "\n",
    "When order doesn't matter, compare lists as sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51597881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.322412Z",
     "iopub.status.busy": "2026-02-06T07:22:34.322315Z",
     "iopub.status.idle": "2026-02-06T07:22:34.325169Z",
     "shell.execute_reply": "2026-02-06T07:22:34.324902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: ['Fatigue', 'fever', 'Cough']\n",
      "verify():  True\n",
      "\n",
      "Missing item: ['fever', 'cough']\n",
      "verify():     False\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    symptoms: list[str] = Field(description=\"The symptoms of the condition listed in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"symptoms\": [\"fever\", \"cough\", \"fatigue\"]}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        extracted = {s.strip().lower() for s in self.symptoms}\n",
    "        expected = {s.lower() for s in self.correct[\"symptoms\"]}\n",
    "        return extracted == expected\n",
    "\n",
    "\n",
    "# Order doesn't matter; normalization handles case\n",
    "parsed = Answer(symptoms=[\"Fatigue\", \"fever\", \"Cough\"])\n",
    "print(f\"Extracted: {parsed.symptoms}\")\n",
    "print(f\"verify():  {parsed.verify()}\")\n",
    "\n",
    "# Missing or extra items fail\n",
    "parsed2 = Answer(symptoms=[\"fever\", \"cough\"])\n",
    "print(f\"\\nMissing item: {parsed2.symptoms}\")\n",
    "print(f\"verify():     {parsed2.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55990eaa",
   "metadata": {},
   "source": [
    "## Subset Matching\n",
    "\n",
    "Sometimes you want to check that the response includes at least the expected items, but extra items are acceptable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ad8797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.326563Z",
     "iopub.status.busy": "2026-02-06T07:22:34.326444Z",
     "iopub.status.idle": "2026-02-06T07:22:34.329343Z",
     "shell.execute_reply": "2026-02-06T07:22:34.329067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: ['EGFR', 'RAS', 'RAF', 'MEK', 'ERK']\n",
      "verify():  True\n",
      "\n",
      "Missing required: ['EGFR', 'MEK']\n",
      "verify():         False\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    proteins: list[str] = Field(description=\"Proteins involved in the signaling pathway mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"required_proteins\": [\"EGFR\", \"RAS\", \"RAF\"]}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        extracted = {p.strip().upper() for p in self.proteins}\n",
    "        required = {p.upper() for p in self.correct[\"required_proteins\"]}\n",
    "        return required.issubset(extracted)\n",
    "\n",
    "\n",
    "# Extra proteins are fine as long as required ones are present\n",
    "parsed = Answer(proteins=[\"EGFR\", \"RAS\", \"RAF\", \"MEK\", \"ERK\"])\n",
    "print(f\"Extracted: {parsed.proteins}\")\n",
    "print(f\"verify():  {parsed.verify()}\")\n",
    "\n",
    "# Missing a required protein fails\n",
    "parsed2 = Answer(proteins=[\"EGFR\", \"MEK\"])\n",
    "print(f\"\\nMissing required: {parsed2.proteins}\")\n",
    "print(f\"verify():         {parsed2.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e8eed",
   "metadata": {},
   "source": [
    "## Multi-Field with Partial Credit\n",
    "\n",
    "For complex templates with multiple attributes, implement both `verify()` (all-or-nothing) and `verify_granular()` (partial credit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1914115f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.331019Z",
     "iopub.status.busy": "2026-02-06T07:22:34.330933Z",
     "iopub.status.idle": "2026-02-06T07:22:34.334715Z",
     "shell.execute_reply": "2026-02-06T07:22:34.334359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug:      True\n",
      "Target:    True\n",
      "Mechanism: False\n",
      "verify():          False\n",
      "verify_granular(): 0.67\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    drug_name: str = Field(description=\"The name of the drug mentioned in the response\")\n",
    "    target: str = Field(description=\"The protein target of the drug\")\n",
    "    mechanism: str = Field(description=\"The mechanism of action (e.g., inhibitor, agonist)\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\n",
    "            \"drug_name\": \"venetoclax\",\n",
    "            \"target\": \"BCL2\",\n",
    "            \"mechanism\": \"inhibitor\",\n",
    "        }\n",
    "\n",
    "    def _check_drug_name(self) -> bool:\n",
    "        return self.drug_name.strip().lower() == self.correct[\"drug_name\"].lower()\n",
    "\n",
    "    def _check_target(self) -> bool:\n",
    "        # Normalize common gene symbol variations\n",
    "        extracted = self.target.strip().upper().replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "        expected = self.correct[\"target\"].upper().replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "        return extracted == expected\n",
    "\n",
    "    def _check_mechanism(self) -> bool:\n",
    "        return self.mechanism.strip().lower() == self.correct[\"mechanism\"].lower()\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self._check_drug_name() and self._check_target() and self._check_mechanism()\n",
    "\n",
    "    def verify_granular(self) -> float:\n",
    "        checks = [self._check_drug_name(), self._check_target(), self._check_mechanism()]\n",
    "        return sum(checks) / len(checks)\n",
    "\n",
    "\n",
    "# 2 out of 3 correct → verify() fails, verify_granular() gives partial credit\n",
    "parsed = Answer(drug_name=\"Venetoclax\", target=\"Bcl-2\", mechanism=\"agonist\")\n",
    "print(f\"Drug:      {parsed._check_drug_name()}\")\n",
    "print(f\"Target:    {parsed._check_target()}\")\n",
    "print(f\"Mechanism: {parsed._check_mechanism()}\")\n",
    "print(f\"verify():          {parsed.verify()}\")\n",
    "print(f\"verify_granular(): {parsed.verify_granular():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b66ba",
   "metadata": {},
   "source": [
    "## Boolean Attribute Pattern\n",
    "\n",
    "For rigorous evaluation, use boolean fields that check for concept presence rather than extracting text. This avoids string matching pitfalls entirely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d59284f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.336110Z",
     "iopub.status.busy": "2026-02-06T07:22:34.336026Z",
     "iopub.status.idle": "2026-02-06T07:22:34.339582Z",
     "shell.execute_reply": "2026-02-06T07:22:34.339142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify():          False\n",
      "verify_granular(): 0.67\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    mentions_bcl2: bool = Field(description=\"True if the response identifies BCL2 (or BCL-2, Bcl-2) as the target\")\n",
    "    mentions_inhibition: bool = Field(\n",
    "        description=\"True if the response describes inhibition as the mechanism of action\"\n",
    "    )\n",
    "    mentions_apoptosis: bool = Field(description=\"True if the response mentions apoptosis or programmed cell death\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\n",
    "            \"mentions_bcl2\": True,\n",
    "            \"mentions_inhibition\": True,\n",
    "            \"mentions_apoptosis\": True,\n",
    "        }\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return all(getattr(self, field) == self.correct[field] for field in self.correct)\n",
    "\n",
    "    def verify_granular(self) -> float:\n",
    "        matches = sum(1 for field in self.correct if getattr(self, field) == self.correct[field])\n",
    "        return matches / len(self.correct)\n",
    "\n",
    "\n",
    "parsed = Answer(mentions_bcl2=True, mentions_inhibition=True, mentions_apoptosis=False)\n",
    "print(f\"verify():          {parsed.verify()}\")\n",
    "print(f\"verify_granular(): {parsed.verify_granular():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ba3b2",
   "metadata": {},
   "source": [
    "## Adding Templates to a Benchmark\n",
    "\n",
    "Templates are added to benchmarks as **code strings** via `add_question()` or `add_answer_template()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743ab211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T07:22:34.341358Z",
     "iopub.status.busy": "2026-02-06T07:22:34.341246Z",
     "iopub.status.idle": "2026-02-06T07:22:34.344578Z",
     "shell.execute_reply": "2026-02-06T07:22:34.344209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question added with template: urn:uuid:question-what-is-the-approved-d...\n",
      "Template added to: urn:uuid:question-how-many-chromosomes-a...\n",
      "\n",
      "Total questions: 2\n",
      "With templates:  1\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.create(name=\"Custom Templates Example\")\n",
    "\n",
    "# Option 1: Provide template when adding the question\n",
    "template_code = \"\"\"class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target mentioned\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCL2\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.target.strip().upper() == self.correct[\"target\"].upper()\n",
    "\"\"\"\n",
    "\n",
    "question_id = benchmark.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\",\n",
    "    raw_answer=\"BCL2\",\n",
    "    answer_template=template_code,\n",
    ")\n",
    "print(f\"Question added with template: {question_id[:40]}...\")\n",
    "\n",
    "# Option 2: Add template to an existing question\n",
    "q2_id = benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\",\n",
    "    raw_answer=\"46\",\n",
    ")\n",
    "\n",
    "count_template = \"\"\"class Answer(BaseAnswer):\n",
    "    count: int = Field(description=\"The number of chromosomes mentioned\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"count\": 46}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.count == self.correct[\"count\"]\n",
    "\"\"\"\n",
    "\n",
    "benchmark.add_answer_template(q2_id, count_template)\n",
    "print(f\"Template added to: {q2_id[:40]}...\")\n",
    "\n",
    "# Check status\n",
    "print(f\"\\nTotal questions: {benchmark.question_count}\")\n",
    "print(f\"With templates:  {len(benchmark.get_finished_templates())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b37907",
   "metadata": {},
   "source": [
    "!!! tip \"Code strings vs class objects\"\n",
    "    In notebooks, always use **code strings** for templates. Passing a class object requires `inspect.getsource()` which doesn't work reliably in notebook environments. Code strings work everywhere.\n",
    "\n",
    "## Template Design Guidelines\n",
    "\n",
    "**Keep verify() deterministic.** The `verify()` method should always produce the same result for the same input. Avoid randomness, network calls, or time-dependent logic.\n",
    "\n",
    "**Normalize before comparing.** Strip whitespace, standardize case, and handle common formatting variations (hyphens, underscores, spaces in gene symbols).\n",
    "\n",
    "**Use helper methods.** For multi-field templates, extract each field check into a private method (e.g., `_check_target()`). This makes `verify_granular()` easy to implement and simplifies debugging.\n",
    "\n",
    "**Write clear field descriptions.** The Judge LLM only sees the field name, type, and description. A vague description like \"The answer\" will produce unreliable parsing. Be specific about what to extract and how.\n",
    "\n",
    "**Test locally before running verification.** Instantiate your template with sample values and call `verify()` and `verify_granular()` to confirm the logic works before adding it to a benchmark.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Generating Templates](generating-templates.md) — Automatic template generation for common question types\n",
    "- [Defining Rubrics](defining-rubrics.md) — Add quality assessment alongside correctness checks\n",
    "- [Saving Benchmarks](saving-benchmarks.md) — Save your benchmark with templates to a checkpoint\n",
    "- [Answer Templates](../04-core-concepts/answer-templates.md) — Conceptual foundation (field types, naming requirement)\n",
    "- [Running Verification](../06-running-verification/index.md) — Execute verification with your custom templates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
