{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEPA Integration Quick Start\n",
    "\n",
    "This notebook provides a quick introduction to the GEPA (Generative Evolutionary Prompt Advancement) integration in Karenina. GEPA is a framework for automatically optimizing prompts and instructions used in LLM evaluation pipelines.\n",
    "\n",
    "## What is GEPA?\n",
    "\n",
    "GEPA uses evolutionary algorithms combined with LLM reflection to iteratively improve text components like:\n",
    "- **Answering system prompts**: The system prompt given to models being evaluated\n",
    "- **Parsing instructions**: Instructions for the judge LLM that extracts structured data\n",
    "- **MCP tool descriptions**: Descriptions that guide model tool selection\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install karenina with GEPA support\n",
    "pip install karenina[gepa]\n",
    "\n",
    "# Or install GEPA separately\n",
    "pip install gepa\n",
    "\n",
    "# Set your API key\n",
    "export ANTHROPIC_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add karenina to path (for development)\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "# Core imports\n",
    "from karenina import Benchmark\n",
    "\n",
    "# GEPA integration imports\n",
    "from karenina.integrations.gepa import (\n",
    "    GEPA_AVAILABLE,\n",
    "    OptimizationConfig,\n",
    "    OptimizationTarget,\n",
    "    OptimizationTracker,\n",
    "    compute_single_score,\n",
    "    export_to_preset,\n",
    "    split_benchmark,\n",
    ")\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "print(f\"GEPA available: {GEPA_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the AIME 2025 Benchmark\n",
    "\n",
    "We'll use the AIME 2025 benchmark which contains 30 math problems with integer answers (0-999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the benchmark\n",
    "benchmark_path = Path.home() / \"Projects/karenina-monorepo/local_data/data/checkpoints/aime_2025.jsonld\"\n",
    "benchmark = Benchmark.load(benchmark_path)\n",
    "\n",
    "print(f\"Benchmark: {benchmark.name}\")\n",
    "print(f\"Description: {benchmark.description}\")\n",
    "print(f\"Questions: {len(benchmark.get_question_ids())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Overview of Key Components\n",
    "\n",
    "### 1. OptimizationTarget\n",
    "\n",
    "Specifies what text components can be optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available optimization targets\n",
    "for target in OptimizationTarget:\n",
    "    print(f\"{target.name}: {target.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. OptimizationConfig\n",
    "\n",
    "Configuration for a GEPA optimization run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimization config\n",
    "opt_config = OptimizationConfig(\n",
    "    # What to optimize\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    # Initial seed prompt\n",
    "    seed_answering_prompt=\"You are a helpful math assistant. Solve the problem step by step.\",\n",
    "    # Scoring weights (must sum to 1.0)\n",
    "    template_weight=0.7,  # Weight for correctness\n",
    "    rubric_weight=0.3,  # Weight for quality traits\n",
    "    # Data splitting\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.1,\n",
    "    split_seed=42,\n",
    "    # GEPA parameters\n",
    "    reflection_model=\"anthropic/claude-haiku-4-5\",\n",
    "    max_metric_calls=50,\n",
    ")\n",
    "\n",
    "print(\"Optimization Config:\")\n",
    "print(f\"  Targets: {[t.value for t in opt_config.targets]}\")\n",
    "print(f\"  Weights: template={opt_config.template_weight}, rubric={opt_config.rubric_weight}\")\n",
    "print(f\"  Split: train={opt_config.train_ratio}, val={opt_config.val_ratio}, test={opt_config.test_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Benchmark Splitting\n",
    "\n",
    "Split the benchmark into train/val/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the benchmark\n",
    "split = split_benchmark(\n",
    "    benchmark,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(split.summary())\n",
    "print(f\"\\nTrain question IDs: {split.train_ids[:3]}...\")\n",
    "print(f\"Val question IDs: {split.val_ids[:2]}...\")\n",
    "print(f\"Test question IDs: {split.test_ids[:1] if split.test_ids else 'None'}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. KareninaDataInst\n",
    "\n",
    "Each split contains `KareninaDataInst` objects - the GEPA-compatible representation of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a data instance\n",
    "sample_inst = split.train[0]\n",
    "\n",
    "print(f\"Question ID: {sample_inst.question_id[:50]}...\")\n",
    "print(f\"Question: {sample_inst.question_text[:100]}...\")\n",
    "print(f\"Answer: {sample_inst.raw_answer}\")\n",
    "print(f\"Has template: {len(sample_inst.template_code) > 0}\")\n",
    "print(f\"Has rubric: {sample_inst.rubric is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run Verification (Baseline)\n",
    "\n",
    "Before optimization, let's run verification to establish a baseline score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create verification config with Claude Haiku\n",
    "verification_config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are a helpful math assistant. Solve the problem step by step and provide the final integer answer.\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku-parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "print(\"Verification config created\")\n",
    "print(f\"  Answering model: {verification_config.answering_models[0].model_name}\")\n",
    "print(f\"  Parsing model: {verification_config.parsing_models[0].model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run verification on a subset (first 5 questions) to get baseline\n",
    "sample_ids = split.train_ids[:5]\n",
    "\n",
    "print(f\"Running verification on {len(sample_ids)} questions...\")\n",
    "results = benchmark.run_verification(verification_config, question_ids=sample_ids)\n",
    "\n",
    "# Calculate scores\n",
    "scores = []\n",
    "for result in results:\n",
    "    score = compute_single_score(result, template_weight=1.0, rubric_weight=0.0)\n",
    "    scores.append(score)\n",
    "    status = \"✓\" if score > 0 else \"✗\"\n",
    "    print(f\"  {status} {result.metadata.question_id[:40]}... Score: {score:.2f}\")\n",
    "\n",
    "baseline_score = sum(scores) / len(scores) if scores else 0.0\n",
    "print(f\"\\nBaseline Score: {baseline_score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimization Tracking\n",
    "\n",
    "Track optimization runs for reproducibility and comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "from karenina.integrations.gepa import OptimizationRun\n",
    "\n",
    "# Create a tracker (using temp directory for this demo)\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "tracker = OptimizationTracker(temp_dir / \"optimization_history.db\")\n",
    "\n",
    "# Log a sample run (simulating what would happen after optimization)\n",
    "sample_run = OptimizationRun(\n",
    "    benchmark_name=\"AIME 2025\",\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT.value],\n",
    "    seed_prompts={\"answering_system_prompt\": \"You are a helpful math assistant.\"},\n",
    "    optimized_prompts={\n",
    "        \"answering_system_prompt\": \"You are an expert competition mathematician. Solve AIME problems systematically.\"\n",
    "    },\n",
    "    train_score=0.75,\n",
    "    val_score=0.70,\n",
    "    test_score=0.68,\n",
    "    improvement=0.15,  # 15% improvement\n",
    "    reflection_model=\"anthropic/claude-haiku-4-5\",\n",
    "    metric_calls=50,\n",
    "    best_generation=8,\n",
    "    total_generations=10,\n",
    ")\n",
    "\n",
    "run_id = tracker.log_run(sample_run)\n",
    "print(f\"Logged run: {run_id}\")\n",
    "\n",
    "# Retrieve the run\n",
    "retrieved = tracker.get_run(run_id)\n",
    "print(\"\\nRetrieved run:\")\n",
    "print(f\"  Benchmark: {retrieved.benchmark_name}\")\n",
    "print(f\"  Val Score: {retrieved.val_score:.2%}\")\n",
    "print(f\"  Improvement: {retrieved.improvement:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export Optimized Prompts\n",
    "\n",
    "Export optimized prompts as a Karenina preset for reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.integrations.gepa import export_prompts_json\n",
    "\n",
    "# Export as a verification preset\n",
    "optimized_prompts = {\n",
    "    \"answering_system_prompt\": \"You are an expert competition mathematician specialized in AIME problems. Always show your work step by step and box the final integer answer.\"\n",
    "}\n",
    "\n",
    "preset_path = export_to_preset(\n",
    "    optimized_prompts=optimized_prompts,\n",
    "    base_config=verification_config,\n",
    "    output_path=temp_dir / \"optimized_preset.json\",\n",
    ")\n",
    "\n",
    "print(f\"Exported preset to: {preset_path}\")\n",
    "\n",
    "# Also export as lightweight JSON\n",
    "prompts_path = export_prompts_json(\n",
    "    optimized_prompts=optimized_prompts,\n",
    "    metadata={\n",
    "        \"benchmark\": \"AIME 2025\",\n",
    "        \"improvement\": 0.15,\n",
    "        \"train_score\": 0.75,\n",
    "        \"val_score\": 0.70,\n",
    "    },\n",
    "    output_path=temp_dir / \"optimized_prompts.json\",\n",
    ")\n",
    "\n",
    "print(f\"Exported prompts to: {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this quickstart, we covered:\n",
    "\n",
    "1. **Loading a benchmark** - Using the AIME 2025 benchmark with 30 math problems\n",
    "2. **OptimizationTarget** - The three types of text components that can be optimized\n",
    "3. **OptimizationConfig** - Configuration for GEPA optimization runs\n",
    "4. **Benchmark splitting** - Creating train/val/test sets for optimization\n",
    "5. **Running verification** - Establishing a baseline score\n",
    "6. **OptimizationTracker** - Persisting optimization runs for reproducibility\n",
    "7. **Exporting results** - Saving optimized prompts as presets\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [02_configuration.ipynb](02_configuration.ipynb) - Deep dive into OptimizationConfig\n",
    "- [03_data_splitting.ipynb](03_data_splitting.ipynb) - Advanced splitting strategies\n",
    "- [04_scoring_deep_dive.ipynb](04_scoring_deep_dive.ipynb) - Understanding score computation\n",
    "- [05_karenina_adapter.ipynb](05_karenina_adapter.ipynb) - Using the KareninaAdapter\n",
    "- [09_full_optimization_workflow.ipynb](09_full_optimization_workflow.ipynb) - Complete end-to-end example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
