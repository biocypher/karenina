{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KareninaAdapter: GEPA Integration\n",
    "\n",
    "The `KareninaAdapter` is the core integration class that bridges GEPA's optimization framework with Karenina's verification pipeline. This notebook covers:\n",
    "\n",
    "1. Adapter initialization and configuration\n",
    "2. The `evaluate()` method for scoring candidates\n",
    "3. Capturing execution trajectories\n",
    "4. Building reflective datasets for GEPA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.schemas import VerificationConfig, ModelConfig\n",
    "from karenina.integrations.gepa import (\n",
    "    GEPA_AVAILABLE,\n",
    "    OptimizationTarget,\n",
    "    split_benchmark,\n",
    ")\n",
    "\n",
    "# Check if GEPA is available\n",
    "print(f\"GEPA available: {GEPA_AVAILABLE}\")\n",
    "\n",
    "if GEPA_AVAILABLE:\n",
    "    from karenina.integrations.gepa import KareninaAdapter\n",
    "else:\n",
    "    print(\"Note: GEPA not installed. Install with: pip install gepa\")\n",
    "    print(\"This notebook will show the API but won't execute GEPA-specific code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark\n",
    "benchmark_path = Path.home() / \"Projects/karenina-monorepo/local_data/data/checkpoints/aime_2025.jsonld\"\n",
    "benchmark = Benchmark.load(benchmark_path)\n",
    "\n",
    "# Create a data split\n",
    "split = split_benchmark(\n",
    "    benchmark,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Loaded: {benchmark.name}\")\n",
    "print(split.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating a VerificationConfig\n",
    "\n",
    "The adapter needs a base `VerificationConfig` that it will modify with optimized components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base verification config\n",
    "base_config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            # This prompt will be replaced by GEPA during optimization\n",
    "            system_prompt=\"You are a helpful assistant.\",\n",
    "        ),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku-parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "print(\"Base config created\")\n",
    "print(f\"  Answering model: {base_config.answering_models[0].model_name}\")\n",
    "print(f\"  Parsing model: {base_config.parsing_models[0].model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Initializing KareninaAdapter\n",
    "\n",
    "The adapter connects GEPA to Karenina's verification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE:\n",
    "    # Create the adapter\n",
    "    adapter = KareninaAdapter(\n",
    "        benchmark=benchmark,\n",
    "        base_config=base_config,\n",
    "        targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "        template_weight=1.0,  # AIME: correctness only\n",
    "        rubric_weight=0.0,\n",
    "        feedback_model_config=None,  # Optional: for LLM feedback\n",
    "        enable_differential_analysis=True,\n",
    "    )\n",
    "    \n",
    "    print(\"KareninaAdapter initialized:\")\n",
    "    print(f\"  Targets: {adapter.targets}\")\n",
    "    print(f\"  Template weight: {adapter.template_weight}\")\n",
    "    print(f\"  Rubric weight: {adapter.rubric_weight}\")\n",
    "else:\n",
    "    print(\"Skipping adapter creation (GEPA not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapter Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `benchmark` | Karenina Benchmark with questions |\n",
    "| `base_config` | VerificationConfig to modify |\n",
    "| `targets` | Which components to optimize |\n",
    "| `template_weight` | Weight for correctness (0-1) |\n",
    "| `rubric_weight` | Weight for quality (0-1) |\n",
    "| `feedback_model_config` | Optional ModelConfig for LLM feedback |\n",
    "| `enable_differential_analysis` | Compare success vs failure traces |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The evaluate() Method\n",
    "\n",
    "The core GEPA interface method. Evaluates a candidate (optimized prompts) on a batch of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE:\n",
    "    # Define a candidate (the prompts to evaluate)\n",
    "    candidate = {\n",
    "        \"answering_system_prompt\": \"\"\"\n",
    "You are an expert competition mathematician solving AIME problems.\n",
    "\n",
    "IMPORTANT:\n",
    "- AIME answers are ALWAYS integers from 0 to 999\n",
    "- Show your complete step-by-step reasoning\n",
    "- Verify your answer before submitting\n",
    "- State your final answer clearly at the end\n",
    "\"\"\".strip()\n",
    "    }\n",
    "    \n",
    "    print(\"Candidate prompt:\")\n",
    "    print(candidate[\"answering_system_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE:\n",
    "    # Evaluate on a small batch (first 3 training questions)\n",
    "    batch = split.train[:3]\n",
    "    \n",
    "    print(f\"Evaluating on {len(batch)} questions...\")\n",
    "    \n",
    "    eval_result = adapter.evaluate(\n",
    "        batch=batch,\n",
    "        candidate=candidate,\n",
    "        capture_traces=False,  # Don't capture trajectories yet\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEvaluation results:\")\n",
    "    print(f\"  Outputs: {len(eval_result.outputs)} results\")\n",
    "    print(f\"  Scores: {eval_result.scores}\")\n",
    "    print(f\"  Average score: {sum(eval_result.scores) / len(eval_result.scores):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding EvaluationBatch\n",
    "\n",
    "The `evaluate()` method returns an `EvaluationBatch` with:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `outputs` | list[dict] | VerificationResults per question (keyed by model) |\n",
    "| `scores` | list[float] | Score per question (0.0-1.0) |\n",
    "| `trajectories` | list[KareninaTrajectory] | Execution traces (if capture_traces=True) |\n",
    "| `objective_scores` | list[dict] | Per-model scores for Pareto optimization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE:\n",
    "    # Inspect the outputs\n",
    "    for i, (output, score) in enumerate(zip(eval_result.outputs, eval_result.scores)):\n",
    "        print(f\"\\nQuestion {i+1}:\")\n",
    "        for model_name, result in output.items():\n",
    "            passed = result.template.verify_result if result.template else False\n",
    "            status = \"PASS\" if passed else \"FAIL\"\n",
    "            print(f\"  {model_name}: {status} (score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capturing Trajectories\n",
    "\n",
    "Trajectories are detailed execution traces used for GEPA's reflective feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE:\n",
    "    # Evaluate with trajectory capture\n",
    "    eval_with_traces = adapter.evaluate(\n",
    "        batch=split.train[:2],  # Just 2 questions\n",
    "        candidate=candidate,\n",
    "        capture_traces=True,  # Capture trajectories\n",
    "    )\n",
    "    \n",
    "    print(f\"Captured {len(eval_with_traces.trajectories)} trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE and eval_with_traces.trajectories:\n",
    "    # Inspect a trajectory\n",
    "    traj = eval_with_traces.trajectories[0]\n",
    "    \n",
    "    print(\"KareninaTrajectory fields:\")\n",
    "    print(f\"  Question: {traj.data_inst.question_text[:60]}...\")\n",
    "    print(f\"  Model: {traj.model_name}\")\n",
    "    print(f\"  Score: {traj.score:.2f}\")\n",
    "    print(f\"  Passed: {traj.passed()}\")\n",
    "    print(f\"  Raw response: {traj.raw_llm_response[:100] if traj.raw_llm_response else 'None'}...\")\n",
    "    print(f\"  Parsing error: {traj.parsing_error}\")\n",
    "    print(f\"  Failed fields: {traj.failed_fields}\")\n",
    "    print(f\"  Rubric scores: {traj.rubric_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE and eval_with_traces.trajectories:\n",
    "    # Convert trajectory to feedback dict (for GEPA)\n",
    "    feedback_dict = traj.to_feedback_dict()\n",
    "    \n",
    "    print(\"Feedback dict for GEPA:\")\n",
    "    for key, value in feedback_dict.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-Model Evaluation\n",
    "\n",
    "For multi-model benchmarks, the adapter evaluates all models and provides per-model objective scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config with multiple answering models\n",
    "multi_model_config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are a helpful assistant.\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"sonnet\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-sonnet-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are a helpful assistant.\",\n",
    "        ),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "print(\"Multi-model config:\")\n",
    "for model in multi_model_config.answering_models:\n",
    "    print(f\"  - {model.id}: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE:\n",
    "    # Create multi-model adapter\n",
    "    multi_adapter = KareninaAdapter(\n",
    "        benchmark=benchmark,\n",
    "        base_config=multi_model_config,\n",
    "        targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "        template_weight=1.0,\n",
    "        rubric_weight=0.0,\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    multi_eval = multi_adapter.evaluate(\n",
    "        batch=split.train[:2],\n",
    "        candidate=candidate,\n",
    "        capture_traces=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Multi-model evaluation:\")\n",
    "    print(f\"  Total trajectories: {len(multi_eval.trajectories)}\")\n",
    "    print(f\"  Objective scores: {multi_eval.objective_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building Reflective Datasets\n",
    "\n",
    "The `make_reflective_dataset()` method builds feedback for GEPA's reflection LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE:\n",
    "    # Build reflective dataset from evaluation\n",
    "    reflective_data = adapter.make_reflective_dataset(\n",
    "        candidate=candidate,\n",
    "        eval_batch=eval_with_traces,\n",
    "        components_to_update=[\"answering_system_prompt\"],\n",
    "    )\n",
    "    \n",
    "    print(f\"Reflective dataset:\")\n",
    "    for component, examples in reflective_data.items():\n",
    "        print(f\"\\n  Component: {component}\")\n",
    "        print(f\"  Examples: {len(examples)}\")\n",
    "        \n",
    "        if examples:\n",
    "            ex = examples[0]\n",
    "            print(f\"\\n  Sample example:\")\n",
    "            print(f\"    Inputs: {list(ex.get('Inputs', {}).keys())}\")\n",
    "            print(f\"    Generated Outputs: {ex.get('Generated Outputs', '')[:80]}...\")\n",
    "            print(f\"    Feedback: {ex.get('Feedback', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## With LLM Feedback Generator\n",
    "\n",
    "For richer feedback, configure an LLM feedback generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEPA_AVAILABLE:\n",
    "    # Create adapter with LLM feedback\n",
    "    feedback_model = ModelConfig(\n",
    "        id=\"feedback-haiku\",\n",
    "        model_provider=\"anthropic\",\n",
    "        model_name=\"claude-haiku-4-5\",\n",
    "        temperature=0.7,\n",
    "        interface=\"langchain\",\n",
    "    )\n",
    "    \n",
    "    adapter_with_feedback = KareninaAdapter(\n",
    "        benchmark=benchmark,\n",
    "        base_config=base_config,\n",
    "        targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "        template_weight=1.0,\n",
    "        rubric_weight=0.0,\n",
    "        feedback_model_config=feedback_model,\n",
    "        enable_differential_analysis=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Adapter with LLM feedback:\")\n",
    "    print(f\"  Feedback generator: {adapter_with_feedback.feedback_generator is not None}\")\n",
    "    print(f\"  Differential analysis: {adapter_with_feedback.enable_differential_analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Integration with GEPA\n",
    "\n",
    "Here's how the adapter is used in a full GEPA optimization loop (conceptual):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual GEPA optimization loop\n",
    "print(\"\"\"\n",
    "# Full GEPA integration (conceptual)\n",
    "\n",
    "from gepa import GEPA\n",
    "\n",
    "# 1. Create adapter\n",
    "adapter = KareninaAdapter(\n",
    "    benchmark=benchmark,\n",
    "    base_config=verification_config,\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    ")\n",
    "\n",
    "# 2. Define seed candidate\n",
    "seed_candidate = {\n",
    "    \"answering_system_prompt\": \"You are a math expert.\"\n",
    "}\n",
    "\n",
    "# 3. Run GEPA optimization\n",
    "result = GEPA.optimize(\n",
    "    seed_candidate=seed_candidate,\n",
    "    trainset=split.train,\n",
    "    valset=split.val,\n",
    "    adapter=adapter,\n",
    "    max_metric_calls=100,\n",
    "    reflection_model=\"anthropic/claude-haiku-4-5\",\n",
    ")\n",
    "\n",
    "# 4. Get optimized prompts\n",
    "optimized = result.best_candidate\n",
    "print(f\"Optimized prompt: {optimized['answering_system_prompt']}\")\n",
    "print(f\"Improvement: {result.improvement:.2%}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|--------|\n",
    "| `__init__()` | Initialize adapter with benchmark and config |\n",
    "| `evaluate()` | Score a candidate on a batch of questions |\n",
    "| `make_reflective_dataset()` | Build feedback for GEPA reflection |\n",
    "\n",
    "| EvaluationBatch Field | Description |\n",
    "|----------------------|-------------|\n",
    "| `outputs` | VerificationResults per question |\n",
    "| `scores` | Score per question (0.0-1.0) |\n",
    "| `trajectories` | Execution traces for reflection |\n",
    "| `objective_scores` | Per-model scores for Pareto |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [06_feedback_generation.ipynb](06_feedback_generation.ipynb) - LLM-powered feedback generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
