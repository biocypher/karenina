{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Run Tracking\n",
    "\n",
    "This notebook covers the `OptimizationTracker` and `OptimizationRun` classes for persisting and analyzing optimization experiments. Learn how to:\n",
    "\n",
    "1. Create and configure an OptimizationTracker\n",
    "2. Log optimization runs\n",
    "3. Query and compare runs\n",
    "4. Export optimization history\n",
    "5. Analyze improvement trends\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "from karenina.integrations.gepa import (\n",
    "    OptimizationTracker,\n",
    "    OptimizationRun,\n",
    "    OptimizationTarget,\n",
    ")\n",
    "\n",
    "# Create a temporary directory for the database\n",
    "temp_dir = Path(tempfile.mkdtemp(prefix=\"gepa_tracking_\"))\n",
    "print(f\"Using temp directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating an OptimizationTracker\n",
    "\n",
    "The tracker uses SQLite for persistent storage of optimization runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tracker\n",
    "db_path = temp_dir / \"optimization_history.db\"\n",
    "tracker = OptimizationTracker(db_path)\n",
    "\n",
    "print(f\"Tracker created\")\n",
    "print(f\"  Database path: {tracker.storage_path}\")\n",
    "print(f\"  Database exists: {tracker.storage_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Usage\n",
    "\n",
    "For production, use a persistent location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended: Use ~/.karenina for persistent storage\n",
    "print(\"\"\"\n",
    "# Production tracker\n",
    "tracker = OptimizationTracker(\"~/.karenina/optimization_history.db\")\n",
    "\n",
    "# Or project-specific\n",
    "tracker = OptimizationTracker(\"./experiments/gepa_runs.db\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## OptimizationRun: Run Records\n",
    "\n",
    "Each optimization run is recorded as an `OptimizationRun` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample optimization run\n",
    "run1 = OptimizationRun(\n",
    "    benchmark_name=\"AIME 2025\",\n",
    "    \n",
    "    # What was optimized\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT.value],\n",
    "    \n",
    "    # Initial prompts\n",
    "    seed_prompts={\n",
    "        \"answering_system_prompt\": \"You are a helpful math assistant.\"\n",
    "    },\n",
    "    \n",
    "    # Optimized prompts (result)\n",
    "    optimized_prompts={\n",
    "        \"answering_system_prompt\": \"\"\"You are an expert competition mathematician.\n",
    "AIME answers are integers 0-999. Show your reasoning.\"\"\"\n",
    "    },\n",
    "    \n",
    "    # Scores\n",
    "    train_score=0.75,\n",
    "    val_score=0.70,\n",
    "    test_score=0.68,\n",
    "    improvement=0.166,  # 16.6% improvement\n",
    "    \n",
    "    # GEPA parameters\n",
    "    reflection_model=\"anthropic/claude-haiku-4-5\",\n",
    "    metric_calls=75,\n",
    "    \n",
    "    # Trajectory info\n",
    "    best_generation=8,\n",
    "    total_generations=10,\n",
    ")\n",
    "\n",
    "print(\"OptimizationRun created:\")\n",
    "print(f\"  Run ID: {run1.run_id}\")\n",
    "print(f\"  Benchmark: {run1.benchmark_name}\")\n",
    "print(f\"  Val Score: {run1.val_score:.2%}\")\n",
    "print(f\"  Improvement: {run1.improvement:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Fields\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `run_id` | str | Unique identifier (auto-generated) |\n",
    "| `timestamp` | datetime | When the run occurred |\n",
    "| `benchmark_name` | str | Name of the benchmark |\n",
    "| `targets` | list[str] | What was optimized |\n",
    "| `seed_prompts` | dict | Initial prompts |\n",
    "| `optimized_prompts` | dict | Final optimized prompts |\n",
    "| `train_score` | float | Training set score |\n",
    "| `val_score` | float | Validation set score |\n",
    "| `test_score` | float | Test set score (optional) |\n",
    "| `improvement` | float | Relative improvement |\n",
    "| `reflection_model` | str | GEPA reflection model |\n",
    "| `metric_calls` | int | Number of evaluations |\n",
    "| `best_generation` | int | Best generation number |\n",
    "| `total_generations` | int | Total generations run |\n",
    "| `model_scores` | dict | Per-model scores (for Pareto) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Logging Runs\n",
    "\n",
    "Use `log_run()` to persist optimization runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the run\n",
    "run_id = tracker.log_run(run1)\n",
    "print(f\"Logged run: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and log more runs for demonstration\n",
    "runs_to_log = [\n",
    "    OptimizationRun(\n",
    "        benchmark_name=\"AIME 2025\",\n",
    "        targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT.value],\n",
    "        seed_prompts={\"answering_system_prompt\": \"Solve math problems.\"},\n",
    "        optimized_prompts={\"answering_system_prompt\": \"You are an AIME expert.\"},\n",
    "        train_score=0.70,\n",
    "        val_score=0.65,\n",
    "        improvement=0.083,\n",
    "        reflection_model=\"anthropic/claude-haiku-4-5\",\n",
    "        metric_calls=50,\n",
    "        best_generation=5,\n",
    "        total_generations=8,\n",
    "        timestamp=datetime.now() - timedelta(hours=2),\n",
    "    ),\n",
    "    OptimizationRun(\n",
    "        benchmark_name=\"AIME 2025\",\n",
    "        targets=[\n",
    "            OptimizationTarget.ANSWERING_SYSTEM_PROMPT.value,\n",
    "            OptimizationTarget.PARSING_INSTRUCTIONS.value,\n",
    "        ],\n",
    "        seed_prompts={\n",
    "            \"answering_system_prompt\": \"You are a math tutor.\",\n",
    "            \"parsing_instructions\": \"Extract the answer.\",\n",
    "        },\n",
    "        optimized_prompts={\n",
    "            \"answering_system_prompt\": \"You are an AIME competition solver.\",\n",
    "            \"parsing_instructions\": \"Find the integer 0-999 in the response.\",\n",
    "        },\n",
    "        train_score=0.80,\n",
    "        val_score=0.75,\n",
    "        test_score=0.72,\n",
    "        improvement=0.25,\n",
    "        reflection_model=\"anthropic/claude-sonnet-4-5\",\n",
    "        metric_calls=100,\n",
    "        best_generation=12,\n",
    "        total_generations=15,\n",
    "        timestamp=datetime.now() - timedelta(hours=1),\n",
    "    ),\n",
    "    OptimizationRun(\n",
    "        benchmark_name=\"Math Benchmark\",  # Different benchmark\n",
    "        targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT.value],\n",
    "        seed_prompts={\"answering_system_prompt\": \"Calculate.\"},\n",
    "        optimized_prompts={\"answering_system_prompt\": \"Solve step by step.\"},\n",
    "        train_score=0.85,\n",
    "        val_score=0.82,\n",
    "        improvement=0.37,\n",
    "        reflection_model=\"anthropic/claude-haiku-4-5\",\n",
    "        metric_calls=60,\n",
    "        best_generation=7,\n",
    "        total_generations=10,\n",
    "    ),\n",
    "]\n",
    "\n",
    "for run in runs_to_log:\n",
    "    run_id = tracker.log_run(run)\n",
    "    print(f\"Logged: {run_id} ({run.benchmark_name}, val_score={run.val_score:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Retrieving Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Specific Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get run by ID\n",
    "retrieved = tracker.get_run(run1.run_id)\n",
    "\n",
    "if retrieved:\n",
    "    print(f\"Retrieved run:\")\n",
    "    print(f\"  ID: {retrieved.run_id}\")\n",
    "    print(f\"  Benchmark: {retrieved.benchmark_name}\")\n",
    "    print(f\"  Val Score: {retrieved.val_score:.2%}\")\n",
    "    print(f\"  Timestamp: {retrieved.timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Best Run for a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best by validation score\n",
    "best_by_val = tracker.get_best_run(\"AIME 2025\", metric=\"val_score\")\n",
    "\n",
    "if best_by_val:\n",
    "    print(f\"Best AIME 2025 run (by val_score):\")\n",
    "    print(f\"  ID: {best_by_val.run_id}\")\n",
    "    print(f\"  Val Score: {best_by_val.val_score:.2%}\")\n",
    "    print(f\"  Improvement: {best_by_val.improvement:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best by improvement\n",
    "best_by_imp = tracker.get_best_run(\"AIME 2025\", metric=\"improvement\")\n",
    "\n",
    "if best_by_imp:\n",
    "    print(f\"Best AIME 2025 run (by improvement):\")\n",
    "    print(f\"  ID: {best_by_imp.run_id}\")\n",
    "    print(f\"  Val Score: {best_by_imp.val_score:.2%}\")\n",
    "    print(f\"  Improvement: {best_by_imp.improvement:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all runs\n",
    "all_runs = tracker.list_runs()\n",
    "\n",
    "print(f\"All runs ({len(all_runs)}):\")\n",
    "for run in all_runs:\n",
    "    print(f\"  {run.run_id}: {run.benchmark_name} | val={run.val_score:.2%} | imp={run.improvement:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List runs for specific benchmark\n",
    "aime_runs = tracker.list_runs(benchmark_name=\"AIME 2025\")\n",
    "\n",
    "print(f\"AIME 2025 runs ({len(aime_runs)}):\")\n",
    "for run in aime_runs:\n",
    "    print(f\"  {run.run_id}: val={run.val_score:.2%} | imp={run.improvement:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagination\n",
    "first_two = tracker.list_runs(limit=2, offset=0)\n",
    "next_two = tracker.list_runs(limit=2, offset=2)\n",
    "\n",
    "print(f\"First 2: {[r.run_id for r in first_two]}\")\n",
    "print(f\"Next 2: {[r.run_id for r in next_two]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare specific runs\n",
    "run_ids = [r.run_id for r in aime_runs[:3]]\n",
    "comparison = tracker.compare_runs(run_ids)\n",
    "\n",
    "print(\"Run Comparison:\")\n",
    "print(f\"  Runs compared: {len(comparison['runs'])}\")\n",
    "print(f\"  Best by val_score: {comparison['best']['val_score']}\")\n",
    "print(f\"  Best by improvement: {comparison['best']['improvement']}\")\n",
    "\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Val scores: {comparison['metrics']['val_score']}\")\n",
    "print(f\"  Improvements: {comparison['metrics']['improvement']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Improvement Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get improvement trend for a benchmark\n",
    "trend = tracker.get_improvement_trend(\"AIME 2025\", limit=10)\n",
    "\n",
    "print(\"AIME 2025 Improvement Trend:\")\n",
    "for entry in trend:\n",
    "    print(f\"  {entry['timestamp'][:16]}: val={entry['val_score']:.2%}, imp={entry['improvement']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exporting History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as JSON\n",
    "json_export = tracker.export_history(format=\"json\", benchmark_name=\"AIME 2025\")\n",
    "\n",
    "print(\"JSON Export (first 500 chars):\")\n",
    "print(json_export[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as CSV\n",
    "csv_export = tracker.export_history(format=\"csv\")\n",
    "\n",
    "print(\"CSV Export:\")\n",
    "for line in csv_export.split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deleting Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a run\n",
    "if all_runs:\n",
    "    run_to_delete = all_runs[-1].run_id\n",
    "    deleted = tracker.delete_run(run_to_delete)\n",
    "    print(f\"Deleted run {run_to_delete}: {deleted}\")\n",
    "    \n",
    "    # Verify deletion\n",
    "    remaining = tracker.list_runs()\n",
    "    print(f\"Remaining runs: {len(remaining)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temp directory\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "print(f\"Cleaned up: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|--------|\n",
    "| `log_run()` | Persist an optimization run |\n",
    "| `get_run()` | Retrieve a specific run |\n",
    "| `get_best_run()` | Get best run for a benchmark |\n",
    "| `list_runs()` | List runs with filtering |\n",
    "| `compare_runs()` | Compare multiple runs |\n",
    "| `get_improvement_trend()` | Analyze trends over time |\n",
    "| `export_history()` | Export as JSON/CSV |\n",
    "| `delete_run()` | Remove a run |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [08_export_and_reuse.ipynb](08_export_and_reuse.ipynb) - Export optimized prompts for reuse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
