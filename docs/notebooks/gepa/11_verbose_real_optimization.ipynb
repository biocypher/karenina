{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Verbose Real GEPA Optimization on AIME\n\nThis notebook demonstrates a **real GEPA optimization loop** with:\n\n- **Verbose logging** for tracking optimization progress\n- **LLM-generated feedback** for richer diagnostic information\n- **Differential analysis** comparing successful vs failed responses\n- **Multi-model evaluation** using both Haiku and Sonnet\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook requires:\n",
    "- GEPA package installed: `pip install gepa`\n",
    "- API keys configured for your LLM providers (Anthropic, OpenAI, etc.)\n",
    "- The AIME 2025 benchmark file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nsys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n\n# Core Karenina imports\nfrom karenina import Benchmark\n\n# GEPA integration imports\nfrom karenina.integrations.gepa import (\n    GEPA_AVAILABLE,\n    KareninaAdapter,\n    ObjectiveConfig,\n    OptimizationTarget,\n    SimpleLogger,\n    split_benchmark,\n)\nfrom karenina.schemas import ModelConfig, VerificationConfig\n\nprint(f\"GEPA available: {GEPA_AVAILABLE}\")\n\nif not GEPA_AVAILABLE:\n    raise ImportError(\"GEPA is required for this notebook. Install with: pip install gepa\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load the AIME Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: AIME 2025\n",
      "Description: American Invitational Mathematics Examination 2025 - 30 problems from AIME I and AIME II. All answers are integers from 0 to 999.\n",
      "Total questions: 30\n"
     ]
    }
   ],
   "source": [
    "# Load the AIME 2025 benchmark\n",
    "benchmark_path = Path.home() / \"Projects/karenina-monorepo/local_data/data/checkpoints/aime_2025.jsonld\"\n",
    "benchmark = Benchmark.load(benchmark_path)\n",
    "\n",
    "print(f\"Benchmark: {benchmark.name}\")\n",
    "print(f\"Description: {benchmark.description}\")\n",
    "print(f\"Total questions: {len(benchmark.get_question_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample AIME problem:\n",
      "  ID: urn:uuid:question-find-the-sum-of-all-integer-bases-$b>9$-for-which--bb2069b5\n",
      "  Question: Find the sum of all integer bases $b>9$ for which $17_{b}$ is a divisor of $97_{b}$....\n",
      "  Answer: 70\n"
     ]
    }
   ],
   "source": [
    "# Explore a sample question\n",
    "question_ids = benchmark.get_question_ids()\n",
    "sample_q = benchmark.get_question(question_ids[0])\n",
    "\n",
    "print(\"Sample AIME problem:\")\n",
    "print(f\"  ID: {question_ids[0]}\")\n",
    "print(f\"  Question: {sample_q['question'][:150]}...\")\n",
    "print(f\"  Answer: {sample_q['raw_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Configure the Optimization\n",
    "\n",
    "We'll configure the optimization parameters including the data split and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Define the seed prompt (starting point for optimization)\nSEED_PROMPT = \"\"\"You are a helpful math assistant.\nSolve the problem step by step and provide the final answer.\"\"\"\n\n# Optimization parameters\nMAX_METRIC_CALLS = 100  # Total evaluation budget; production would use 500-1000\nTRAIN_RATIO = 0.8\nVAL_RATIO = 0.2\nSPLIT_SEED = 42\n\n# Model configuration\nREFLECTION_MODEL = \"anthropic/claude-sonnet-4-5\"  # Sonnet for reflection\nANSWERING_MODELS = [\n    (\"claude-haiku\", \"claude-haiku-4-5\"),\n    (\"claude-sonnet\", \"claude-sonnet-4-5\"),\n]\nPARSING_MODEL = \"claude-haiku-4-5\"  # Haiku for parsing (fast)\n\nprint(\"Optimization Configuration:\")\nprint(f\"  Seed prompt: {SEED_PROMPT[:50]}...\")\nprint(f\"  Max metric calls: {MAX_METRIC_CALLS}\")\nprint(f\"  Reflection model: {REFLECTION_MODEL}\")\nprint(f\"  Answering models: {[m[1] for m in ANSWERING_MODELS]}\")\nprint(f\"  Split: {TRAIN_RATIO:.0%} train, {VAL_RATIO:.0%} val\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Split the Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split:\n",
      "  Train: 24 questions, Val: 6 questions, Seed: 42\n",
      "\n",
      "  Train: 24 questions (for optimization feedback)\n",
      "  Val: 6 questions (for candidate selection)\n"
     ]
    }
   ],
   "source": [
    "# Split the benchmark into train/val sets\n",
    "split = split_benchmark(\n",
    "    benchmark,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    seed=SPLIT_SEED,\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"  {split.summary()}\")\n",
    "print(f\"\\n  Train: {len(split.train)} questions (for optimization feedback)\")\n",
    "print(f\"  Val: {len(split.val)} questions (for candidate selection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Create Verification Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Config:\n",
      "  Answering models:\n",
      "    - claude-haiku: claude-haiku-4-5\n",
      "    - claude-sonnet: claude-sonnet-4-5\n",
      "  Parsing model: claude-haiku-4-5\n",
      "  Evaluation mode: template_only\n"
     ]
    }
   ],
   "source": [
    "# Create verification config with multiple answering models\n",
    "verification_config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=model_id,\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=model_name,\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=SEED_PROMPT,\n",
    "        )\n",
    "        for model_id, model_name in ANSWERING_MODELS\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=PARSING_MODEL,\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",  # AIME uses template-based correctness\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "print(\"Verification Config:\")\n",
    "print(f\"  Answering models:\")\n",
    "for model in verification_config.answering_models:\n",
    "    print(f\"    - {model.id}: {model.model_name}\")\n",
    "print(f\"  Parsing model: {PARSING_MODEL}\")\n",
    "print(f\"  Evaluation mode: {verification_config.evaluation_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Create the Karenina Adapter with LLM Feedback\n",
    "\n",
    "The `KareninaAdapter` supports **LLM-generated feedback** which provides richer diagnostic information for GEPA's reflection process. When a model fails on a question, the LLM feedback generator:\n",
    "\n",
    "1. Analyzes why the response failed\n",
    "2. Compares with successful responses (if any)\n",
    "3. Generates actionable suggestions for prompt improvement\n",
    "\n",
    "This is more effective than simple programmatic feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback Model Config:\n",
      "  Model: claude-sonnet-4-5\n",
      "  Provider: anthropic\n"
     ]
    }
   ],
   "source": [
    "# Configure feedback model for LLM-generated feedback (using Sonnet for quality)\n",
    "feedback_model_config = ModelConfig(\n",
    "    id=\"feedback-model\",\n",
    "    model_provider=\"anthropic\",\n",
    "    model_name=\"claude-sonnet-4-5\",  # Sonnet for quality feedback\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\",\n",
    ")\n",
    "\n",
    "print(\"Feedback Model Config:\")\n",
    "print(f\"  Model: {feedback_model_config.model_name}\")\n",
    "print(f\"  Provider: {feedback_model_config.model_provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rsk0z8n52te",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KareninaAdapter created with LLM feedback enabled\n",
      "  Targets: ['answering_system_prompt']\n",
      "  LLM Feedback: True\n",
      "  Differential Analysis: True\n"
     ]
    }
   ],
   "source": [
    "# Create adapter for GEPA optimization with LLM feedback\n",
    "adapter = KareninaAdapter(\n",
    "    benchmark=benchmark,\n",
    "    base_config=verification_config,\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    objective_config=ObjectiveConfig(\n",
    "        include_template=True,  # Optimize for template correctness\n",
    "        include_rubric=False,   # AIME doesn't use rubrics\n",
    "    ),\n",
    "    # Enable LLM-generated feedback for richer diagnostics\n",
    "    feedback_model_config=feedback_model_config,\n",
    "    # Enable differential analysis: compare successful vs failed responses\n",
    "    enable_differential_analysis=True,\n",
    ")\n",
    "\n",
    "print(\"KareninaAdapter created with LLM feedback enabled\")\n",
    "print(f\"  Targets: {[t.value for t in adapter.targets]}\")\n",
    "print(f\"  LLM Feedback: {adapter.feedback_generator is not None}\")\n",
    "print(f\"  Differential Analysis: {adapter.enable_differential_analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "---\n\n## Step 7: Run GEPA Optimization\n\nNow we run the actual optimization with a SimpleLogger for progress tracking."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Prepare seed candidate\nseed_candidate = {\n    \"answering_system_prompt\": SEED_PROMPT,\n}\n\nprint(\"Starting GEPA Optimization...\")\nprint(\"=\" * 60)\nprint(f\"Seed prompt: {SEED_PROMPT[:60]}...\")\nprint(f\"Max metric calls: {MAX_METRIC_CALLS}\")\nprint(f\"Train set: {len(split.train)} questions\")\nprint(f\"Val set: {len(split.val)} questions\")\nprint(\"=\" * 60)\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "import gepa\n\n# Create simple logger for progress tracking\nlogger = SimpleLogger(show_all=False)\n\n# Run GEPA optimization\nresult = gepa.optimize(\n    seed_candidate=seed_candidate,\n    trainset=split.train,\n    valset=split.val,\n    adapter=adapter,\n    reflection_lm=REFLECTION_MODEL,\n    max_metric_calls=MAX_METRIC_CALLS,\n    frontier_type=\"objective\",\n    logger=logger,\n    display_progress_bar=False,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "---\n\n## Step 8: Analyze the Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 60)\nprint(\"Results\")\nprint(\"=\" * 60)\n\n# Extract results\nbest_candidate = result.best_candidate\nval_scores = result.val_aggregate_scores\n\nprint(f\"Candidates evaluated: {len(val_scores)}\")\nprint(f\"Best candidate: {result.best_idx}\")\n\nif val_scores:\n    baseline_score = val_scores[0]\n    best_score = val_scores[result.best_idx]\n    improvement = (best_score - baseline_score) / baseline_score if baseline_score > 0 else 0\n    print(f\"Baseline: {baseline_score:.2%} â†’ Best: {best_score:.2%} ({improvement:+.1%})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# Show the optimized prompt\noptimized_prompt = best_candidate.get(\"answering_system_prompt\", \"\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Optimized Prompt:\")\nprint(\"=\" * 60)\nprint(optimized_prompt)\n\n# Save to file\noutput_file = Path.cwd() / \"optimized_prompt.txt\"\noutput_file.write_text(optimized_prompt)\nprint(f\"\\nSaved to: {output_file}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}