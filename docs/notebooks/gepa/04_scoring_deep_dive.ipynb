{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Deep Dive\n",
    "\n",
    "This notebook explains how GEPA computes scores from Karenina's verification results. Understanding scoring is crucial for:\n",
    "\n",
    "1. Interpreting optimization progress\n",
    "2. Balancing correctness vs quality\n",
    "3. Multi-model optimization with Pareto frontiers\n",
    "4. Diagnosing template/rubric failures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.integrations.gepa import (\n",
    "    compute_improvement,\n",
    "    compute_multi_model_score,\n",
    "    compute_single_score,\n",
    "    compute_weighted_score,\n",
    "    extract_failed_fields,\n",
    ")\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Load benchmark\n",
    "benchmark_path = Path.home() / \"Projects/karenina-monorepo/local_data/data/checkpoints/aime_2025.jsonld\"\n",
    "benchmark = Benchmark.load(benchmark_path)\n",
    "print(f\"Loaded: {benchmark.name} ({len(benchmark.get_question_ids())} questions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run Verification to Get Results\n",
    "\n",
    "First, let's run verification to get some real results to score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure verification with Claude Haiku\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are a math expert. Solve the problem and provide the final integer answer (0-999).\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku-parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "# Run on a small subset\n",
    "question_ids = benchmark.get_question_ids()[:5]\n",
    "print(f\"Running verification on {len(question_ids)} questions...\")\n",
    "\n",
    "results = benchmark.run_verification(config, question_ids=question_ids)\n",
    "print(f\"Got {len(results.results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## compute_single_score(): Scoring One Result\n",
    "\n",
    "The core scoring function that converts a `VerificationResult` to a float score (0.0-1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score individual results\n",
    "for result in results.results:\n",
    "    # Default weights: 70% template, 30% rubric\n",
    "    score = compute_single_score(result)\n",
    "\n",
    "    # Get verification status\n",
    "    passed = result.template.verify_result if result.template else False\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "\n",
    "    print(f\"{status} | Score: {score:.2f} | Question: {result.metadata.question_text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Score Formula\n",
    "\n",
    "```\n",
    "score = template_weight * template_score + rubric_weight * rubric_score\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **template_score**: Binary (1.0 if verify_result=True, else 0.0)\n",
    "- **rubric_score**: Average of normalized rubric trait scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with different weights\n",
    "result = results.results[0]\n",
    "\n",
    "# Template-only (correctness focused)\n",
    "score_template_only = compute_single_score(result, template_weight=1.0, rubric_weight=0.0)\n",
    "\n",
    "# Rubric-only (quality focused)\n",
    "score_rubric_only = compute_single_score(result, template_weight=0.0, rubric_weight=1.0)\n",
    "\n",
    "# Balanced\n",
    "score_balanced = compute_single_score(result, template_weight=0.5, rubric_weight=0.5)\n",
    "\n",
    "print(f\"Template-only score: {score_template_only:.2f}\")\n",
    "print(f\"Rubric-only score: {score_rubric_only:.2f}\")\n",
    "print(f\"Balanced score: {score_balanced:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## compute_weighted_score(): Aggregating Multiple Results\n",
    "\n",
    "Aggregate scores across multiple questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build results dict (keyed by result index)\n",
    "results_dict = {str(i): r for i, r in enumerate(results.results)}\n",
    "\n",
    "# Compute aggregate score\n",
    "aggregate_score = compute_weighted_score(\n",
    "    results_dict,\n",
    "    template_weight=1.0,  # AIME: correctness only\n",
    "    rubric_weight=0.0,\n",
    ")\n",
    "\n",
    "print(f\"Aggregate score across {len(results_dict)} questions: {aggregate_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation to verify\n",
    "individual_scores = [compute_single_score(r, template_weight=1.0, rubric_weight=0.0) for r in results.results]\n",
    "manual_avg = sum(individual_scores) / len(individual_scores)\n",
    "\n",
    "print(f\"Individual scores: {individual_scores}\")\n",
    "print(f\"Manual average: {manual_avg:.2%}\")\n",
    "print(f\"compute_weighted_score: {aggregate_score:.2%}\")\n",
    "print(f\"Match: {abs(manual_avg - aggregate_score) < 1e-6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## compute_multi_model_score(): Multi-Model Scoring\n",
    "\n",
    "For multi-model benchmarks, compute per-model scores for Pareto optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run verification with two models\n",
    "multi_model_config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"claude-haiku\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are a math expert. Give the final integer answer.\",\n",
    "        ),\n",
    "        ModelConfig(\n",
    "            id=\"claude-sonnet\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-sonnet-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "            system_prompt=\"You are a math expert. Show your work and give the final integer answer.\",\n",
    "        ),\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parser\",\n",
    "            model_provider=\"anthropic\",\n",
    "            model_name=\"claude-haiku-4-5\",\n",
    "            temperature=0.0,\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    evaluation_mode=\"template_only\",\n",
    "    replicate_count=1,\n",
    ")\n",
    "\n",
    "# Run on subset\n",
    "print(\"Running multi-model verification on 3 questions...\")\n",
    "multi_results = benchmark.run_verification(multi_model_config, question_ids=question_ids[:3])\n",
    "print(f\"Got {len(multi_results.results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group results by model\n",
    "from collections import defaultdict\n",
    "\n",
    "results_by_model = defaultdict(list)\n",
    "for result in multi_results.results:\n",
    "    model_name = result.metadata.answering_model or \"unknown\"\n",
    "    results_by_model[model_name].append(result)\n",
    "\n",
    "print(\"Results per model:\")\n",
    "for model, model_results in results_by_model.items():\n",
    "    print(f\"  {model}: {len(model_results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute multi-model score\n",
    "overall_score, model_scores = compute_multi_model_score(\n",
    "    dict(results_by_model),\n",
    "    template_weight=1.0,\n",
    "    rubric_weight=0.0,\n",
    ")\n",
    "\n",
    "print(f\"Overall score: {overall_score:.2%}\")\n",
    "print(\"\\nPer-model scores:\")\n",
    "for model, score in model_scores.items():\n",
    "    print(f\"  {model}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## extract_failed_fields(): Diagnosing Failures\n",
    "\n",
    "Identify which template fields failed verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a failed result\n",
    "failed_results = [r for r in results.results if r.template and not r.template.verify_result]\n",
    "\n",
    "if failed_results:\n",
    "    failed = failed_results[0]\n",
    "\n",
    "    # Extract failed fields\n",
    "    failed_fields = extract_failed_fields(failed)\n",
    "\n",
    "    print(f\"Question: {failed.metadata.question_text[:60]}...\")\n",
    "    print(f\"Expected answer: {failed.metadata.raw_answer}\")\n",
    "    print(f\"Model response: {failed.template.raw_llm_response[:100]}...\")\n",
    "    print(f\"\\nFailed fields: {failed_fields}\")\n",
    "else:\n",
    "    print(\"All results passed! No failures to analyze.\")\n",
    "    print(\"\\nExample of extract_failed_fields() with a passing result:\")\n",
    "    print(f\"Failed fields: {extract_failed_fields(results.results[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## compute_improvement(): Measuring Optimization Progress\n",
    "\n",
    "Compute relative improvement from baseline to optimized score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Baseline vs optimized scores\n",
    "baseline_score = 0.60  # 60% before optimization\n",
    "optimized_score = 0.75  # 75% after optimization\n",
    "\n",
    "improvement = compute_improvement(baseline_score, optimized_score)\n",
    "\n",
    "print(f\"Baseline: {baseline_score:.2%}\")\n",
    "print(f\"Optimized: {optimized_score:.2%}\")\n",
    "print(f\"Improvement: {improvement:.2%} ({improvement * 100:.1f}% relative improvement)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge cases\n",
    "print(\"Edge cases:\")\n",
    "\n",
    "# Improvement from zero baseline\n",
    "imp_from_zero = compute_improvement(0.0, 0.50)\n",
    "print(f\"  From 0% to 50%: {imp_from_zero:.2%} (returns absolute score when baseline is 0)\")\n",
    "\n",
    "# Negative improvement (worse)\n",
    "negative_imp = compute_improvement(0.70, 0.60)\n",
    "print(f\"  From 70% to 60%: {negative_imp:.2%} (negative = got worse)\")\n",
    "\n",
    "# No change\n",
    "no_change = compute_improvement(0.50, 0.50)\n",
    "print(f\"  No change: {no_change:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practical Scoring Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: AIME Benchmark (Correctness Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For AIME, we only care about correctness\n",
    "aime_scores = []\n",
    "for result in results.results:\n",
    "    score = compute_single_score(result, template_weight=1.0, rubric_weight=0.0)\n",
    "    aime_scores.append(score)\n",
    "\n",
    "accuracy = sum(aime_scores) / len(aime_scores)\n",
    "print(f\"AIME Accuracy: {accuracy:.2%} ({sum(aime_scores):.0f}/{len(aime_scores)} correct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Tracking Optimization Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate optimization progress\n",
    "generation_scores = [\n",
    "    0.40,  # Gen 0: Seed prompt\n",
    "    0.45,  # Gen 1\n",
    "    0.52,  # Gen 2\n",
    "    0.58,  # Gen 3\n",
    "    0.55,  # Gen 4 (regression)\n",
    "    0.62,  # Gen 5\n",
    "    0.65,  # Gen 6\n",
    "    0.70,  # Gen 7: Best\n",
    "]\n",
    "\n",
    "baseline = generation_scores[0]\n",
    "best_score = max(generation_scores)\n",
    "best_gen = generation_scores.index(best_score)\n",
    "\n",
    "print(\"Optimization Progress:\")\n",
    "for gen, score in enumerate(generation_scores):\n",
    "    imp = compute_improvement(baseline, score)\n",
    "    marker = \" <- BEST\" if gen == best_gen else \"\"\n",
    "    print(f\"  Gen {gen}: {score:.2%} (improvement: {imp:+.1%}){marker}\")\n",
    "\n",
    "final_improvement = compute_improvement(baseline, best_score)\n",
    "print(f\"\\nTotal improvement: {final_improvement:+.1%} (from {baseline:.2%} to {best_score:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Multi-Model Pareto Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multi-model scores for different prompts\n",
    "candidates = {\n",
    "    \"Prompt A\": {\"haiku\": 0.65, \"sonnet\": 0.70},\n",
    "    \"Prompt B\": {\"haiku\": 0.70, \"sonnet\": 0.60},  # Better for haiku\n",
    "    \"Prompt C\": {\"haiku\": 0.60, \"sonnet\": 0.75},  # Better for sonnet\n",
    "    \"Prompt D\": {\"haiku\": 0.68, \"sonnet\": 0.72},  # Balanced\n",
    "}\n",
    "\n",
    "print(\"Multi-Model Candidate Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for prompt, scores in candidates.items():\n",
    "    avg = sum(scores.values()) / len(scores)\n",
    "    print(f\"{prompt}:\")\n",
    "    for model, score in scores.items():\n",
    "        print(f\"  {model}: {score:.2%}\")\n",
    "    print(f\"  Average: {avg:.2%}\")\n",
    "    print()\n",
    "\n",
    "# Find Pareto-optimal candidates\n",
    "print(\"Pareto Analysis:\")\n",
    "print(\"  - Prompt A: Not Pareto-optimal (dominated by D)\")\n",
    "print(\"  - Prompt B: Pareto-optimal for haiku\")\n",
    "print(\"  - Prompt C: Pareto-optimal for sonnet\")\n",
    "print(\"  - Prompt D: Pareto-optimal (best average)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Function | Purpose | Returns |\n",
    "|----------|---------|--------|\n",
    "| `compute_single_score()` | Score one result | float (0.0-1.0) |\n",
    "| `compute_weighted_score()` | Aggregate multiple results | float (0.0-1.0) |\n",
    "| `compute_multi_model_score()` | Per-model + overall scores | (float, dict) |\n",
    "| `compute_improvement()` | Relative improvement | float (fraction) |\n",
    "| `extract_failed_fields()` | Find failed template fields | list[str] |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Score = weighted combination** of template (correctness) and rubric (quality)\n",
    "2. **Use template_weight=1.0** for factual benchmarks like AIME\n",
    "3. **Multi-model scoring** enables Pareto optimization across models\n",
    "4. **extract_failed_fields()** helps diagnose verification failures\n",
    "5. **compute_improvement()** tracks optimization progress\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [05_karenina_adapter.ipynb](05_karenina_adapter.ipynb) - Using the KareninaAdapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
