{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEPA Configuration Deep Dive\n",
    "\n",
    "This notebook explores the configuration options for GEPA optimization runs in detail. You'll learn how to:\n",
    "\n",
    "1. Configure optimization targets\n",
    "2. Set up seed prompts for each target\n",
    "3. Balance template vs rubric scoring weights\n",
    "4. Configure GEPA algorithm parameters\n",
    "5. Set up data splitting strategies\n",
    "6. Enable LLM-powered feedback generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.schemas import ModelConfig\n",
    "from karenina.integrations.gepa import (\n",
    "    OptimizationConfig,\n",
    "    OptimizationTarget,\n",
    ")\n",
    "\n",
    "# Load benchmark for examples\n",
    "benchmark_path = Path.home() / \"Projects/karenina-monorepo/local_data/data/checkpoints/aime_2025.jsonld\"\n",
    "benchmark = Benchmark.load(benchmark_path)\n",
    "print(f\"Loaded: {benchmark.name} ({len(benchmark.get_question_ids())} questions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## OptimizationTarget\n",
    "\n",
    "GEPA can optimize three types of text components in the verification pipeline:\n",
    "\n",
    "| Target | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `ANSWERING_SYSTEM_PROMPT` | System prompt for the model being evaluated | Improve answer quality |\n",
    "| `PARSING_INSTRUCTIONS` | Instructions for the judge LLM | Improve parsing accuracy |\n",
    "| `MCP_TOOL_DESCRIPTIONS` | Descriptions for MCP tools | Guide tool selection |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All available targets\n",
    "print(\"Available OptimizationTarget values:\\n\")\n",
    "for target in OptimizationTarget:\n",
    "    print(f\"  {target.name}\")\n",
    "    print(f\"    Value: {target.value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Target Optimization\n",
    "\n",
    "Most common: optimize just the answering system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the answering prompt\n",
    "single_target_config = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    seed_answering_prompt=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "print(f\"Targets: {single_target_config.targets}\")\n",
    "print(f\"Seed prompt: {single_target_config.seed_answering_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Target Optimization\n",
    "\n",
    "Optimize multiple components simultaneously for better overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize both answering and parsing\n",
    "multi_target_config = OptimizationConfig(\n",
    "    targets=[\n",
    "        OptimizationTarget.ANSWERING_SYSTEM_PROMPT,\n",
    "        OptimizationTarget.PARSING_INSTRUCTIONS,\n",
    "    ],\n",
    "    seed_answering_prompt=\"You are a math expert. Show your reasoning.\",\n",
    "    seed_parsing_instructions=\"Extract the integer answer from the response. Look for the final answer.\",\n",
    ")\n",
    "\n",
    "print(\"Multi-target configuration:\")\n",
    "print(f\"  Targets: {[t.value for t in multi_target_config.targets]}\")\n",
    "print(f\"  Answering seed: {multi_target_config.seed_answering_prompt[:50]}...\")\n",
    "print(f\"  Parsing seed: {multi_target_config.seed_parsing_instructions[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCP Tool Description Optimization\n",
    "\n",
    "For benchmarks that use MCP tools, optimize the tool descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MCP tool optimization\n",
    "mcp_config = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.MCP_TOOL_DESCRIPTIONS],\n",
    "    seed_mcp_tool_descriptions={\n",
    "        \"calculator\": \"A calculator for basic arithmetic operations.\",\n",
    "        \"wolfram_alpha\": \"Query Wolfram Alpha for mathematical computations.\",\n",
    "        \"python_repl\": \"Execute Python code to solve problems.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"MCP tool descriptions:\")\n",
    "for tool, desc in mcp_config.seed_mcp_tool_descriptions.items():\n",
    "    print(f\"  {tool}: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seed Prompts\n",
    "\n",
    "Seed prompts are the starting point for optimization. GEPA evolves these through mutation and reflection.\n",
    "\n",
    "### Default Seeds\n",
    "\n",
    "If you don't provide seeds, defaults are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config without explicit seeds - defaults are applied\n",
    "auto_seed_config = OptimizationConfig(\n",
    "    targets=[\n",
    "        OptimizationTarget.ANSWERING_SYSTEM_PROMPT,\n",
    "        OptimizationTarget.PARSING_INSTRUCTIONS,\n",
    "    ],\n",
    "    # No seeds provided - defaults will be used\n",
    ")\n",
    "\n",
    "print(\"Auto-generated seeds:\")\n",
    "print(f\"  Answering: {auto_seed_config.seed_answering_prompt}\")\n",
    "print(f\"  Parsing: {auto_seed_config.seed_parsing_instructions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-Specific Seeds\n",
    "\n",
    "For best results, provide domain-specific seed prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIME-specific seed prompt\n",
    "aime_config = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    seed_answering_prompt=\"\"\"\n",
    "You are an expert competition mathematician solving AIME (American Invitational Mathematics Examination) problems.\n",
    "\n",
    "Key guidelines:\n",
    "1. AIME answers are always integers from 0 to 999\n",
    "2. Show your complete reasoning step by step\n",
    "3. Verify your answer by checking edge cases\n",
    "4. Box your final integer answer\n",
    "\n",
    "Solve the following problem:\n",
    "\"\"\".strip(),\n",
    ")\n",
    "\n",
    "print(\"AIME-specific seed:\")\n",
    "print(aime_config.seed_answering_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Seed Candidate\n",
    "\n",
    "Use `get_seed_candidate()` to build the initial candidate dict for GEPA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build seed candidate for GEPA\n",
    "config = OptimizationConfig(\n",
    "    targets=[\n",
    "        OptimizationTarget.ANSWERING_SYSTEM_PROMPT,\n",
    "        OptimizationTarget.PARSING_INSTRUCTIONS,\n",
    "    ],\n",
    "    seed_answering_prompt=\"Solve this math problem step by step.\",\n",
    "    seed_parsing_instructions=\"Extract the final integer answer.\",\n",
    ")\n",
    "\n",
    "seed_candidate = config.get_seed_candidate()\n",
    "\n",
    "print(\"Seed candidate dict:\")\n",
    "for key, value in seed_candidate.items():\n",
    "    print(f\"  {key}: {value[:50]}...\" if len(value) > 50 else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scoring Weights\n",
    "\n",
    "Control how template correctness and rubric quality are balanced in the optimization score.\n",
    "\n",
    "**Formula**: `score = template_weight * template_score + rubric_weight * rubric_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default weights (70% correctness, 30% quality)\n",
    "default_weights = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    template_weight=0.7,\n",
    "    rubric_weight=0.3,\n",
    ")\n",
    "\n",
    "print(f\"Default: template={default_weights.template_weight}, rubric={default_weights.rubric_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctness-only (for factual benchmarks like AIME)\n",
    "correctness_only = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    template_weight=1.0,\n",
    "    rubric_weight=0.0,\n",
    ")\n",
    "\n",
    "print(f\"Correctness-only: template={correctness_only.template_weight}, rubric={correctness_only.rubric_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality-focused (for open-ended tasks)\n",
    "quality_focused = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    template_weight=0.3,\n",
    "    rubric_weight=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Quality-focused: template={quality_focused.template_weight}, rubric={quality_focused.rubric_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights must sum to 1.0 - this will raise an error\n",
    "try:\n",
    "    invalid_weights = OptimizationConfig(\n",
    "        targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "        template_weight=0.5,\n",
    "        rubric_weight=0.3,  # Sum = 0.8, not 1.0\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GEPA Algorithm Parameters\n",
    "\n",
    "Configure the GEPA optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full GEPA parameter configuration\n",
    "gepa_params_config = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    \n",
    "    # Reflection model for prompt mutation\n",
    "    reflection_model=\"anthropic/claude-haiku-4-5\",  # LiteLLM format\n",
    "    \n",
    "    # Optimization budget (number of evaluations)\n",
    "    max_metric_calls=150,\n",
    "    \n",
    "    # Candidate selection strategy\n",
    "    candidate_selection_strategy=\"pareto\",  # \"pareto\", \"current_best\", \"epsilon_greedy\"\n",
    ")\n",
    "\n",
    "print(\"GEPA parameters:\")\n",
    "print(f\"  Reflection model: {gepa_params_config.reflection_model}\")\n",
    "print(f\"  Max metric calls: {gepa_params_config.max_metric_calls}\")\n",
    "print(f\"  Selection strategy: {gepa_params_config.candidate_selection_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Selection Strategies\n",
    "\n",
    "| Strategy | Description | Best For |\n",
    "|----------|-------------|---------|\n",
    "| `pareto` | Multi-objective Pareto optimization | Multi-model benchmarks |\n",
    "| `current_best` | Greedily select highest scorer | Single-objective tasks |\n",
    "| `epsilon_greedy` | Explore vs exploit tradeoff | Avoiding local optima |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-model Pareto optimization\n",
    "pareto_config = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    candidate_selection_strategy=\"pareto\",\n",
    ")\n",
    "print(f\"Pareto: {pareto_config.candidate_selection_strategy}\")\n",
    "\n",
    "# Greedy best selection\n",
    "greedy_config = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    candidate_selection_strategy=\"current_best\",\n",
    ")\n",
    "print(f\"Greedy: {greedy_config.candidate_selection_strategy}\")\n",
    "\n",
    "# Epsilon-greedy exploration\n",
    "explore_config = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    candidate_selection_strategy=\"epsilon_greedy\",\n",
    ")\n",
    "print(f\"Explore: {explore_config.candidate_selection_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Splitting Configuration\n",
    "\n",
    "Configure how the benchmark is split into train/val/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio-Based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default 80/20 split\n",
    "default_split = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.2,\n",
    ")\n",
    "print(f\"Default: train={default_split.train_ratio}, val={default_split.val_ratio}\")\n",
    "\n",
    "# With test set\n",
    "with_test = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    split_seed=42,  # For reproducibility\n",
    ")\n",
    "print(f\"With test: train={with_test.train_ratio}, val={with_test.val_ratio}, test={with_test.test_ratio}\")\n",
    "print(f\"  Seed: {with_test.split_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratios must sum to 1.0 - this will raise an error\n",
    "try:\n",
    "    invalid_ratios = OptimizationConfig(\n",
    "        targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "        train_ratio=0.5,\n",
    "        val_ratio=0.3,\n",
    "        test_ratio=0.1,  # Sum = 0.9, not 1.0\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Question ID Lists\n",
    "\n",
    "For precise control, specify exact question IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question IDs from benchmark\n",
    "all_ids = benchmark.get_question_ids()\n",
    "\n",
    "# Explicit ID-based splitting\n",
    "explicit_split = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    train_question_ids=all_ids[:20],  # First 20 for training\n",
    "    val_question_ids=all_ids[20:26],  # Next 6 for validation\n",
    "    test_question_ids=all_ids[26:],   # Last 4 for testing\n",
    ")\n",
    "\n",
    "print(f\"Explicit split:\")\n",
    "print(f\"  Train: {len(explicit_split.train_question_ids)} questions\")\n",
    "print(f\"  Val: {len(explicit_split.val_question_ids)} questions\")\n",
    "print(f\"  Test: {len(explicit_split.test_question_ids)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feedback Generation\n",
    "\n",
    "Enable LLM-powered feedback for richer diagnostics during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure feedback model\n",
    "feedback_model = ModelConfig(\n",
    "    id=\"feedback-haiku\",\n",
    "    model_provider=\"anthropic\",\n",
    "    model_name=\"claude-haiku-4-5\",\n",
    "    temperature=0.7,\n",
    "    interface=\"langchain\",\n",
    ")\n",
    "\n",
    "feedback_config = OptimizationConfig(\n",
    "    targets=[OptimizationTarget.ANSWERING_SYSTEM_PROMPT],\n",
    "    \n",
    "    # Enable LLM feedback\n",
    "    feedback_model=feedback_model,\n",
    "    \n",
    "    # Enable differential analysis (compare successful vs failed traces)\n",
    "    enable_differential_analysis=True,\n",
    ")\n",
    "\n",
    "print(\"Feedback configuration:\")\n",
    "print(f\"  Model: {feedback_config.feedback_model.model_name}\")\n",
    "print(f\"  Differential analysis: {feedback_config.enable_differential_analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Configuration Example\n",
    "\n",
    "Putting it all together for the AIME benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete AIME optimization configuration\n",
    "complete_config = OptimizationConfig(\n",
    "    # What to optimize\n",
    "    targets=[\n",
    "        OptimizationTarget.ANSWERING_SYSTEM_PROMPT,\n",
    "        OptimizationTarget.PARSING_INSTRUCTIONS,\n",
    "    ],\n",
    "    \n",
    "    # Domain-specific seeds\n",
    "    seed_answering_prompt=\"\"\"\n",
    "You are an expert competition mathematician solving AIME problems.\n",
    "AIME answers are always integers from 0 to 999.\n",
    "Show complete step-by-step reasoning and box your final answer.\n",
    "\"\"\".strip(),\n",
    "    seed_parsing_instructions=\"\"\"\n",
    "Extract the final integer answer from the response.\n",
    "Look for boxed answers or the last integer mentioned.\n",
    "Return only the integer value (0-999).\n",
    "\"\"\".strip(),\n",
    "    \n",
    "    # Scoring: correctness-focused for AIME\n",
    "    template_weight=1.0,\n",
    "    rubric_weight=0.0,\n",
    "    \n",
    "    # Data splitting\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.1,\n",
    "    split_seed=42,\n",
    "    \n",
    "    # GEPA parameters\n",
    "    reflection_model=\"anthropic/claude-haiku-4-5\",\n",
    "    max_metric_calls=100,\n",
    "    candidate_selection_strategy=\"pareto\",\n",
    "    \n",
    "    # Feedback (optional)\n",
    "    feedback_model=feedback_model,\n",
    "    enable_differential_analysis=True,\n",
    ")\n",
    "\n",
    "print(\"Complete AIME Configuration:\")\n",
    "print(f\"  Targets: {[t.value for t in complete_config.targets]}\")\n",
    "print(f\"  Weights: template={complete_config.template_weight}, rubric={complete_config.rubric_weight}\")\n",
    "print(f\"  Split: train={complete_config.train_ratio}, val={complete_config.val_ratio}, test={complete_config.test_ratio}\")\n",
    "print(f\"  Reflection model: {complete_config.reflection_model}\")\n",
    "print(f\"  Max calls: {complete_config.max_metric_calls}\")\n",
    "print(f\"  Selection: {complete_config.candidate_selection_strategy}\")\n",
    "print(f\"  Feedback enabled: {complete_config.feedback_model is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `targets` | What to optimize | Required |\n",
    "| `seed_*` | Initial prompts | Auto-generated |\n",
    "| `template_weight` | Weight for correctness | 0.7 |\n",
    "| `rubric_weight` | Weight for quality | 0.3 |\n",
    "| `reflection_model` | LLM for mutations | `openai/gpt-4o` |\n",
    "| `max_metric_calls` | Evaluation budget | 150 |\n",
    "| `candidate_selection_strategy` | How to pick candidates | `pareto` |\n",
    "| `train_ratio` | Training set fraction | 0.8 |\n",
    "| `val_ratio` | Validation set fraction | 0.2 |\n",
    "| `test_ratio` | Test set fraction | None |\n",
    "| `split_seed` | Random seed for splitting | None |\n",
    "| `feedback_model` | LLM for diagnostics | None |\n",
    "| `enable_differential_analysis` | Compare success/failure | True |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [03_data_splitting.ipynb](03_data_splitting.ipynb) - Advanced splitting strategies\n",
    "- [04_scoring_deep_dive.ipynb](04_scoring_deep_dive.ipynb) - Understanding score computation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
