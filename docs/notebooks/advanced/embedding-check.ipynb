{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mock-setup",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Set common attributes with realistic defaults\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        self.subunits = kwargs.get(\"subunits\", 4)\n",
    "        self.diseases = kwargs.get(\"diseases\", [\"asthma\", \"bronchitis\", \"pneumonia\"])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def create_mock_verification_result(\n",
    "    question_id: str, question_text: str, answer: str, passed: bool = True, embedding_check: dict | None = None\n",
    "):\n",
    "    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "\n",
    "    # Create mock template result with embedding check metadata\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=f\"The answer is {answer}.\",\n",
    "        parsed_llm_response={\"value\": answer},\n",
    "        parsed_gt_response={\"value\": answer},\n",
    "        verify_result=passed,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80},\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "        # Embedding check metadata\n",
    "        embedding_check_performed=embedding_check.get(\"performed\", False) if embedding_check else False,\n",
    "        embedding_similarity_score=embedding_check.get(\"similarity\") if embedding_check else None,\n",
    "        embedding_override_applied=embedding_check.get(\"override\", False) if embedding_check else False,\n",
    "        embedding_model_used=embedding_check.get(\"model\") if embedding_check else None,\n",
    "    )\n",
    "\n",
    "    # Create mock rubric result\n",
    "    rubric = VerificationResultRubric(\n",
    "        rubric_evaluation_performed=True,\n",
    "        llm_trait_scores={\n",
    "            \"Conciseness\": 4,\n",
    "            \"Clarity\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Create metadata with all required fields\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=answer,\n",
    "        answering_model=\"gpt-4.1-mini\",\n",
    "        parsing_model=\"gpt-4.1-mini\",\n",
    "        execution_time=1.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n",
    "    )\n",
    "\n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "\n",
    "# Store original run_verification\n",
    "_original_run_verification = None\n",
    "\n",
    "# Store the benchmark questions globally for the mock\n",
    "_benchmark_questions = []\n",
    "\n",
    "\n",
    "def mock_run_verification(self, config):\n",
    "    \"\"\"Mock run_verification that returns realistic results with embedding check.\"\"\"\n",
    "    global _original_run_verification, _benchmark_questions\n",
    "\n",
    "    # Get all finished questions\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "\n",
    "    # If no finished questions, generate mock results from stored questions\n",
    "    if len(finished) == 0:\n",
    "        # Use the globally stored questions\n",
    "        questions_to_process = _benchmark_questions\n",
    "    else:\n",
    "        questions_to_process = finished\n",
    "\n",
    "    # If still no questions, return empty results\n",
    "    if len(questions_to_process) == 0:\n",
    "        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Map question keywords to expected answers and embedding check scenarios\n",
    "    mock_data = [\n",
    "        {\n",
    "            \"keywords\": [\"venetoclax\", \"bcl2\"],\n",
    "            \"answer\": \"BCL2\",\n",
    "            \"passed\": False,  # Initial verification fails\n",
    "            \"embedding\": {\"performed\": True, \"similarity\": 0.9123, \"override\": True, \"model\": \"all-MiniLM-L6-v2\"},\n",
    "        },\n",
    "        {\n",
    "            \"keywords\": [\"chromosomes\"],\n",
    "            \"answer\": \"46\",\n",
    "            \"passed\": False,  # Initial verification fails\n",
    "            \"embedding\": {\"performed\": True, \"similarity\": 0.8801, \"override\": True, \"model\": \"all-MiniLM-L6-v2\"},\n",
    "        },\n",
    "        {\n",
    "            \"keywords\": [\"hemoglobin\", \"subunits\"],\n",
    "            \"answer\": \"4\",\n",
    "            \"passed\": False,  # Initial verification fails\n",
    "            \"embedding\": {\"performed\": True, \"similarity\": 0.8634, \"override\": True, \"model\": \"all-MiniLM-L6-v2\"},\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for question in questions_to_process:\n",
    "        # Handle both dict format and Question object format\n",
    "        if isinstance(question, dict):\n",
    "            q_id = question.get(\"id\", \"unknown\")\n",
    "            q_text = question.get(\"question\", \"\")\n",
    "            raw_answer = question.get(\"raw_answer\", \"\")\n",
    "        else:\n",
    "            # Question object\n",
    "            q_id = getattr(question, \"id\", \"unknown\")\n",
    "            q_text = getattr(question, \"question\", \"\")\n",
    "            raw_answer = getattr(question, \"raw_answer\", \"\")\n",
    "\n",
    "        passed = True\n",
    "        mock_ans = raw_answer\n",
    "        embedding_data = None\n",
    "        q_text_lower = q_text.lower()\n",
    "\n",
    "        for data in mock_data:\n",
    "            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n",
    "                passed = data[\"passed\"]\n",
    "                mock_ans = data[\"answer\"]\n",
    "                embedding_data = data[\"embedding\"]\n",
    "                break\n",
    "\n",
    "        # Generate a deterministic question_id if needed\n",
    "        if not q_id or q_id == \"unknown\":\n",
    "            q_id = f\"question-{abs(hash(q_text)) % 1000000:07d}\"\n",
    "\n",
    "        results.append(\n",
    "            create_mock_verification_result(\n",
    "                question_id=q_id, question_text=q_text, answer=mock_ans, passed=passed, embedding_check=embedding_data\n",
    "            )\n",
    "        )\n",
    "\n",
    "    template_results = TemplateResults(results=results)\n",
    "\n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "\n",
    "# Patch add_question to store questions globally\n",
    "_original_add_question = None\n",
    "\n",
    "\n",
    "def mock_add_question(self, *args, **kwargs):\n",
    "    \"\"\"Mock add_question that stores questions globally.\"\"\"\n",
    "    # Call the original to add the question\n",
    "    qid = _original_add_question(self, *args, **kwargs)\n",
    "    # Store the question data for the mock verification\n",
    "    question_data = {\"id\": qid, **kwargs}\n",
    "    if args:\n",
    "        # Handle positional args\n",
    "        if len(args) > 0:\n",
    "            question_data[\"question\"] = args[0]\n",
    "        if len(args) > 1:\n",
    "            question_data[\"raw_answer\"] = args[1]\n",
    "    _benchmark_questions.append(question_data)\n",
    "    return qid\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Patch Benchmark methods\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "_original_add_question = Benchmark.add_question\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "Benchmark.add_question = mock_add_question\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    Benchmark.add_question = _original_add_question\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(\"✓ Mock verification results enabled - examples will show realistic output\")\n",
    "print(\"✓ Embedding check scenarios configured for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Embedding Check (Semantic Fallback)\n",
    "\n",
    "Embedding check provides a semantic fallback mechanism that can rescue verification failures when answers are semantically correct despite structural differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-is",
   "metadata": {},
   "source": [
    "## What is Embedding Check?\n",
    "\n",
    "**Embedding check** is an optional feature that uses sentence embeddings to detect semantically equivalent answers that fail strict template matching. When verification fails, this feature computes the semantic similarity between the expected answer and the model's response. If similarity exceeds a configurable threshold, an LLM validates semantic equivalence and can override the initial failure.\n",
    "\n",
    "**Key benefits:**\n",
    "\n",
    "- **Reduces false negatives**: Catches paraphrased but correct answers\n",
    "- **Flexible evaluation**: Handles structural variations without changing templates\n",
    "- **Semantic awareness**: Uses deep learning embeddings for meaning comparison\n",
    "- **LLM validation**: Confirms equivalence with parsing model judgment\n",
    "- **Zero overhead when disabled**: Only runs on failed verifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-it-works",
   "metadata": {},
   "source": [
    "## How Embedding Check Works\n",
    "\n",
    "Embedding check activates **only when initial verification returns `False`**:\n",
    "\n",
    "**1. Initial Verification Fails**\n",
    "\n",
    "Template-based verification returns `False` due to structural mismatch.\n",
    "\n",
    "**2. Compute Embedding Similarity**\n",
    "\n",
    "Uses SentenceTransformer models to generate embeddings for both the expected answer and the model's response, then computes cosine similarity (0.0-1.0).\n",
    "\n",
    "**3. Check Threshold**\n",
    "\n",
    "If similarity score exceeds the configured threshold (default: 0.85), proceed to LLM validation.\n",
    "\n",
    "**4. LLM Semantic Validation**\n",
    "\n",
    "The parsing model evaluates whether the two answers are semantically equivalent, providing a yes/no judgment with reasoning.\n",
    "\n",
    "**5. Override Result**\n",
    "\n",
    "If the LLM confirms semantic equivalence, the verification result is overridden from `False` to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-cases",
   "metadata": {},
   "source": [
    "## Common Use Cases\n",
    "\n",
    "### Use Case 1: Paraphrased Answers\n",
    "\n",
    "**Scenario**: LLM provides the correct answer with different wording.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "- Expected Answer: \"BCL2\"\n",
    "- Model Response: \"The BCL-2 protein\"\n",
    "\n",
    "**Result**:\n",
    "\n",
    "- Initial verification: `False` (different structure)\n",
    "- Embedding similarity: `0.91`\n",
    "- Semantic check: `True` (same protein mentioned)\n",
    "- Final result: `True` (overridden) ✓\n",
    "\n",
    "### Use Case 2: Numerical Format Differences\n",
    "\n",
    "**Scenario**: Same number in different representations.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "- Expected Answer: \"46\"\n",
    "- Model Response: \"Forty-six chromosomes\"\n",
    "\n",
    "**Result**:\n",
    "\n",
    "- Initial verification: `False` (string \"46\" ≠ \"Forty-six chromosomes\")\n",
    "- Embedding similarity: `0.88`\n",
    "- Semantic check: `True` (same numerical value)\n",
    "- Final result: `True` (overridden) ✓\n",
    "\n",
    "### Use Case 3: Structural Variations\n",
    "\n",
    "**Scenario**: Correct information in different structure.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "- Expected Answer: \"4\"\n",
    "- Model Response: \"Hemoglobin A consists of four protein subunits\"\n",
    "\n",
    "**Result**:\n",
    "\n",
    "- Initial verification: `False` (template expects just number)\n",
    "- Embedding similarity: `0.86`\n",
    "- Semantic check: `True` (correct count mentioned)\n",
    "- Final result: `True` (overridden) ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-intro",
   "metadata": {},
   "source": [
    "## Enabling Embedding Check\n",
    "\n",
    "Embedding check is **disabled by default**. Enable it using environment variables.\n",
    "\n",
    "### Installation\n",
    "\n",
    "Embedding check requires the `sentence-transformers` library. Install it with the optional dependency:\n",
    "\n",
    "```bash\n",
    "pip install karenina[embeddings]\n",
    "```\n",
    "\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable embedding check\n",
    "os.environ[\"EMBEDDING_CHECK\"] = \"true\"\n",
    "\n",
    "# Specify embedding model (default: all-MiniLM-L6-v2)\n",
    "os.environ[\"EMBEDDING_CHECK_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Set similarity threshold 0.0-1.0 (default: 0.85)\n",
    "os.environ[\"EMBEDDING_CHECK_THRESHOLD\"] = \"0.85\"\n",
    "\n",
    "print(\"Embedding check configuration:\")\n",
    "print(f\"  EMBEDDING_CHECK = {os.getenv('EMBEDDING_CHECK')}\")\n",
    "print(f\"  EMBEDDING_CHECK_MODEL = {os.getenv('EMBEDDING_CHECK_MODEL')}\")\n",
    "print(f\"  EMBEDDING_CHECK_THRESHOLD = {os.getenv('EMBEDDING_CHECK_THRESHOLD')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-table",
   "metadata": {},
   "source": [
    "### Supported Embedding Models\n",
    "\n",
    "Any SentenceTransformer model is supported. Popular choices:\n",
    "\n",
    "| Model | Speed | Accuracy | Use Case |\n",
    "|-------|-------|----------|----------|\n",
    "| `all-MiniLM-L6-v2` (default) | Fast | Good | General purpose, balanced |\n",
    "| `all-mpnet-base-v2` | Slower | Better | Higher accuracy needed |\n",
    "| `multi-qa-MiniLM-L6-cos-v1` | Fast | Good | Question-answering tasks |\n",
    "| `paraphrase-multilingual-MiniLM-L12-v2` | Medium | Good | Multilingual support |\n",
    "| `all-distilroberta-v1` | Fast | Medium | Fast inference |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-intro",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "\n",
    "Here's an end-to-end workflow using embedding check with a genomics benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# 1. Enable embedding check (already set above, confirming)\n",
    "os.environ[\"EMBEDDING_CHECK\"] = \"true\"\n",
    "os.environ[\"EMBEDDING_CHECK_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "os.environ[\"EMBEDDING_CHECK_THRESHOLD\"] = \"0.85\"\n",
    "\n",
    "# 2. Create benchmark with genomics questions\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    description=\"Testing LLM knowledge of genomics and molecular biology\",\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "# Add questions\n",
    "benchmark.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\", raw_answer=\"BCL2\", author={\"name\": \"Pharma Curator\"}\n",
    ")\n",
    "\n",
    "benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\", raw_answer=\"46\", author={\"name\": \"Genetics Curator\"}\n",
    ")\n",
    "\n",
    "benchmark.add_question(\n",
    "    question=\"How many protein subunits does hemoglobin A have?\",\n",
    "    raw_answer=\"4\",\n",
    "    author={\"name\": \"Biochemistry Curator\"},\n",
    ")\n",
    "\n",
    "print(f\"Created benchmark: {benchmark.name}\")\n",
    "print(f\"Added {len(benchmark.get_finished_questions())} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-templates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate templates\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.0, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "print(\"Generating templates...\")\n",
    "benchmark.generate_all_templates(\n",
    "    model=model_config.model_name,\n",
    "    model_provider=model_config.model_provider,\n",
    "    temperature=model_config.temperature,\n",
    "    interface=model_config.interface,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(benchmark.get_finished_questions())} templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Run verification with embedding check\n",
    "print(\"Running verification...\")\n",
    "\n",
    "# Enable embedding check in config as well\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    embedding_check_enabled=True,  # Enable embedding check in config\n",
    "    embedding_check_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_check_threshold=0.85,\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "print(f\"Verification complete! Processed {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analyze embedding check results\n",
    "print(\"\\n=== Embedding Check Results ===\")\n",
    "override_count = 0\n",
    "\n",
    "for result in results.results:\n",
    "    if result.template and result.template.embedding_check_performed:\n",
    "        print(f\"\\nQuestion: {result.metadata.question_text[:60]}...\")\n",
    "        print(f\"  Expected: {result.metadata.raw_answer}\")\n",
    "        print(f\"  Model Response: {result.template.raw_llm_response[:60]}...\")\n",
    "        print(f\"  Similarity Score: {result.template.embedding_similarity_score:.4f}\")\n",
    "        print(f\"  Model Used: {result.template.embedding_model_used}\")\n",
    "        print(f\"  Override Applied: {result.template.embedding_override_applied}\")\n",
    "\n",
    "        if result.template.embedding_override_applied:\n",
    "            override_count += 1\n",
    "            print(\"  ✓ Verification overridden: False → True\")\n",
    "\n",
    "print(f\"\\nTotal overrides: {override_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-intro",
   "metadata": {},
   "source": [
    "## Understanding Results\n",
    "\n",
    "### Result Metadata\n",
    "\n",
    "When embedding check runs, results include additional metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metadata-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access embedding check metadata from a result\n",
    "# Note: Access through result.template, not result directly\n",
    "for result in results.results:\n",
    "    if result.template and result.template.embedding_check_performed:\n",
    "        print(\"Embedding check metadata:\")\n",
    "        print(f\"  embedding_check_performed: {result.template.embedding_check_performed}\")\n",
    "        print(f\"  embedding_similarity_score: {result.template.embedding_similarity_score}\")\n",
    "        print(f\"  embedding_override_applied: {result.template.embedding_override_applied}\")\n",
    "        print(f\"  embedding_model_used: {result.template.embedding_model_used}\")\n",
    "        break  # Just show first example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filtering-intro",
   "metadata": {},
   "source": [
    "### Filtering for Overrides\n",
    "\n",
    "Find all cases where embedding check rescued a failed verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filtering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all overridden results\n",
    "overridden = [r for r in results.results if r.template and r.template.embedding_override_applied]\n",
    "\n",
    "print(f\"Found {len(overridden)} overridden verifications\")\n",
    "\n",
    "for result in overridden:\n",
    "    print(f\"  {result.metadata.question_id}: similarity={result.template.embedding_similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-intro",
   "metadata": {},
   "source": [
    "### Computing Override Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embedding check statistics\n",
    "total_questions = len(results)\n",
    "embedding_checks_performed = sum(1 for r in results.results if r.template and r.template.embedding_check_performed)\n",
    "overrides_applied = sum(1 for r in results.results if r.template and r.template.embedding_override_applied)\n",
    "\n",
    "print(f\"Total questions: {total_questions}\")\n",
    "print(f\"Embedding checks performed: {embedding_checks_performed}\")\n",
    "print(f\"Overrides applied: {overrides_applied}\")\n",
    "if embedding_checks_performed > 0:\n",
    "    print(f\"Override rate: {overrides_applied / embedding_checks_performed * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-intro",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "### When Disabled\n",
    "\n",
    "- **Zero overhead**: Feature not loaded or executed\n",
    "- **No dependencies required**: sentence-transformers not needed\n",
    "\n",
    "### When Enabled\n",
    "\n",
    "Embedding check only runs on **failed verifications**, so the impact depends on your failure rate.\n",
    "\n",
    "**Cost impact:**\n",
    "\n",
    "Embedding check adds one additional LLM call (semantic validation) for each failed verification where similarity exceeds the threshold. This uses your configured parsing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threshold-intro",
   "metadata": {},
   "source": [
    "## Tuning the Similarity Threshold\n",
    "\n",
    "The similarity threshold (default: 0.85) controls when LLM validation is triggered.\n",
    "\n",
    "### Threshold Guidelines\n",
    "\n",
    "| Threshold | Behavior | Use Case |\n",
    "|-----------|----------|----------|\n",
    "| **0.80-0.85** | Moderate selectivity | General purpose, balanced |\n",
    "| **0.85-0.90** (default) | Higher selectivity | Reduce false overrides |\n",
    "| **0.90-0.95** | Very selective | Only very similar answers |\n",
    "| **0.75-0.80** | Lower selectivity | Catch more paraphrases |\n",
    "\n",
    "### Finding the Right Threshold\n",
    "\n",
    "**Start with default (0.85):**\n",
    "\n",
    "```python\n",
    "os.environ[\"EMBEDDING_CHECK_THRESHOLD\"] = \"0.85\"\n",
    "```\n",
    "\n",
    "**Too many false overrides?** → Increase threshold:\n",
    "\n",
    "```python\n",
    "os.environ[\"EMBEDDING_CHECK_THRESHOLD\"] = \"0.90\"\n",
    "```\n",
    "\n",
    "**Missing valid paraphrases?** → Decrease threshold:\n",
    "\n",
    "```python\n",
    "os.environ[\"EMBEDDING_CHECK_THRESHOLD\"] = \"0.80\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimentation-intro",
   "metadata": {},
   "source": [
    "### Threshold Experimentation\n",
    "\n",
    "Test different thresholds on a sample. Note: This requires `sentence-transformers` to be installed. In this demo, we'll show the API usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Testing different thresholds\n",
    "# Note: compute_embedding_similarity requires sentence-transformers\n",
    "# This is a demonstration of the API - in actual use, install with:\n",
    "# pip install karenina[embeddings]\n",
    "\n",
    "\n",
    "# Test cases (expected answer, model response)\n",
    "test_cases = [\n",
    "    (\"BCL2\", \"The BCL-2 protein\"),\n",
    "    (\"46\", \"Forty-six chromosomes\"),\n",
    "    (\"4\", \"Four protein subunits\"),\n",
    "    (\"hemoglobin\", \"haemoglobin\"),  # Spelling variant\n",
    "]\n",
    "\n",
    "# Try different thresholds\n",
    "thresholds = [0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "\n",
    "print(\"Testing similarity scores across different answer pairs:\\n\")\n",
    "\n",
    "# In this demo, we'll simulate the similarity scores\n",
    "# In actual use, compute_embedding_similarity would return real scores\n",
    "mock_similarities = [0.91, 0.88, 0.86, 0.84]\n",
    "\n",
    "for i, (expected, response) in enumerate(test_cases):\n",
    "    similarity = mock_similarities[i]\n",
    "    print(f\"Expected: '{expected}'\")\n",
    "    print(f\"Response: '{response}'\")\n",
    "    print(f\"Similarity: {similarity:.4f}\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        would_trigger = \"✓\" if similarity >= threshold else \"✗\"\n",
    "        print(f\"  Threshold {threshold}: {would_trigger}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-use",
   "metadata": {},
   "source": [
    "## When to Use Embedding Check\n",
    "\n",
    "### ✅ Use Embedding Check When:\n",
    "\n",
    "- **Paraphrased answers are common**: Models often rephrase correct answers\n",
    "- **Format flexibility needed**: Accept \"46\" and \"forty-six\" as equivalent\n",
    "- **Reducing false negatives**: Minimize cases where correct answers are marked wrong\n",
    "- **Testing creative models**: Models that elaborate or rephrase more frequently\n",
    "- **Multi-language evaluation**: Detecting equivalent meanings across languages\n",
    "\n",
    "### ❌ Don't Use Embedding Check When:\n",
    "\n",
    "- **Strict format required**: Exact format is critical (e.g., gene symbols, IDs)\n",
    "- **High precision needed**: False positives are more costly than false negatives\n",
    "- **Templates handle variations**: Templates already account for expected variations\n",
    "- **Performance is critical**: Cannot afford extra 500-2000ms per failed verification\n",
    "- **Deterministic evaluation required**: Need reproducible results without LLM judgment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-intro",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Enable Selectively\n",
    "\n",
    "Don't enable embedding check for all benchmarks. Use it when you know paraphrasing is common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selectively",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Enable for natural language questions\n",
    "os.environ[\"EMBEDDING_CHECK\"] = \"true\"\n",
    "# benchmark_nl = Benchmark.load(\"natural_language_qa.jsonld\")\n",
    "\n",
    "# Good: Disable for strict format questions\n",
    "os.environ[\"EMBEDDING_CHECK\"] = \"false\"\n",
    "# benchmark_ids = Benchmark.load(\"gene_id_extraction.jsonld\")\n",
    "\n",
    "print(\"Tip: Enable embedding check selectively based on question type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor-intro",
   "metadata": {},
   "source": [
    "### 2. Monitor Override Rates\n",
    "\n",
    "Track how often embedding check overrides results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate override rate from results\n",
    "override_rate = sum(1 for r in results.results if r.template and r.template.embedding_override_applied) / len(results)\n",
    "\n",
    "print(f\"Override rate: {override_rate * 100:.1f}%\")\n",
    "\n",
    "if override_rate > 0.20:  # More than 20% overrides\n",
    "    print(\"Warning: High override rate. Consider:\")\n",
    "    print(\"  - Reviewing template definitions\")\n",
    "    print(\"  - Adjusting similarity threshold\")\n",
    "    print(\"  - Examining overridden cases manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review-intro",
   "metadata": {},
   "source": [
    "### 3. Review Overridden Cases\n",
    "\n",
    "Manually inspect overridden verifications to ensure quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "review",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review all overridden cases\n",
    "print(\"Reviewing overridden cases:\\n\")\n",
    "for result in results.results:\n",
    "    if result.template and result.template.embedding_override_applied:\n",
    "        print(f\"Question: {result.metadata.question_text}\")\n",
    "        print(f\"Expected: {result.metadata.raw_answer}\")\n",
    "        print(f\"Got: {result.template.raw_llm_response}\")\n",
    "        print(f\"Similarity: {result.template.embedding_similarity_score:.4f}\")\n",
    "        print()\n",
    "\n",
    "        # In manual review, you would validate:\n",
    "        # is_correct = input(\"Is this override correct? (y/n): \")\n",
    "        # if is_correct.lower() != 'y':\n",
    "        #     print(\"⚠ False override detected - consider higher threshold\")\n",
    "\n",
    "print(\"Manual review complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-intro",
   "metadata": {},
   "source": [
    "### 4. Choose the Right Model\n",
    "\n",
    "**For most use cases:** Use default `all-MiniLM-L6-v2` (fast, good accuracy)\n",
    "\n",
    "**For higher accuracy:** Use `all-mpnet-base-v2` (slower, better)\n",
    "\n",
    "**For question-answering:** Use `multi-qa-MiniLM-L6-cos-v1` (optimized for Q&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-accuracy configuration example\n",
    "print(\"High-accuracy configuration:\")\n",
    "\n",
    "# Set to use a more accurate model\n",
    "os.environ[\"EMBEDDING_CHECK_MODEL\"] = \"all-mpnet-base-v2\"\n",
    "# Higher threshold with better model\n",
    "os.environ[\"EMBEDDING_CHECK_THRESHOLD\"] = \"0.90\"\n",
    "\n",
    "print(f\"  EMBEDDING_CHECK_MODEL = {os.getenv('EMBEDDING_CHECK_MODEL')}\")\n",
    "print(f\"  EMBEDDING_CHECK_THRESHOLD = {os.getenv('EMBEDDING_CHECK_THRESHOLD')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep-judgment-intro",
   "metadata": {},
   "source": [
    "### 5. Combine with Deep-Judgment\n",
    "\n",
    "Embedding check works well with deep-judgment parsing for maximum transparency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deep-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable both features\n",
    "os.environ[\"EMBEDDING_CHECK\"] = \"true\"\n",
    "\n",
    "# Example configuration with both features\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.0, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "config_example = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    embedding_check_enabled=True,  # Enable embedding check\n",
    "    deep_judgment_enabled=True,  # Also enable deep-judgment\n",
    "    deep_judgment_max_excerpts_per_attribute=3,\n",
    ")\n",
    "\n",
    "print(\"Configuration example with both embedding check and deep-judgment enabled\")\n",
    "print(\"This provides maximum transparency for verification results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integration-intro",
   "metadata": {},
   "source": [
    "## Integration with Other Features\n",
    "\n",
    "### Embedding Check + Deep-Judgment\n",
    "\n",
    "Use embedding check to catch paraphrases, deep-judgment for transparency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integration-deep-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Enable both features\n",
    "config_both = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    embedding_check_enabled=True,\n",
    "    deep_judgment_enabled=True,\n",
    "    deep_judgment_max_excerpts_per_attribute=3,\n",
    ")\n",
    "\n",
    "print(\"Both features enabled:\")\n",
    "print(\"  - Embedding check: Semantic fallback for paraphrased answers\")\n",
    "print(\"  - Deep-judgment: Multi-stage parsing with evidence extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstention-intro",
   "metadata": {},
   "source": [
    "### Embedding Check + Abstention Detection\n",
    "\n",
    "Both features can run together. Abstention detection identifies refusals; embedding check handles paraphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Enable both features\n",
    "config_abstention = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    embedding_check_enabled=True,\n",
    "    abstention_check_enabled=True,\n",
    ")\n",
    "\n",
    "print(\"Both features enabled:\")\n",
    "print(\"  - Abstention detection: Identifies model refusals\")\n",
    "print(\"  - Embedding check: Handles paraphrased correct answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Issue 1: Embedding Check Not Running\n",
    "\n",
    "**Symptom**: `embedding_check_performed` is always `False`\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. Verify environment variable is set: `os.getenv(\"EMBEDDING_CHECK\")`\n",
    "2. Check that initial verification is failing (embedding check only runs on failures)\n",
    "3. Ensure sentence-transformers is installed: `pip install karenina[embeddings]`\n",
    "\n",
    "### Issue 2: No Overrides Applied\n",
    "\n",
    "**Symptom**: Embedding checks run but never override results\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. Lower the similarity threshold: `os.environ[\"EMBEDDING_CHECK_THRESHOLD\"] = \"0.80\"`\n",
    "2. Review similarity scores to see if they're below threshold\n",
    "3. Try a more accurate embedding model: `all-mpnet-base-v2`\n",
    "\n",
    "### Issue 3: Too Many Overrides\n",
    "\n",
    "**Symptom**: High override rate (>20%) suggesting false positives\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. Raise the similarity threshold: `os.environ[\"EMBEDDING_CHECK_THRESHOLD\"] = \"0.90\"`\n",
    "2. Review templates to ensure they're capturing expected variations\n",
    "3. Manually inspect overridden cases to identify patterns\n",
    "\n",
    "### Issue 4: Slow Performance\n",
    "\n",
    "**Symptom**: Verification takes too long with embedding check enabled\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. Use faster embedding model: `all-MiniLM-L6-v2` or `all-distilroberta-v1`\n",
    "2. Increase threshold to reduce LLM validation calls\n",
    "3. Improve templates to reduce initial verification failures\n",
    "4. Disable embedding check for benchmarks where it's not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once you have embedding check configured, you can:\n",
    "\n",
    "- **[Deep-Judgment Parsing](deep-judgment.md)** - Multi-stage parsing with evidence extraction\n",
    "- **[Abstention Detection](abstention-detection.md)** - Identify model refusals\n",
    "- **[Verification](../using-karenina/verification.md)** - Complete verification workflow\n",
    "- **[Saving and Loading](../using-karenina/saving-loading.md)** - Persist benchmarks\n",
    "\n",
    "## Related Documentation\n",
    "\n",
    "- **[Verification](../using-karenina/verification.md)** - Core verification workflow\n",
    "- **[Templates](../using-karenina/templates.md)** - Answer template creation\n",
    "- **[Deep-Judgment](deep-judgment.md)** - Multi-stage parsing\n",
    "- **[Abstention Detection](abstention-detection.md)** - Refusal detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}