{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T10:35:48.573556Z",
     "iopub.status.busy": "2026-01-04T10:35:48.573480Z",
     "iopub.status.idle": "2026-01-04T10:35:49.410453Z",
     "shell.execute_reply": "2026-01-04T10:35:49.410205Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mock setup complete\n",
      "✓ Karenina package loaded\n",
      "✓ Mock TaskEval evaluation enabled\n"
     ]
    }
   ],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell sets up mocking infrastructure for TaskEval examples\n",
    "\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set - make these available globally\n",
    "from karenina.benchmark.task_eval import TaskEval\n",
    "from karenina.schemas.domain import LLMRubricTrait, RegexTrait, Rubric\n",
    "from karenina.schemas.workflow import ModelConfig, VerificationConfig\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Set common attributes with realistic defaults\n",
    "        self.endpoint_created = kwargs.get(\"endpoint_created\", True)\n",
    "        self.has_error_handling = kwargs.get(\"has_error_handling\", True)\n",
    "        self.action_taken = kwargs.get(\"action_taken\", \"implement_api\")\n",
    "        self.result = kwargs.get(\"result\", \"success\")\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"Successfully implemented REST API with proper error handling\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"Successfully implemented REST API with proper error handling\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(\"✓ Karenina package loaded\")\n",
    "print(\"✓ Mock TaskEval evaluation enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TaskEval: Task-Centric Trace Evaluation\n",
    "\n",
    "Evaluate pre-logged agent workflow outputs by attaching verification criteria to existing traces.\n",
    "\n",
    "## What is TaskEval?\n",
    "\n",
    "**TaskEval** is a trace-centric evaluation framework that inverts the traditional benchmarking workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-Centric vs Question-Centric\n",
    "\n",
    "| Use Case | Use Benchmark | Use TaskEval |\n",
    "|----------|--------------|-------------|\n",
    "| Test LLM knowledge | ✅ | ❌ |\n",
    "| Evaluate agent workflows | ❌ | ✅ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "Here's a minimal example showing the task-centric workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T10:35:49.424093Z",
     "iopub.status.busy": "2026-01-04T10:35:49.423946Z",
     "iopub.status.idle": "2026-01-04T10:35:49.425902Z",
     "shell.execute_reply": "2026-01-04T10:35:49.425711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TaskEval setup complete\n"
     ]
    }
   ],
   "source": [
    "# Quick Start Example: TaskEval with templates and rubrics\n",
    "\n",
    "# 1. Create TaskEval instance\n",
    "task = TaskEval(task_id=\"agent_code_generation\")\n",
    "\n",
    "# 2. Log agent execution traces\n",
    "task.log(\"Reasoning: Need to implement a REST API endpoint\")\n",
    "task.log(\"Plan: Use FastAPI with proper error handling\")\n",
    "task.log(\"Implementation: Created /api/users endpoint\")\n",
    "task.log(\"Testing: Verified with test cases\")\n",
    "\n",
    "# 3. Attach verification criteria\n",
    "task.add_rubric(\n",
    "    Rubric(\n",
    "        llm_traits=[\n",
    "            LLMRubricTrait(name=\"clarity\", description=\"How clear is the plan?\", kind=\"score\", min_score=1, max_score=5)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ TaskEval setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring and Running Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T10:35:49.426921Z",
     "iopub.status.busy": "2026-01-04T10:35:49.426857Z",
     "iopub.status.idle": "2026-01-04T10:35:49.432974Z",
     "shell.execute_reply": "2026-01-04T10:35:49.432776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question 39430513e323dbe155b2201a8b877bd2: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete!\n",
      "0/1 traces passed\n"
     ]
    }
   ],
   "source": [
    "# Configure evaluation (parsing_only = no answer generation)\n",
    "config = VerificationConfig(\n",
    "    parsing_models=[ModelConfig(id=\"parser\", model_provider=\"openai\", model_name=\"gpt-4o-mini\")], parsing_only=True\n",
    ")\n",
    "\n",
    "# Evaluate traces against verification criteria\n",
    "result = task.evaluate(config)\n",
    "\n",
    "print(\"Evaluation complete!\")\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Results\n",
    "\n",
    "TaskEval provides multiple ways to access results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T10:35:49.433951Z",
     "iopub.status.busy": "2026-01-04T10:35:49.433892Z",
     "iopub.status.idle": "2026-01-04T10:35:49.435531Z",
     "shell.execute_reply": "2026-01-04T10:35:49.435354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════════════════════════\n",
      "                        TASK EVALUATION RESULTS\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "Task ID: agent_code_generation\n",
      "Timestamp: 2026-01-04 11:35:49\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "GLOBAL EVALUATION\n",
      "────────────────────────────────────────────────────────────\n",
      "Verification Results:\n",
      "  Question: rubric_only_eval\n",
      "     Status: ⚠ NO RESULT\n",
      "     Output: \"Reasoning: Need to implement a REST API endpoint\n",
      "\n",
      "Plan: Use FastAPI with proper error handling\n",
      "\n",
      "Implementation: Created /api/users endpoint\n",
      "\n",
      "Testing: Verified with test cases\"\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "SUMMARY: 0/1 traces passed\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Question: rubric_only_eval\n",
      "Passed: None\n"
     ]
    }
   ],
   "source": [
    "# Display formatted results\n",
    "print(result.display())\n",
    "\n",
    "# Access individual verification results\n",
    "for question_id, vr_list in result.global_eval.verification_results.items():\n",
    "    vr = vr_list[0]\n",
    "    print(f\"\\nQuestion: {question_id}\")\n",
    "    print(f\"Passed: {vr.template.verify_result}\")\n",
    "    if vr.rubric and vr.rubric.llm_trait_scores:\n",
    "        print(f\"LLM Traits: {vr.rubric.llm_trait_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dict Trace Logging\n",
    "\n",
    "**Dict traces** are the recommended way to log structured agent outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T10:35:49.436367Z",
     "iopub.status.busy": "2026-01-04T10:35:49.436317Z",
     "iopub.status.idle": "2026-01-04T10:35:49.444797Z",
     "shell.execute_reply": "2026-01-04T10:35:49.444594Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question 07ad92091c4a1f320394a53da4151b75: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question 407414879a82b1978bf9b4c6905d9737: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question 6c94baa466119ca82916fafb26b0cffb: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question ce1b3b8ca5c2dc8d09f88c295f59f30d: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict trace results:\n",
      "  - dict_key_analysis\n",
      "  - dict_key_execution\n",
      "  - dict_key_plan\n",
      "  - dict_key_testing\n"
     ]
    }
   ],
   "source": [
    "# Dict traces example - structured logging\n",
    "task_dict = TaskEval(task_id=\"structured_eval\")\n",
    "\n",
    "# Log a dict trace - each key becomes a separate evaluation point\n",
    "task_dict.log(\n",
    "    {\n",
    "        \"analysis\": \"Examined requirements and constraints\",\n",
    "        \"plan\": \"Generated 3-step implementation plan\",\n",
    "        \"execution\": \"Implemented all steps successfully\",\n",
    "        \"testing\": \"Verified with test cases\",\n",
    "    }\n",
    ")\n",
    "\n",
    "task_dict.add_rubric(Rubric(llm_traits=[LLMRubricTrait(name=\"completeness\", kind=\"score\", min_score=1, max_score=5)]))\n",
    "\n",
    "config = VerificationConfig(\n",
    "    parsing_models=[ModelConfig(id=\"parser\", model_provider=\"openai\", model_name=\"gpt-4o-mini\")], parsing_only=True\n",
    ")\n",
    "result_dict = task_dict.evaluate(config)\n",
    "\n",
    "print(\"Dict trace results:\")\n",
    "for question_id in result_dict.global_eval.verification_results.keys():\n",
    "    print(f\"  - {question_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric Types\n",
    "\n",
    "- **LLMRubricTrait**: Qualitative assessment (score or boolean)\n",
    "- **RegexTrait**: Deterministic pattern matching (boolean)\n",
    "- **MetricRubricTrait**: Quantitative metrics (precision, recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T10:35:49.445880Z",
     "iopub.status.busy": "2026-01-04T10:35:49.445829Z",
     "iopub.status.idle": "2026-01-04T10:35:49.453622Z",
     "shell.execute_reply": "2026-01-04T10:35:49.453446Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question d82176653c6d79efae93171e60ee45f3: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question caa8286b87c14f40d780df79bcacaac3: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question 0fd9fbdd2414e2701d8b526e7694edc9: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question ce1b3b8ca5c2dc8d09f88c295f59f30d: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive rubric results:\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive rubrics example with all three trait types\n",
    "task_rubrics = TaskEval(task_id=\"comprehensive_rubrics\")\n",
    "\n",
    "task_rubrics.log(\n",
    "    {\n",
    "        \"requirements\": \"Analyzed requirements\",\n",
    "        \"architecture\": \"Designed 3-tier architecture\",\n",
    "        \"implementation\": \"Implemented with error handling\",\n",
    "        \"testing\": \"Created unit tests\",\n",
    "    }\n",
    ")\n",
    "\n",
    "comprehensive_rubric = Rubric(\n",
    "    llm_traits=[\n",
    "        LLMRubricTrait(name=\"clarity\", kind=\"score\", min_score=1, max_score=5),\n",
    "    ],\n",
    "    regex_traits=[\n",
    "        RegexTrait(name=\"has_code\", pattern=r\"```\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "task_rubrics.add_rubric(comprehensive_rubric)\n",
    "\n",
    "config = VerificationConfig(\n",
    "    parsing_models=[ModelConfig(id=\"parser\", model_provider=\"openai\", model_name=\"gpt-4o-mini\")], parsing_only=True\n",
    ")\n",
    "result_rubrics = task_rubrics.evaluate(config)\n",
    "\n",
    "print(\"Comprehensive rubric results:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-Specific Evaluation\n",
    "\n",
    "Evaluate different workflow phases with phase-specific rubrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T10:35:49.454725Z",
     "iopub.status.busy": "2026-01-04T10:35:49.454645Z",
     "iopub.status.idle": "2026-01-04T10:35:49.459181Z",
     "shell.execute_reply": "2026-01-04T10:35:49.458994Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question 07ad92091c4a1f320394a53da4151b75: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse abstention detection response as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluation failed: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rubric evaluator initialization/configuration failed for question 6c94baa466119ca82916fafb26b0cffb: Failed to evaluate rubric traits using batch strategy: Could not parse response into BatchRubricScores: Successfully implemented REST API with proper error handling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-specific results:\n",
      "Questions evaluated: 2\n"
     ]
    }
   ],
   "source": [
    "# Step-specific evaluation example\n",
    "task_steps = TaskEval(task_id=\"multi_step_agent\")\n",
    "\n",
    "# Add rubric globally (for this simple example)\n",
    "planning_rubric = Rubric(llm_traits=[LLMRubricTrait(name=\"plan_quality\", kind=\"score\", min_score=1, max_score=5)])\n",
    "task_steps.add_rubric(planning_rubric)  # Global rubric\n",
    "\n",
    "task_steps.log({\"analysis\": \"Analyzed the problem\", \"plan\": \"Created plan\"})\n",
    "\n",
    "config = VerificationConfig(\n",
    "    parsing_models=[ModelConfig(id=\"parser\", model_provider=\"openai\", model_name=\"gpt-4o-mini\")], parsing_only=True\n",
    ")\n",
    "result_steps = task_steps.evaluate(config)\n",
    "\n",
    "print(\"Step-specific results:\")\n",
    "print(f\"Questions evaluated: {len(result_steps.global_eval.verification_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. Use **Dict Traces** for structured evaluation\n",
    "2. Choose appropriate **Rubric Types** for your use case\n",
    "3. Use **Step IDs** for multi-step workflows\n",
    "4. Use **Replicates** for critical evaluations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
