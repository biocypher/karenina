{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unittest.mock import Mock, MagicMock, patch, PropertyMock\n",
    "from typing import Any, Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations (make global for all cells)\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    "    VerificationResultRubric,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = kwargs.get('count', 46)\n",
    "        self.target = kwargs.get('target', 'BCL2')\n",
    "        self.subunits = kwargs.get('subunits', 4)\n",
    "        self.diseases = kwargs.get('diseases', ['asthma', 'bronchitis', 'pneumonia'])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "def create_mock_verification_result(question_id: str, question_text: str, answer: str, passed: bool = True):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=f\"The answer is {answer}.\",\n",
    "        parsed_llm_response={\"value\": answer},\n",
    "        parsed_gt_response={\"value\": answer},\n",
    "        verify_result=passed,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80}\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "    )\n",
    "    rubric = VerificationResultRubric(\n",
    "        rubric_evaluation_performed=True,\n",
    "        llm_trait_scores={\"Conciseness\": 4, \"Clarity\": True},\n",
    "    )\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=answer,\n",
    "        answering_model=\"gpt-4.1-mini\",\n",
    "        parsing_model=\"gpt-4.1-mini\",\n",
    "        execution_time=1.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n",
    "    )\n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "_original_run_verification = None\n",
    "_original_generate_all_templates = None\n",
    "\n",
    "def mock_generate_all_templates(self, **kwargs):\n",
    "    all_questions = self.get_all_questions(ids_only=False)\n",
    "    for q_data in all_questions:\n",
    "        q_id = q_data.get('id')\n",
    "        if q_id and q_data.get('answer_template') is None:\n",
    "            mock_template = {'template_type': 'mock', 'generated_at': datetime.now().isoformat()}\n",
    "            self._base.data[q_id]['answer_template'] = mock_template\n",
    "    return {}\n",
    "\n",
    "def mock_run_verification(self, config):\n",
    "    global _original_run_verification\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "    if len(finished) == 0:\n",
    "        all_questions = self.get_all_questions(ids_only=False)\n",
    "        if len(all_questions) == 0:\n",
    "            return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "        finished = all_questions\n",
    "    results = []\n",
    "    mock_data = [\n",
    "        {\"keywords\": [\"chromosomes\"], \"answer\": \"46\", \"passed\": True},\n",
    "        {\"keywords\": [\"venetoclax\", \"bcl2\"], \"answer\": \"BCL2\", \"passed\": True},\n",
    "        {\"keywords\": [\"hemoglobin\", \"subunits\"], \"answer\": \"4\", \"passed\": True},\n",
    "        {\"keywords\": [\"inflammatory\", \"lung\"], \"answer\": \"asthma, bronchitis, pneumonia\", \"passed\": True},\n",
    "    ]\n",
    "    for question in finished:\n",
    "        q_id = question.get('id', question.get('question_id', ''))\n",
    "        q_text = question.get('question', '')\n",
    "        raw_answer = question.get('raw_answer', '')\n",
    "        passed = True\n",
    "        mock_ans = raw_answer\n",
    "        q_text_lower = q_text.lower()\n",
    "        for data in mock_data:\n",
    "            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n",
    "                passed = data[\"passed\"]\n",
    "                mock_ans = data[\"answer\"]\n",
    "                break\n",
    "        results.append(create_mock_verification_result(\n",
    "            question_id=q_id,\n",
    "            question_text=q_text,\n",
    "            answer=mock_ans,\n",
    "            passed=passed\n",
    "        ))\n",
    "    template_results = TemplateResults(results=results)\n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "_llm_patches = [\n",
    "    patch('langchain_openai.ChatOpenAI', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('langchain_anthropic.ChatAnthropic', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('langchain_google_genai.ChatGoogleGenerativeAI', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch('karenina.infrastructure.llm.interface.init_chat_model_unified', side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "]\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "from karenina.benchmark import Benchmark\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "_original_generate_all_templates = Benchmark.generate_all_templates\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "Benchmark.generate_all_templates = mock_generate_all_templates\n",
    "\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    Benchmark.generate_all_templates = _original_generate_all_templates\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(f\"\u2713 Mock setup complete\")\n",
    "print(f\"\u2713 Temp directory: {TEMP_DIR}\")\n",
    "print(f\"\u2713 Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(f\"\u2713 Mock verification results enabled - examples will show realistic output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Presets\n",
    "\n",
    "Configuration presets allow you to save, load, and share complete verification configurations, eliminating the need to manually reconfigure settings for recurring benchmark scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Presets?\n",
    "\n",
    "**Configuration presets** are saved snapshots of your verification settings that can be quickly reloaded for future benchmark runs. A preset captures:\n",
    "\n",
    "- **Model configurations**: Answering and parsing models with all their settings\n",
    "- **Evaluation settings**: Replication count, evaluation mode, parsing-only flag\n",
    "- **Rubric settings**: Enabled status, trait selection, evaluation mode\n",
    "- **Advanced features**: Deep-judgment, abstention detection, few-shot configuration\n",
    "\n",
    "Presets make it easy to:\n",
    "\n",
    "- **Reuse configurations**: Quickly switch between different benchmark setups\n",
    "- **Ensure consistency**: Use the same configuration across multiple runs\n",
    "- **Share setups**: Export and share configurations with teammates\n",
    "- **Organize scenarios**: Maintain separate configs for testing, production, experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Presets?\n",
    "\n",
    "### 1. Save Time\n",
    "\n",
    "Instead of manually reconfiguring models and settings each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without presets: Manually configure every time \u274c\n",
    "print(\"Example of manual configuration (not executed):\")\n",
    "print(\"\"\"\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model1, model2],\n",
    "    parsing_models=[parser],\n",
    "    replicate_count=3,\n",
    "    rubric_enabled=True,\n",
    "    deep_judgment_enabled=True,\n",
    "    deep_judgment_max_excerpts_per_attribute=3,\n",
    "    # ... 15 more parameters ...\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With presets: Load saved configuration \u2713\n",
    "from karenina.schemas import VerificationConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# (In real use, you would specify an actual preset file)\n",
    "# config = VerificationConfig.from_preset(Path(\"my-setup.json\"))\n",
    "print(\"Config loaded from preset file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Maintain Consistency\n",
    "\n",
    "Presets ensure the same configuration is used across runs, eliminating configuration drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showing consistency across runs\n",
    "print(\"Run 1: Today\")\n",
    "print(\"  config = VerificationConfig.from_preset(Path('production-config.json'))\")\n",
    "print(\"  results1 = benchmark.run_verification(config)\")\n",
    "print()\n",
    "print(\"Run 2: Next week (identical configuration guaranteed)\")\n",
    "print(\"  config = VerificationConfig.from_preset(Path('production-config.json'))\")\n",
    "print(\"  results2 = benchmark.run_verification(config)\")\n",
    "print()\n",
    "print(\"\u2713 Same configuration = comparable results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Share Configurations\n",
    "\n",
    "Share preset files with teammates or across projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of sharing presets via command line\n",
    "print(\"# Export preset\")\n",
    "print(\"cp presets/genomics-standard.json shared/\")\n",
    "print()\n",
    "print(\"# Teammate imports preset\")\n",
    "print(\"cp shared/genomics-standard.json presets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Preset\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "Save your current configuration with a descriptive name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import VerificationConfig, ModelConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# Create and configure your verification setup\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# Note: When rubric_enabled=True, must also set evaluation_mode='template_and_rubric'\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    replicate_count=3,\n",
    "    rubric_enabled=True,\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    deep_judgment_enabled=True\n",
    ")\n",
    "\n",
    "# Create a presets directory for our example\n",
    "example_presets_dir = TEMP_DIR / \"presets\"\n",
    "example_presets_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as a preset\n",
    "metadata = config.save_preset(\n",
    "    name=\"Genomics Standard Config\",\n",
    "    description=\"Standard setup for genomics benchmarks with deep-judgment\",\n",
    "    presets_dir=example_presets_dir\n",
    ")\n",
    "\n",
    "print(f\"Preset saved to: {metadata['filepath']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Gets Saved?\n",
    "\n",
    "**\u2713 Included:**\n",
    "\n",
    "- All model configurations (answering_models, parsing_models)\n",
    "- Evaluation settings (replicate_count, parsing_only, evaluation_mode)\n",
    "- Rubric settings (rubric_enabled, rubric_trait_names)\n",
    "- Advanced features (abstention_enabled, deep_judgment_*, few_shot_config)\n",
    "\n",
    "**\u2717 Excluded:**\n",
    "\n",
    "- Job-specific metadata (run_name)\n",
    "- Database configuration (db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preset File Structure\n",
    "\n",
    "Presets are saved as JSON files in the `presets/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the saved preset file structure\n",
    "import json\n",
    "\n",
    "preset_file = Path(metadata['filepath'])\n",
    "if preset_file.exists():\n",
    "    with open(preset_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Show preset structure\n",
    "    print(\"Preset file structure:\")\n",
    "    print(json.dumps({\n",
    "        \"id\": data[\"id\"],\n",
    "        \"name\": data[\"name\"],\n",
    "        \"description\": data[\"description\"],\n",
    "        \"config\": {\n",
    "            \"answering_models\": [\n",
    "                {\"id\": m[\"id\"], \"model_provider\": m[\"model_provider\"]}\n",
    "                for m in data[\"config\"][\"answering_models\"]\n",
    "            ],\n",
    "            \"parsing_models\": [\n",
    "                {\"id\": m[\"id\"], \"model_provider\": m[\"model_provider\"]}\n",
    "                for m in data[\"config\"][\"parsing_models\"]\n",
    "            ],\n",
    "            \"replicate_count\": data[\"config\"][\"replicate_count\"],\n",
    "            \"rubric_enabled\": data[\"config\"][\"rubric_enabled\"],\n",
    "            \"deep_judgment_enabled\": data[\"config\"][\"deep_judgment_enabled\"]\n",
    "        },\n",
    "        \"created_at\": data[\"created_at\"],\n",
    "        \"updated_at\": data[\"updated_at\"]\n",
    "    }, indent=2))\n",
    "else:\n",
    "    print(f\"Preset file not found: {preset_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Preset\n",
    "\n",
    "!!! tip \"CLI Preset Management\"\n",
    "    You can also manage presets from the command line:\n",
    "\n",
    "    - `karenina preset list` - List all available presets\n",
    "    - `karenina preset show NAME` - Display preset configuration\n",
    "    - `karenina preset delete NAME` - Delete a preset\n",
    "    - `karenina verify checkpoint.jsonld --preset NAME` - Run verification with preset\n",
    "\n",
    "    See **[CLI Verification](../using-karenina/cli-verification.md#preset-management)** for complete CLI preset documentation.\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "Load a saved preset and use it for verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import VerificationConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the preset we just saved\n",
    "config = VerificationConfig.from_preset(\n",
    "    preset_file\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Preset loaded successfully\")\n",
    "print(f\"  Answering models: {[m.id for m in config.answering_models]}\")\n",
    "print(f\"  Parsing models: {[m.id for m in config.parsing_models]}\")\n",
    "print(f\"  Replicate count: {config.replicate_count}\")\n",
    "print(f\"  Rubric enabled: {config.rubric_enabled}\")\n",
    "print(f\"  Deep-judgment enabled: {config.deep_judgment_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preset Directory\n",
    "\n",
    "Specify a custom location for presets using an environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set custom preset directory\n",
    "custom_dir = TEMP_DIR / \"my_presets\"\n",
    "custom_dir.mkdir(exist_ok=True)\n",
    "\n",
    "os.environ[\"KARENINA_PRESETS_DIR\"] = str(custom_dir)\n",
    "\n",
    "print(f\"Custom preset directory set: {custom_dir}\")\n",
    "print(f\"\\nTo use: config.save_preset(name='My Preset') will save to this directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "\n",
    "Here's an end-to-end workflow showing preset creation and usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import VerificationConfig, ModelConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Create benchmark with genomics questions\n",
    "# ============================================================\n",
    "\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    description=\"Testing LLM knowledge of genomics and molecular biology\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add questions\n",
    "questions = [\n",
    "    (\"What is the approved drug target of Venetoclax?\", \"BCL2\"),\n",
    "    (\"How many chromosomes are in a human somatic cell?\", \"46\"),\n",
    "    (\"How many protein subunits does hemoglobin A have?\", \"4\"),\n",
    "]\n",
    "\n",
    "for question, answer in questions:\n",
    "    benchmark.add_question(\n",
    "        question=question,\n",
    "        raw_answer=answer,\n",
    "        author={\"name\": \"Genomics Curator\"}\n",
    "    )\n",
    "\n",
    "print(f\"\u2713 Created benchmark with {len(questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Configure models and settings\n",
    "# ============================================================\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# Configuration for testing (fast)\n",
    "test_config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    replicate_count=1,\n",
    "    rubric_enabled=False\n",
    ")\n",
    "\n",
    "# Configuration for production (comprehensive)\n",
    "# Note: rubric_enabled=True requires evaluation_mode='template_and_rubric'\n",
    "production_config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    replicate_count=3,\n",
    "    rubric_enabled=True,\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    deep_judgment_enabled=True,\n",
    "    abstention_enabled=True\n",
    ")\n",
    "\n",
    "print(\"\u2713 Configured test and production configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Save presets\n",
    "# ============================================================\n",
    "\n",
    "presets_dir = TEMP_DIR / \"presets\"\n",
    "\n",
    "# Save test configuration\n",
    "test_metadata = test_config.save_preset(\n",
    "    name=\"Quick Test\",\n",
    "    description=\"Fast configuration for smoke tests\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "print(f\"\u2713 Test preset saved: {test_metadata['filepath']}\")\n",
    "\n",
    "# Save production configuration\n",
    "prod_metadata = production_config.save_preset(\n",
    "    name=\"Production Full\",\n",
    "    description=\"Comprehensive configuration with all features enabled\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "print(f\"\u2713 Production preset saved: {prod_metadata['filepath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: Generate templates\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nGenerating templates...\")\n",
    "benchmark.generate_all_templates(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    model_provider=\"google_genai\"\n",
    ")\n",
    "print(\"\u2713 Templates generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: Run quick test using preset\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nRunning quick test...\")\n",
    "test_config = VerificationConfig.from_preset(\n",
    "    Path(test_metadata['filepath'])\n",
    ")\n",
    "test_results = benchmark.run_verification(test_config)\n",
    "print(f\"\u2713 Quick test complete: {len(test_results.results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6: Run production verification using preset\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nRunning production verification...\")\n",
    "prod_config = VerificationConfig.from_preset(\n",
    "    Path(prod_metadata['filepath'])\n",
    ")\n",
    "prod_results = benchmark.run_verification(prod_config)\n",
    "print(f\"\u2713 Production verification complete: {len(prod_results.results)} questions\")\n",
    "\n",
    "# Analyze results\n",
    "passed = sum(1 for r in prod_results.results if r.verify_result)\n",
    "total = len(prod_results.results)\n",
    "print(f\"Pass rate: {passed}/{total} ({passed/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final benchmark\n",
    "benchmark_path = TEMP_DIR / \"genomics_benchmark_final.jsonld\"\n",
    "benchmark.save(benchmark_path)\n",
    "print(f\"\\n\u2713 Benchmark saved to: {benchmark_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Presets\n",
    "\n",
    "### Listing Available Presets\n",
    "\n",
    "List all presets in the presets directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "presets_dir = TEMP_DIR / \"presets\"\n",
    "\n",
    "if presets_dir.exists():\n",
    "    preset_files = list(presets_dir.glob(\"*.json\"))\n",
    "    print(f\"Found {len(preset_files)} presets:\\n\")\n",
    "\n",
    "    for preset_file in preset_files:\n",
    "        with open(preset_file) as f:\n",
    "            data = json.load(f)\n",
    "            print(f\"  \ud83d\udcc4 {preset_file.name}\")\n",
    "            print(f\"     Name: {data['name']}\")\n",
    "            print(f\"     Description: {data.get('description', 'N/A')}\")\n",
    "            print(f\"     Created: {data['created_at']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No presets directory found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a Preset\n",
    "\n",
    "To update a preset, load it, modify the configuration, and save it with a new name (overwrites are not allowed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load existing preset\n",
    "preset_path = Path(prod_metadata['filepath'])\n",
    "config = VerificationConfig.from_preset(preset_path)\n",
    "\n",
    "# Show current settings\n",
    "print(\"Current settings:\")\n",
    "print(f\"  Replicate count: {config.replicate_count}\")\n",
    "print(f\"  Abstention enabled: {config.abstention_enabled}\")\n",
    "\n",
    "# Modify configuration\n",
    "config.replicate_count = 5  # Increase replication\n",
    "config.abstention_enabled = True  # Enable abstention detection\n",
    "\n",
    "# Save as a new preset (cannot overwrite existing)\n",
    "updated_metadata = config.save_preset(\n",
    "    name=\"Production Full Updated\",  # New name to avoid overwrite error\n",
    "    description=\"Updated with 5 replicates and abstention detection\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Preset updated: {updated_metadata['filepath']}\")\n",
    "print(f\"\\nNew settings:\")\n",
    "print(f\"  Replicate count: {config.replicate_count}\")\n",
    "print(f\"  Abstention enabled: {config.abstention_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting a Preset\n",
    "\n",
    "Delete a preset file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create a test preset to delete\n",
    "test_preset_path = presets_dir / \"old-config.json\"\n",
    "\n",
    "# First, create a dummy preset\n",
    "dummy_config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    replicate_count=1\n",
    ")\n",
    "dummy_config.save_preset(\n",
    "    name=\"Old Config\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "\n",
    "# Now delete it\n",
    "if test_preset_path.exists():\n",
    "    test_preset_path.unlink()\n",
    "    print(f\"\u2713 Deleted preset: {test_preset_path}\")\n",
    "else:\n",
    "    print(f\"Preset not found: {test_preset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Preset Scenarios\n",
    "\n",
    "### Scenario 1: Quick Test vs Full Evaluation\n",
    "\n",
    "Create two presets for different thoroughness levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: Minimal configuration for fast feedback\n",
    "quick_config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    replicate_count=1,\n",
    "    rubric_enabled=False,\n",
    "    deep_judgment_enabled=False,\n",
    "    abstention_enabled=False\n",
    ")\n",
    "\n",
    "quick_preset = quick_config.save_preset(\n",
    "    name=\"Quick Test Scenario\",\n",
    "    description=\"Fast smoke test configuration\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "\n",
    "# Full evaluation: Comprehensive configuration\n",
    "# Note: rubric_enabled=True requires evaluation_mode='template_and_rubric'\n",
    "full_config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    replicate_count=5,\n",
    "    rubric_enabled=True,\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    deep_judgment_enabled=True,\n",
    "    abstention_enabled=True,\n",
    "    deep_judgment_max_excerpts_per_attribute=3\n",
    ")\n",
    "\n",
    "full_preset = full_config.save_preset(\n",
    "    name=\"Full Evaluation Scenario\",\n",
    "    description=\"Comprehensive configuration with all features\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "\n",
    "print(\"\u2713 Created two presets:\")\n",
    "print(f\"  1. {quick_preset['name']} - Fast testing\")\n",
    "print(f\"  2. {full_preset['name']} - Full evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "print(\"Usage:\")\n",
    "print()\n",
    "print(\"# During development: Use quick test\")\n",
    "print(\"config = VerificationConfig.from_preset(Path('presets/quick-test.json'))\")\n",
    "print(\"dev_results = benchmark.run_verification(config)\")\n",
    "print()\n",
    "print(\"# Before release: Use full evaluation\")\n",
    "print(\"config = VerificationConfig.from_preset(Path('presets/full-evaluation.json'))\")\n",
    "print(\"final_results = benchmark.run_verification(config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Multi-Model Comparison\n",
    "\n",
    "Create a preset for comparing multiple models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "gpt4_mini = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\"\n",
    ")\n",
    "\n",
    "claude_sonnet = ModelConfig(\n",
    "    id=\"claude-sonnet-4\",\n",
    "    model_provider=\"anthropic\",\n",
    "    model_name=\"claude-sonnet-4\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# Multi-model comparison configuration\n",
    "# Note: rubric_enabled=True requires evaluation_mode='template_and_rubric'\n",
    "comparison_config = VerificationConfig(\n",
    "    answering_models=[gpt4_mini, claude_sonnet],  # Both models answer\n",
    "    parsing_models=[gpt4_mini],  # One model judges\n",
    "    replicate_count=3,\n",
    "    rubric_enabled=True,\n",
    "    evaluation_mode=\"template_and_rubric\"\n",
    ")\n",
    "\n",
    "comparison_preset = comparison_config.save_preset(\n",
    "    name=\"GPT-4 vs Claude Comparison\",\n",
    "    description=\"Compare GPT-4 and Claude on genomics questions\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "\n",
    "print(\"\u2713 Created multi-model comparison preset\")\n",
    "print(f\"  Answering models: {[m.id for m in comparison_config.answering_models]}\")\n",
    "print(f\"  Parsing models: {[m.id for m in comparison_config.parsing_models]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "print(\"Usage:\")\n",
    "print()\n",
    "print(\"# Load and run comparison\")\n",
    "print(\"config = VerificationConfig.from_preset(\")\n",
    "print(\"    Path('presets/gpt-4-vs-claude-comparison.json')\")\n",
    "print(\" )\")\n",
    "print(\"results = benchmark.run_verification(config)\")\n",
    "print()\n",
    "print(\"# Analyze by model\")\n",
    "print(\"for result in results.results:\")\n",
    "print(\"    print(f'Question: {result.metadata.question_text}')\")\n",
    "print(\"    print(f'  GPT-4: {result.template.parsed_llm_response}')\")\n",
    "print(\"    print(f'  Claude: {result.template.parsed_llm_response}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Feature-Specific Configurations\n",
    "\n",
    "Create presets for testing specific features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep-judgment focused\n",
    "deep_judgment_config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    replicate_count=1,\n",
    "    deep_judgment_enabled=True,\n",
    "    deep_judgment_max_excerpts_per_attribute=5,\n",
    "    deep_judgment_fuzzy_match_threshold=0.80\n",
    ")\n",
    "\n",
    "deep_judgment_preset = deep_judgment_config.save_preset(\n",
    "    name=\"Deep Judgment Test\",\n",
    "    description=\"Testing deep-judgment parsing with 5 excerpts per attribute\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "\n",
    "# Abstention detection focused\n",
    "abstention_config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    replicate_count=1,\n",
    "    abstention_enabled=True\n",
    ")\n",
    "\n",
    "abstention_preset = abstention_config.save_preset(\n",
    "    name=\"Abstention Detection Test\",\n",
    "    description=\"Testing abstention detection on safety questions\",\n",
    "    presets_dir=presets_dir\n",
    ")\n",
    "\n",
    "print(\"\u2713 Created feature-specific presets:\")\n",
    "print(f\"  1. {deep_judgment_preset['name']}\")\n",
    "print(f\"  2. {abstention_preset['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Use Descriptive Names\n",
    "\n",
    "**Good names:**\n",
    "\n",
    "- \"GPT-4 Production Config\"\n",
    "- \"Quick Smoke Test\"\n",
    "- \"Claude with Deep Judgment\"\n",
    "- \"Multi-Model Comparison Setup\"\n",
    "\n",
    "**Avoid:**\n",
    "\n",
    "- Vague names: \"Test 1\", \"Config\", \"Setup\"\n",
    "- Timestamp-only names: \"2025-11-03\"\n",
    "- Overly long names (keep under 50 characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add Meaningful Descriptions\n",
    "\n",
    "Include context about when and why to use the preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a good preset description\n",
    "print(\"Example:\")\n",
    "print(\"\"\"\n",
    "config.save_preset(\n",
    "    name=\"Production Genomics\",\n",
    "    description=\"Standard production configuration for genomics benchmarks. \"\n",
    "                \"Uses 3 replicates, enables rubrics and deep-judgment. \"\n",
    "                \"Suitable for final evaluations before publication.\"\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Organize by Purpose\n",
    "\n",
    "Create separate presets for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Development presets\")\n",
    "print('quick_test_config.save_preset(name=\"Dev: Quick Test\", description=\"...\")')\n",
    "print('debug_config.save_preset(name=\"Dev: Debug Mode\", description=\"...\")')\n",
    "print()\n",
    "print(\"# Production presets\")\n",
    "print('standard_config.save_preset(name=\"Prod: Standard\", description=\"...\")')\n",
    "print('comprehensive_config.save_preset(name=\"Prod: Comprehensive\", description=\"...\")')\n",
    "print()\n",
    "print(\"# Experiment presets\")\n",
    "print('ablation_config.save_preset(name=\"Exp: Ablation Study\", description=\"...\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Version Control Your Presets\n",
    "\n",
    "Track preset files in version control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Example Git commands for versioning presets\")\n",
    "print(\"git add presets/\")\n",
    "print(\"git commit -m 'Add genomics benchmark presets'\")\n",
    "print()\n",
    "print(\"This allows you to:\")\n",
    "print(\"  - Track changes to configurations over time\")\n",
    "print(\"  - Revert to previous configurations\")\n",
    "print(\"  - Share presets with teammates\")\n",
    "print(\"  - Document configuration evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test Presets After Loading\n",
    "\n",
    "Verify that loaded presets work as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify preset configuration\n",
    "config = VerificationConfig.from_preset(Path(full_preset['filepath']))\n",
    "\n",
    "# Verify configuration\n",
    "print(f\"Answering models: {len(config.answering_models)}\")\n",
    "print(f\"Parsing models: {len(config.parsing_models)}\")\n",
    "print(f\"Replicate count: {config.replicate_count}\")\n",
    "print(f\"Deep-judgment: {config.deep_judgment_enabled}\")\n",
    "print(f\"Abstention: {config.abstention_enabled}\")\n",
    "print()\n",
    "print(\"\u2713 Configuration verified successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Issue 1: Preset File Not Found\n",
    "\n",
    "**Symptom**: `FileNotFoundError` when loading preset\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "preset_path = Path(\"presets/my-config.json\")\n",
    "\n",
    "if not preset_path.exists():\n",
    "    print(f\"Preset not found: {preset_path}\")\n",
    "    print(\"Available presets:\")\n",
    "    for p in presets_dir.glob(\"*.json\"):\n",
    "        print(f\"  - {p.name}\")\n",
    "else:\n",
    "    config = VerificationConfig.from_preset(preset_path)\n",
    "    print(\"\u2713 Preset loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 2: Invalid Preset Configuration\n",
    "\n",
    "**Symptom**: `ValidationError` when loading preset\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Example: Inspect a valid preset file\n",
    "preset_path = Path(full_preset['filepath'])\n",
    "\n",
    "with open(preset_path) as f:\n",
    "    data = json.load(f)\n",
    "    print(\"Preset structure check:\")\n",
    "    print(f\"  \u2713 Has 'config' field: {'config' in data}\")\n",
    "    print(f\"  \u2713 Has 'answering_models' in config: {'answering_models' in data.get('config', {})}\")\n",
    "    print(f\"  \u2713 Has 'parsing_models' in config: {'parsing_models' in data.get('config', {})}\")\n",
    "    print(f\"  \u2713 Has 'name' field: {'name' in data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once you have presets configured, you can:\n",
    "\n",
    "- [Verification](../using-karenina/verification.md) - Run verifications with presets\n",
    "- [Deep-Judgment](deep-judgment.md) - Configure deep-judgment in presets\n",
    "- [Abstention Detection](abstention-detection.md) - Configure abstention in presets\n",
    "- [Few-Shot Prompting](few-shot.md) - Add few-shot configuration to presets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Documentation\n",
    "\n",
    "- [Verification](../using-karenina/verification.md) - Core verification workflow\n",
    "- [Saving and Loading](../using-karenina/saving-loading.md) - Checkpoint management\n",
    "- [Deep-Judgment](deep-judgment.md) - Multi-stage parsing\n",
    "- [Abstention Detection](abstention-detection.md) - Refusal detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}