{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import karenina modules after path is set\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultDeepJudgment,\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.drug_target = kwargs.get(\"drug_target\", \"BCL-2\")\n",
    "        self.mechanism = kwargs.get(\"mechanism\", \"Inhibits BCL-2 protein\")\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"BCL-2\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"BCL-2\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def create_mock_deep_judgment_result():\n",
    "    \"\"\"Create a mock deep-judgment result for demonstration.\"\"\"\n",
    "    return VerificationResultDeepJudgment(\n",
    "        deep_judgment_enabled=True,\n",
    "        deep_judgment_performed=True,\n",
    "        extracted_excerpts={\n",
    "            \"drug_target\": [{\"text\": \"targets the BCL-2 protein\", \"confidence\": \"high\", \"similarity_score\": 0.95}],\n",
    "            \"mechanism\": [\n",
    "                {\"text\": \"inhibits BCL-2, which promotes apoptosis\", \"confidence\": \"high\", \"similarity_score\": 0.92}\n",
    "            ],\n",
    "        },\n",
    "        attribute_reasoning={\n",
    "            \"drug_target\": \"The excerpt 'targets the BCL-2 protein' explicitly states BCL-2 as the target protein. This directly answers the question.\",\n",
    "            \"mechanism\": \"The response explains that venetoclax inhibits BCL-2, which promotes apoptosis in cancer cells. This describes the mechanism of action.\",\n",
    "        },\n",
    "        deep_judgment_stages_completed=[\"excerpts\", \"reasoning\", \"parameters\"],\n",
    "        deep_judgment_model_calls=3,\n",
    "        deep_judgment_excerpt_retry_count=0,\n",
    "        attributes_without_excerpts=[],\n",
    "        deep_judgment_search_enabled=False,\n",
    "        hallucination_risk_assessment=None,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_mock_verification_result(question_id: str, question_text: str, deep_judgment: bool = True):\n",
    "    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "\n",
    "    # Create mock template result\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=\"Venetoclax targets the BCL-2 protein, which is an anti-apoptotic protein. By inhibiting BCL-2, venetoclax promotes apoptosis in cancer cells.\",\n",
    "        parsed_llm_response={\"drug_target\": \"BCL-2\", \"mechanism\": \"Inhibits BCL-2\"},\n",
    "        parsed_gt_response={\"drug_target\": \"BCL-2\", \"mechanism\": \"Inhibits BCL-2\"},\n",
    "        verify_result=True,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80},\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "    )\n",
    "\n",
    "    # Create metadata with all required fields\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=\"BCL-2\",\n",
    "        answering_model=\"gpt-4.1-mini\",\n",
    "        parsing_model=\"gpt-4.1-mini\",\n",
    "        execution_time=2.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n",
    "    )\n",
    "\n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=None,\n",
    "        deep_judgment=create_mock_deep_judgment_result() if deep_judgment else None,\n",
    "    )\n",
    "\n",
    "\n",
    "# Store original run_verification\n",
    "_original_run_verification = None\n",
    "\n",
    "\n",
    "def mock_run_verification(self, config):\n",
    "    \"\"\"Mock run_verification that returns realistic deep-judgment results.\"\"\"\n",
    "    from karenina.schemas.workflow.template_results import TemplateResults\n",
    "    from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "\n",
    "    global _original_run_verification\n",
    "\n",
    "    # Get all finished questions\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "\n",
    "    if len(finished) == 0:\n",
    "        if _original_run_verification:\n",
    "            return _original_run_verification(self, config)\n",
    "        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "\n",
    "    results = []\n",
    "    # Check if deep judgment is enabled\n",
    "    deep_judgment_enabled = config.deep_judgment_enabled if hasattr(config, \"deep_judgment_enabled\") else False\n",
    "\n",
    "    for question in finished:\n",
    "        q_id = question[\"id\"]\n",
    "        q_text = question[\"question\"]\n",
    "\n",
    "        results.append(\n",
    "            create_mock_verification_result(question_id=q_id, question_text=q_text, deep_judgment=deep_judgment_enabled)\n",
    "        )\n",
    "\n",
    "    template_results = TemplateResults(results=results)\n",
    "\n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Patch Benchmark.run_verification\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(\"✓ Mock deep-judgment verification results enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Judgment Parsing\n",
    "\n",
    "Deep-judgment is an advanced parsing mode that provides enhanced transparency and accountability by extracting verbatim evidence from LLM responses before drawing conclusions. This guide explains what it is, when to use it, and how to configure it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep-Judgment?\n",
    "\n",
    "**Deep-judgment parsing** is a multi-stage evaluation process that goes beyond standard template-based verification. Instead of directly extracting structured data from LLM responses, deep-judgment performs a three-stage analysis:\n",
    "\n",
    "1. **Excerpt Extraction**: Identifies verbatim quotes that support each answer attribute\n",
    "2. **Reasoning Generation**: Explains how the excerpts map to attribute values\n",
    "3. **Parameter Extraction**: Extracts final structured values with full context\n",
    "\n",
    "This approach creates an **audit trail** showing exactly what evidence the LLM provided and how it was interpreted.\n",
    "\n",
    "### Standard Parsing vs Deep-Judgment\n",
    "\n",
    "**Standard Parsing** (default):\n",
    "```\n",
    "LLM Response → Parse Attributes → Verify Correctness\n",
    "```\n",
    "\n",
    "**Deep-Judgment Parsing**:\n",
    "```\n",
    "LLM Response → Extract Excerpts → Generate Reasoning → Parse Attributes → Verify Correctness\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Deep-Judgment?\n",
    "\n",
    "### 1. Transparency\n",
    "\n",
    "Every extracted attribute is backed by explicit evidence from the LLM response. You can see exactly which parts of the answer support each claim.\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Question: \"What is the approved drug target of Venetoclax?\"\n",
    "\n",
    "LLM Response: \"Venetoclax targets the BCL-2 protein, which is an anti-apoptotic\n",
    "protein. By inhibiting BCL-2, venetoclax promotes apoptosis in cancer cells.\"\n",
    "\n",
    "Standard Parsing:\n",
    "  drug_target: \"BCL-2\" ✓\n",
    "\n",
    "Deep-Judgment Parsing:\n",
    "  Excerpt: \"targets the BCL-2 protein\"\n",
    "  Reasoning: \"The response explicitly states BCL-2 as the target protein\"\n",
    "  drug_target: \"BCL-2\" ✓\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Deep-Judgment Works\n",
    "\n",
    "Deep-judgment uses a **three-stage autoregressive process** where each stage builds on the previous one:\n",
    "\n",
    "### Stage 1: Excerpt Extraction\n",
    "\n",
    "The parsing model identifies **verbatim quotes** from the LLM response that support each template attribute.\n",
    "\n",
    "**For each attribute**:\n",
    "\n",
    "- Extract 0-3 excerpts (configurable)\n",
    "- Assign confidence level: low/medium/high\n",
    "- Validate excerpts actually exist in the response (fuzzy matching)\n",
    "- If no excerpts found, request explanation from LLM\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Question: \"What is the approved drug target of Venetoclax?\"\n",
    "# Response: \"Venetoclax targets BCL-2, a key anti-apoptotic protein\"\n",
    "\n",
    "excerpts = {\n",
    "    \"drug_target\": [\n",
    "        {\n",
    "            \"text\": \"targets BCL-2\",\n",
    "            \"confidence\": \"high\",\n",
    "            \"similarity_score\": 0.95\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extracted excerpts structure\n",
    "excerpts = {\"drug_target\": [{\"text\": \"targets BCL-2\", \"confidence\": \"high\", \"similarity_score\": 0.95}]}\n",
    "print(\"Extracted Excerpts:\")\n",
    "for attr, exc_list in excerpts.items():\n",
    "    print(f\"\\nAttribute: {attr}\")\n",
    "    for exc in exc_list:\n",
    "        print(f\"  Text: {exc['text']}\")\n",
    "        print(f\"  Confidence: {exc['confidence']}\")\n",
    "        print(f\"  Similarity: {exc['similarity_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Reasoning Generation\n",
    "\n",
    "The parsing model explains how the excerpts from Stage 1 inform each attribute value.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "reasoning = {\n",
    "    \"drug_target\": \"The excerpt 'targets BCL-2' explicitly states BCL-2\n",
    "                  as the protein target. This directly answers the question.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Attribute reasoning\n",
    "reasoning = {\n",
    "    \"drug_target\": \"The excerpt 'targets BCL-2' explicitly states BCL-2 as the protein target.\",\n",
    "    \"mechanism\": \"The response explains that venetoclax inhibits BCL-2, which promotes apoptosis in cancer cells.\",\n",
    "}\n",
    "print(\"Attribute Reasoning:\")\n",
    "for attr, reason in reasoning.items():\n",
    "    print(f\"\\n{attr}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Parameter Extraction\n",
    "\n",
    "Using the reasoning context from Stage 2, the parsing model extracts structured attribute values using standard template parsing.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "parsed_answer = {\n",
    "    \"drug_target\": \"BCL-2\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Auto-Fail\n",
    "\n",
    "If any attribute has **missing excerpts** (no verbatim evidence found), verification **automatically fails** even if the final parsed answer seems correct. This ensures all claims are backed by explicit evidence.\n",
    "\n",
    "**Exception**: If abstention is detected (LLM refused to answer), auto-fail is skipped since abstention takes priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Deep-Judgment\n",
    "\n",
    "Deep-judgment is disabled by default. Enable it in your verification configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Configure models\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.0, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# Enable deep-judgment\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    deep_judgment_enabled=True,  # Enable deep-judgment parsing\n",
    ")\n",
    "\n",
    "print(\"Deep-judgment enabled:\", config.deep_judgment_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Configuration\n",
    "\n",
    "You can tune deep-judgment behavior with additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    # Deep-judgment configuration\n",
    "    deep_judgment_enabled=True,  # Enable feature\n",
    "    deep_judgment_max_excerpts_per_attribute=3,  # Excerpts per attribute (1-5)\n",
    "    deep_judgment_fuzzy_match_threshold=0.80,  # Similarity threshold (0.0-1.0)\n",
    "    deep_judgment_excerpt_retry_attempts=2,  # Retry attempts (0-5)\n",
    ")\n",
    "\n",
    "print(\"Deep-Judgment Configuration:\")\n",
    "print(f\"  Enabled: {config.deep_judgment_enabled}\")\n",
    "print(f\"  Max excerpts per attribute: {config.deep_judgment_max_excerpts_per_attribute}\")\n",
    "print(f\"  Fuzzy match threshold: {config.deep_judgment_fuzzy_match_threshold}\")\n",
    "print(f\"  Retry attempts: {config.deep_judgment_excerpt_retry_attempts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration Parameters**:\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `deep_judgment_enabled` | `bool` | `False` | Enable/disable deep-judgment parsing |\n",
    "| `deep_judgment_max_excerpts_per_attribute` | `int` | `3` | Maximum excerpts to extract per attribute (1-5) |\n",
    "| `deep_judgment_fuzzy_match_threshold` | `float` | `0.80` | Similarity threshold for excerpt validation (0.0-1.0). Higher = stricter. |\n",
    "| `deep_judgment_excerpt_retry_attempts` | `int` | `2` | Retry attempts when excerpt validation fails (0-5) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search-Enhanced Deep-Judgment\n",
    "\n",
    "**Search-enhanced deep-judgment** extends the standard three-stage process with an additional validation layer that checks extracted excerpts against external evidence sources. This helps detect potential hallucinations by verifying that the information in excerpts can be corroborated by external search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable search-enhanced deep-judgment\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config],\n",
    "    parsing_models=[model_config],\n",
    "    deep_judgment_enabled=True,  # Enable deep-judgment\n",
    "    deep_judgment_search_enabled=True,  # Enable search validation\n",
    "    deep_judgment_search_tool=\"tavily\",  # Use Tavily search (default)\n",
    ")\n",
    "\n",
    "print(\"Search-Enhanced Deep-Judgment:\")\n",
    "print(f\"  Deep-judgment enabled: {config.deep_judgment_enabled}\")\n",
    "print(f\"  Search enabled: {config.deep_judgment_search_enabled}\")\n",
    "print(f\"  Search tool: {config.deep_judgment_search_tool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Results\n",
    "\n",
    "When deep-judgment is enabled, verification results include additional metadata about the extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock result to show the structure\n",
    "mock_result = create_mock_verification_result(\n",
    "    question_id=\"q1\", question_text=\"What is the approved drug target of Venetoclax?\", deep_judgment=True\n",
    ")\n",
    "\n",
    "# Access deep-judgment results\n",
    "if mock_result.deep_judgment_performed:\n",
    "    print(\"Deep-Judgment Results:\")\n",
    "    print(\"\\nMetadata:\")\n",
    "    print(f\"  Question: {mock_result.question_text}\")\n",
    "    print(f\"  Stages Completed: {mock_result.deep_judgment_stages_completed}\")\n",
    "    print(f\"  Model Calls: {mock_result.deep_judgment_model_calls}\")\n",
    "    print(f\"  Excerpt Retries: {mock_result.deep_judgment_excerpt_retry_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display extracted excerpts\n",
    "if mock_result.extracted_excerpts:\n",
    "    print(\"\\nExtracted Excerpts:\")\n",
    "    for attr, excerpts in mock_result.extracted_excerpts.items():\n",
    "        print(f\"\\n  Attribute: {attr}\")\n",
    "        for exc in excerpts:\n",
    "            if exc.get(\"explanation\"):\n",
    "                # Missing excerpt with explanation\n",
    "                print(f\"    [Missing] {exc['explanation']}\")\n",
    "            else:\n",
    "                # Found excerpt\n",
    "                print(f'    Text: \"{exc[\"text\"]}\"')\n",
    "                print(f\"    Confidence: {exc['confidence']}\")\n",
    "                print(f\"    Similarity: {exc['similarity_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display reasoning traces\n",
    "if mock_result.attribute_reasoning:\n",
    "    print(\"\\nReasoning:\")\n",
    "    for attr, reasoning in mock_result.attribute_reasoning.items():\n",
    "        print(f\"  {attr}: {reasoning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show auto-fail status\n",
    "if mock_result.attributes_without_excerpts:\n",
    "    print(f\"\\n⚠️  AUTO-FAIL: Missing excerpts for {', '.join(mock_result.attributes_without_excerpts)}\")\n",
    "else:\n",
    "    print(\"\\n✓ All attributes have supporting excerpts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excerpt Structure\n",
    "\n",
    "Each excerpt includes:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"text\": str,              # Verbatim quote from response (empty if missing)\n",
    "    \"confidence\": str,        # \"low\" | \"medium\" | \"high\" | \"none\"\n",
    "    \"similarity_score\": float, # 0.0-1.0 (fuzzy match validation score)\n",
    "    \"explanation\": str        # Optional: why excerpt couldn't be found\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with excerpts\n",
    "example_with_excerpts = {\n",
    "    \"drug_target\": [\n",
    "        {\"text\": \"targets BCL-2 protein\", \"confidence\": \"high\", \"similarity_score\": 0.95},\n",
    "        {\"text\": \"inhibits BCL-2\", \"confidence\": \"medium\", \"similarity_score\": 0.87},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Example with excerpts:\")\n",
    "for attr, excerpts in example_with_excerpts.items():\n",
    "    print(f\"\\n{attr}:\")\n",
    "    for exc in excerpts:\n",
    "        print(f'  - \"{exc[\"text\"]}\" (confidence: {exc[\"confidence\"]}, similarity: {exc[\"similarity_score\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "### When to Use Deep-Judgment\n",
    "\n",
    "✅ **High-stakes evaluation** where evidence transparency is critical:\n",
    "\n",
    "- Medical diagnosis benchmarks\n",
    "- Legal document analysis\n",
    "- Scientific fact-checking\n",
    "- Regulatory compliance\n",
    "\n",
    "✅ **Debugging parsing failures**:\n",
    "\n",
    "- Understanding why verification fails\n",
    "- Identifying gaps in LLM responses\n",
    "- Refining question or template design\n",
    "\n",
    "✅ **Quality assurance**:\n",
    "\n",
    "- Ensuring responses contain sufficient evidence\n",
    "- Validating that answers aren't just plausible-sounding\n",
    "- Auditing LLM reasoning processes\n",
    "\n",
    "### When NOT to Use Deep-Judgment\n",
    "\n",
    "❌ **High-volume verification** where speed is critical:\n",
    "\n",
    "- Deep-judgment is 3-5x slower than standard parsing\n",
    "- Uses 3-5 LLM calls per question vs. 1 call for standard\n",
    "\n",
    "❌ **Low-stakes evaluation** where audit trails aren't needed:\n",
    "\n",
    "- Quick prototyping\n",
    "- Informal testing\n",
    "- Cost-sensitive applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "### Execution Time\n",
    "\n",
    "Deep-judgment significantly increases verification time:\n",
    "\n",
    "- **Standard parsing**: 1 LLM call per question (~500-2000ms)\n",
    "- **Deep-judgment parsing**: 3-5 LLM calls per question (~1500-10000ms)\n",
    "  - Stage 1 (excerpts): 1 call + retries\n",
    "  - Stage 2 (reasoning): 1 call\n",
    "  - Stage 3 (parameters): 1 call\n",
    "\n",
    "**Impact**: 3-5x slower than standard verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Tips\n",
    "\n",
    "### Fuzzy Match Threshold\n",
    "\n",
    "Controls how strictly excerpts must match the original response:\n",
    "\n",
    "```python\n",
    "# Lenient matching (accepts paraphrases)\n",
    "deep_judgment_fuzzy_match_threshold=0.70\n",
    "\n",
    "# Default matching (balanced)\n",
    "deep_judgment_fuzzy_match_threshold=0.80\n",
    "\n",
    "# Strict matching (only very close matches)\n",
    "deep_judgment_fuzzy_match_threshold=0.90\n",
    "```\n",
    "\n",
    "**Trade-offs**:\n",
    "\n",
    "- **Lower threshold (0.60-0.75)**: More lenient, may accept paraphrased excerpts\n",
    "- **Higher threshold (0.85-0.95)**: Stricter, only accepts near-exact matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Start with Standard Parsing\n",
    "\n",
    "Begin with standard parsing for your entire benchmark. Only enable deep-judgment when you need to:\n",
    "\n",
    "- Debug specific parsing failures\n",
    "- Audit high-stakes results\n",
    "- Understand model behavior\n",
    "\n",
    "### 2. Use Clear, Evidence-Based Templates\n",
    "\n",
    "Design templates that expect explicit evidence in responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Features\n",
    "\n",
    "Deep-judgment works alongside other advanced features:\n",
    "\n",
    "- **Abstention Detection**: Detects when LLMs refuse to answer. Takes priority over deep-judgment auto-fail.\n",
    "- **Rubrics**: Assess qualitative aspects. Use together with deep-judgment for comprehensive evaluation.\n",
    "- **Verification**: Core verification system. Deep-judgment enhances standard verification with evidence extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
