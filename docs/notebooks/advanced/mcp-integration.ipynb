{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": "# Mock Setup - Hidden in rendered documentation\n# This cell sets up mocking infrastructure for executable examples\n\nimport hashlib\nimport json\nimport sys\nimport tempfile\nfrom datetime import datetime\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\n# Add karenina to path\nsys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n\n# Temporary directory for file operations\nTEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n\n# Import after path is set\nfrom karenina.schemas.results import TemplateResults, VerificationResultSet\nfrom karenina.schemas.verification import (\n    VerificationResult,\n    VerificationResultMetadata,\n    VerificationResultRubric,\n    VerificationResultTemplate,\n)\n\n\n# Mock LLM response generator\nclass MockLLMResponse:\n    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n\n    def __init__(self, content: str = \"Mock response\"):\n        self.content = content\n        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n\n    def __str__(self):\n        return self.content\n\n\nclass MockStructuredOutput:\n    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n\n    def __init__(self, **kwargs):\n        # Set common attributes with realistic defaults\n        self.finding = kwargs.get(\"finding\", \"BCL2 inhibition shows promise in treating certain cancers\")\n        self.status = kwargs.get(\"status\", \"In clinical trials\")\n        self.gene = kwargs.get(\"gene\", \"BCL2\")\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n\n    def dict(self):\n        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n\n    def model_dump(self):\n        return self.dict()\n\n\ndef create_mock_chat_model():\n    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n    mock = MagicMock()\n    mock.invoke.return_value = MockLLMResponse(\"BCL2 inhibition shows promise in treating certain cancers\")\n    mock.ainvoke.return_value = MockLLMResponse(\"BCL2 inhibition shows promise in treating certain cancers\")\n    structured_mock = MagicMock()\n    structured_mock.invoke.return_value = MockStructuredOutput()\n    structured_mock.ainvoke.return_value = MockStructuredOutput()\n    mock.with_structured_output.return_value = structured_mock\n    mock.bind_tools.return_value = mock\n    return mock\n\n\ndef compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n    data = {\n        \"answering_mcp_servers\": [],\n        \"answering_model\": answering_model,\n        \"parsing_model\": parsing_model,\n        \"question_id\": question_id,\n        \"replicate\": None,\n        \"timestamp\": timestamp,\n    }\n    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n    return hash_obj.hexdigest()[:16]\n\n\ndef create_mock_verification_result(question_id: str, question_text: str, answer: str, passed: bool = True):\n    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n    timestamp = datetime.now().isoformat()\n    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n\n    # Create mock template result\n    template = VerificationResultTemplate(\n        raw_llm_response=f\"Based on web search, {answer}.\",\n        parsed_llm_response={\"finding\": answer},\n        parsed_gt_response={\"finding\": answer},\n        verify_result=passed,\n        template_verification_performed=True,\n        usage_metadata={\n            \"answer_generation\": {\"total_tokens\": 150},\n            \"parsing\": {\"total_tokens\": 30},\n            \"total\": {\"total_tokens\": 180},\n        },\n        abstention_check_performed=True,\n        abstention_detected=False,\n    )\n\n    # Create mock rubric result\n    rubric = VerificationResultRubric(\n        rubric_evaluation_performed=True,\n        llm_trait_scores={\n            \"Conciseness\": 4,\n            \"Clarity\": True,\n        },\n    )\n\n    # Create metadata with all required fields\n    metadata = VerificationResultMetadata(\n        question_id=question_id,\n        template_id=template_id,\n        completed_without_errors=True,\n        question_text=question_text,\n        raw_answer=answer,\n        answering_model=\"gpt-4.1-mini\",\n        parsing_model=\"gpt-4.1-mini\",\n        execution_time=2.5,\n        timestamp=timestamp,\n        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n    )\n\n    return VerificationResult(\n        metadata=metadata,\n        template=template,\n        rubric=rubric,\n    )\n\n\n# Store original run_verification\n_original_run_verification = None\n\n\ndef mock_run_verification(self, config):\n    \"\"\"Mock run_verification that returns realistic results.\"\"\"\n    global _original_run_verification\n\n    # Get all finished questions\n    finished = self.get_finished_questions(ids_only=False)\n\n    if len(finished) == 0:\n        if _original_run_verification:\n            return _original_run_verification(self, config)\n        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n\n    results = []\n    # Map question keywords to expected answers\n    mock_data = [\n        {\n            \"keywords\": [\"bcl2\", \"cancer\"],\n            \"answer\": \"BCL2 inhibition shows promise in treating certain cancers\",\n            \"passed\": True,\n        },\n        {\n            \"keywords\": [\"crispr\", \"hemoglobin\"],\n            \"answer\": \"CRISPR treatments for sickle cell disease are in clinical trials\",\n            \"passed\": True,\n        },\n    ]\n\n    for question in finished:\n        q_id = question[\"id\"]\n        q_text = question[\"question\"]\n        raw_answer = question.get(\"raw_answer\", \"\")\n\n        passed = True\n        mock_ans = raw_answer\n        q_text_lower = q_text.lower()\n\n        for data in mock_data:\n            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n                passed = data[\"passed\"]\n                mock_ans = data[\"answer\"]\n                break\n\n        results.append(\n            create_mock_verification_result(question_id=q_id, question_text=q_text, answer=mock_ans, passed=passed)\n        )\n\n    template_results = TemplateResults(results=results)\n\n    return VerificationResultSet(\n        results=results,\n        template_results=template_results,\n        rubric_results=None,\n    )\n\n\n# Mock MCP utilities for documentation examples\ndef mock_fetch_tool_descriptions(mcp_urls_dict, tool_filter=None):\n    \"\"\"Mock fetching tool descriptions from MCP servers.\"\"\"\n    return {\n        \"web_search\": \"Search the web for information\",\n        \"query_gene\": \"Query genomics database for gene information\",\n    }\n\n\n# Patch all LLM providers before any imports\n_llm_patches = [\n    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n]\n\nfor p in _llm_patches:\n    p.start()\n\n# Patch Benchmark.run_verification\nfrom karenina.benchmark import Benchmark\n\n_original_run_verification = Benchmark.run_verification\nBenchmark.run_verification = mock_run_verification\n\n# Patch MCP utilities\nimport karenina.utils.mcp.tools as mcp_tools_module\n\nmcp_tools_module.sync_fetch_tool_descriptions = mock_fetch_tool_descriptions\n\n\ndef temp_path(filename: str) -> Path:\n    \"\"\"Helper to create paths in temp directory.\"\"\"\n    return TEMP_DIR / filename\n\n\n# Cleanup\nimport atexit\nimport shutil\n\n\ndef _cleanup():\n    Benchmark.run_verification = _original_run_verification\n    for p in _llm_patches:\n        try:\n            p.stop()\n        except:\n            pass\n    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n\n\natexit.register(_cleanup)\n\nprint(\"✓ Mock setup complete\")\nprint(f\"✓ Temp directory: {TEMP_DIR}\")\nprint(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\nprint(\"✓ Mock verification results enabled - examples will show realistic output\")\nprint(\"✓ MCP utilities mocked for demonstration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Integration\n",
    "\n",
    "This guide explains how to integrate Model Context Protocol (MCP) servers to provide tool access for LLMs during verification.\n",
    "\n",
    "## What is MCP?\n",
    "\n",
    "**Model Context Protocol (MCP)** is a standardized protocol that enables LLMs to access external tools and data sources. MCP servers provide tools that LLMs can invoke during answer generation, such as:\n",
    "\n",
    "- Web search\n",
    "- Database queries\n",
    "- File system operations\n",
    "- API calls\n",
    "- Code execution\n",
    "- Custom domain-specific tools\n",
    "\n",
    "**Key Benefits**:\n",
    "\n",
    "- Extend LLM capabilities beyond text generation\n",
    "- Access real-time data during verification\n",
    "- Standardized tool invocation protocol\n",
    "- Modular tool integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use MCP with Karenina?\n",
    "\n",
    "MCP integration allows LLMs to access external information when answering benchmark questions.\n",
    "\n",
    "**Use Cases**:\n",
    "\n",
    "- **Current information**: Search web for recent drug approvals\n",
    "- **Database access**: Query genomics databases for gene information\n",
    "- **File operations**: Read configuration files or data files\n",
    "- **API integration**: Call external APIs for real-time data\n",
    "- **Custom tools**: Domain-specific tools for specialized benchmarks\n",
    "\n",
    "**Example**: A benchmark question asks \"What is the current FDA approval status of drug X?\" The LLM can use an MCP web search tool to find the latest information instead of relying on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Server Structure\n",
    "\n",
    "An MCP server provides:\n",
    "\n",
    "1. **Health Check**: Endpoint to verify server is running\n",
    "2. **Tool Discovery**: List available tools and their schemas\n",
    "3. **Tool Invocation**: Execute tools with parameters\n",
    "\n",
    "```\n",
    "MCP Server (http://localhost:3000/mcp)\n",
    "├── GET  /health          # Server status\n",
    "├── GET  /tools           # Available tools\n",
    "└── POST /invoke          # Execute a tool\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "Configure MCP integration via `ModelConfig` using the `mcp_urls_dict` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from karenina import Benchmark\nfrom karenina.schemas.config import ModelConfig\nfrom karenina.schemas.verification import VerificationConfig\n\n# Create benchmark\nbenchmark = Benchmark.create(\n    name=\"Genomics Benchmark\", description=\"Testing genomics knowledge with tool access\", version=\"1.0.0\"\n)\n\nbenchmark.add_question(\n    question=\"What is the latest research on BCL2 protein function?\",\n    raw_answer=\"BCL2 regulates apoptosis\",\n    author={\"name\": \"Research Curator\"},\n)\n\nprint(f\"✓ Created benchmark with {len(benchmark.get_question_ids())} question(s)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate templates (using mock model)\n",
    "# Note: generate_all_templates takes individual parameters, not a ModelConfig\n",
    "benchmark.generate_all_templates(model=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.0)\n",
    "print(\"✓ Templates generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure verification with MCP server using mcp_urls_dict in ModelConfig\n",
    "answering_model_with_mcp = ModelConfig(\n",
    "    id=\"agent-with-mcp\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\",\n",
    "    mcp_urls_dict={\"search_tools\": \"http://localhost:3000/mcp\"},\n",
    ")\n",
    "\n",
    "# Create parsing model config\n",
    "parsing_model = ModelConfig(id=\"parsing-model\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.0)\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[answering_model_with_mcp], parsing_models=[parsing_model], replicate_count=1\n",
    ")\n",
    "\n",
    "print(\"✓ Verification config created\")\n",
    "print(f\"  Answering model has MCP URLs: {answering_model_with_mcp.mcp_urls_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple MCP Servers\n",
    "\n",
    "Configure multiple MCP servers for different tool categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple MCP servers\n",
    "multi_mcp_model = ModelConfig(\n",
    "    id=\"multi-mcp-agent\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\",\n",
    "    mcp_urls_dict={\"search_tools\": \"http://localhost:3000/mcp\", \"database_tools\": \"http://localhost:3001/mcp\"},\n",
    ")\n",
    "\n",
    "print(\"✓ Model configured with multiple MCP servers:\")\n",
    "for name, url in multi_mcp_model.mcp_urls_dict.items():\n",
    "    print(f\"  - {name}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example MCP Tools\n",
    "\n",
    "### Web Search Tool\n",
    "\n",
    "Enables LLMs to search for current information.\n",
    "\n",
    "**Example usage**: LLM searches for \"latest BCL2 protein research\" to answer a genomics question with recent findings.\n",
    "\n",
    "### Database Query Tool\n",
    "\n",
    "Allows LLMs to query databases.\n",
    "\n",
    "**Example usage**: LLM queries database for \"BCL2\" to get official gene information, protein function, and chromosome location.\n",
    "\n",
    "### File Read Tool\n",
    "\n",
    "Enables LLMs to read data files.\n",
    "\n",
    "**Example usage**: LLM reads a drug-target database file to answer questions about approved therapeutics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "\n",
    "This example shows MCP integration for a genomics benchmark with web search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Create benchmark with questions requiring current data\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Current Genomics Research\", description=\"Testing knowledge of recent genomics discoveries\", version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Questions that benefit from tool access\n",
    "benchmark.add_question(\n",
    "    question=\"What are the latest findings on BCL2's role in cancer therapy?\",\n",
    "    raw_answer=\"BCL2 inhibition shows promise in treating certain cancers\",\n",
    "    author={\"name\": \"Oncology Researcher\"},\n",
    ")\n",
    "\n",
    "benchmark.add_question(\n",
    "    question=\"What is the current status of CRISPR therapies for hemoglobin disorders?\",\n",
    "    raw_answer=\"CRISPR treatments for sickle cell disease are in clinical trials\",\n",
    "    author={\"name\": \"Gene Therapy Researcher\"},\n",
    ")\n",
    "\n",
    "print(f\"✓ Created benchmark with {len(benchmark.get_question_ids())} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate templates (one-time)\n",
    "# Note: generate_all_templates takes individual parameters\n",
    "benchmark.generate_all_templates(model=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.0)\n",
    "print(\"✓ Templates generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Configure verification with MCP web search tool\n",
    "# (Assumes MCP server running at localhost:3000 with search tool)\n",
    "answering_model_with_mcp = ModelConfig(\n",
    "    id=\"web-search-agent\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\",\n",
    "    mcp_urls_dict={\n",
    "        \"web_search\": \"http://localhost:3000/mcp\"  # Web search MCP server\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create parsing model config\n",
    "parsing_model = ModelConfig(id=\"parsing-model\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.0)\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[answering_model_with_mcp], parsing_models=[parsing_model], replicate_count=1\n",
    ")\n",
    "\n",
    "print(\"✓ Verification configured with MCP web search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Run verification with tool access\n",
    "# LLM can search web for current information\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "print(f\"✓ Verification complete: {len(results.results)} result(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Analyze results\n",
    "for result in results.results:\n",
    "    question = benchmark.get_question(result.metadata.question_id)\n",
    "    print(f\"\\nQuestion: {question['question'][:80]}...\")\n",
    "    print(f\"Verification: {'✓ PASS' if result.template.verify_result else '✗ FAIL'}\")\n",
    "    print(f\"Tokens used: {result.template.usage_metadata['total']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating MCP Servers\n",
    "\n",
    "Before running verification, validate MCP server connectivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from karenina.utils.mcp import sync_fetch_tool_descriptions\n\n# Test MCP server connectivity by fetching tool descriptions\nserver_url = \"http://localhost:3000/mcp\"\ntry:\n    tool_descriptions = sync_fetch_tool_descriptions({\"local\": server_url})\n    print(\"✓ MCP server is reachable\")\n    print(f\"\\nAvailable tools ({len(tool_descriptions)}):\")\n    for name, desc in tool_descriptions.items():\n        print(f\"  - {name}: {desc[:60]}...\")\nexcept Exception as e:\n    print(\"✗ MCP server validation failed\")\n    print(f\"Error: {e}\")\n    print(\"Ensure server is running and accessible\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering Available Tools\n",
    "\n",
    "Query an MCP server to see what tools it provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from karenina.utils.mcp import sync_fetch_tool_descriptions\n\n# Discover tools from MCP server\nserver_url = \"http://localhost:3000/mcp\"\ntool_descriptions = sync_fetch_tool_descriptions({\"local\": server_url})\n\nprint(f\"Discovered {len(tool_descriptions)} tools:\\n\")\nfor name, description in tool_descriptions.items():\n    print(f\"Tool: {name}\")\n    print(f\"Description: {description}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "### Use Case 1: Current Information Access\n",
    "\n",
    "**Scenario**: Benchmark tests LLM knowledge of recent drug approvals.\n",
    "\n",
    "**Setup**:\n",
    "- Deploy MCP server with web search tool\n",
    "- Configure verification with MCP server URL\n",
    "- Questions ask about recent FDA approvals\n",
    "\n",
    "**Benefit**: LLM can search for current information instead of relying on training data cutoff.\n",
    "\n",
    "### Use Case 2: Database Integration\n",
    "\n",
    "**Scenario**: Questions require querying a genomics database.\n",
    "\n",
    "**Setup**:\n",
    "- Deploy MCP server with database query tool\n",
    "- Configure database connection in MCP server\n",
    "- Questions ask about specific genes\n",
    "\n",
    "**Benefit**: LLM gets accurate, up-to-date gene information from authoritative database.\n",
    "\n",
    "### Use Case 3: File-Based Data\n",
    "\n",
    "**Scenario**: Benchmark uses data files with drug-target mappings.\n",
    "\n",
    "**Setup**:\n",
    "- Deploy MCP server with file read tool\n",
    "- Store drug-target data in structured files\n",
    "- Configure file system access permissions\n",
    "\n",
    "**Benefit**: LLM reads data files to answer questions accurately without relying on memorized facts.\n",
    "\n",
    "### Use Case 4: API Integration\n",
    "\n",
    "**Scenario**: Questions require real-time API data.\n",
    "\n",
    "**Setup**:\n",
    "- Deploy MCP server with API call tools\n",
    "- Configure API keys and endpoints\n",
    "- Questions ask about live data\n",
    "\n",
    "**Benefit**: LLM calls APIs to fetch current data during verification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple MCP Server\n",
    "\n",
    "Example MCP server with a genomics database query tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example MCP server code (not executed - for reference only)\n",
    "print(\"\"\"\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Mock genomics database\n",
    "GENOMICS_DB = {\n",
    "    \"BCL2\": {\n",
    "        \"full_name\": \"B-cell lymphoma 2\",\n",
    "        \"chromosome\": \"18\",\n",
    "        \"function\": \"Regulates apoptosis\"\n",
    "    },\n",
    "    \"HBB\": {\n",
    "        \"full_name\": \"Hemoglobin subunit beta\",\n",
    "        \"chromosome\": \"11\",\n",
    "        \"function\": \"Oxygen transport\"\n",
    "    }\n",
    "}\n",
    "\n",
    "class ToolInvocation(BaseModel):\n",
    "    tool_name: str\n",
    "    parameters: dict\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.get(\"/tools\")\n",
    "def list_tools():\n",
    "    return {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"name\": \"query_gene\",\n",
    "                \"description\": \"Query genomics database for gene information\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"gene_symbol\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Gene symbol (e.g., BCL2, HBB)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"gene_symbol\"]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.post(\"/invoke\")\n",
    "def invoke_tool(invocation: ToolInvocation):\n",
    "    if invocation.tool_name == \"query_gene\":\n",
    "        gene = invocation.parameters.get(\"gene_symbol\", \"\").upper()\n",
    "        if gene in GENOMICS_DB:\n",
    "            return {\"result\": GENOMICS_DB[gene]}\n",
    "        return {\"error\": f\"Gene {gene} not found in database\"}\n",
    "    return {\"error\": \"Unknown tool\"}\n",
    "\n",
    "# Run: uvicorn mcp_server:app --port 3000\n",
    "\"\"\")\n",
    "\n",
    "print(\"# Example usage:\")\n",
    "print(\"# Start MCP server:\")\n",
    "print(\"#   uvicorn mcp_server:app --port 3000\")\n",
    "print(\"\")\n",
    "print(\"# In another terminal, run Karenina verification with MCP:\")\n",
    "print(\"#   python verify_with_mcp.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic Prompt Caching\n",
    "\n",
    "When using Anthropic models (Claude) with MCP tools via the `langchain` interface, **prompt caching is enabled by default** to reduce costs and latency. This caches repetitive prompt content like system prompts, tool definitions, and conversation history on Anthropic's servers.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **First request**: System prompt, tools, and the user message are sent to the API and cached\n",
    "2. **Subsequent requests**: Cached content is retrieved rather than reprocessed\n",
    "3. **Cache expiration**: Content expires after the TTL (5 minutes or 1 hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Prompt caching is configured via `AgentMiddlewareConfig` in `ModelConfig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from karenina.schemas.config import AgentMiddlewareConfig, ModelConfig, PromptCachingConfig\n\nmodel_config = ModelConfig(\n    id=\"cached-claude\",\n    model_provider=\"anthropic\",\n    model_name=\"claude-sonnet-4-5-20250929\",\n    temperature=0.0,\n    interface=\"langchain\",\n    mcp_urls_dict={\"biocontext\": \"https://mcp.biocontext.ai/mcp/\"},\n    agent_middleware=AgentMiddlewareConfig(\n        prompt_caching=PromptCachingConfig(\n            enabled=True,  # Default: True for Anthropic models\n            ttl=\"5m\",  # Cache lifetime: \"5m\" (5 minutes) or \"1h\" (1 hour)\n            min_messages_to_cache=0,  # Min messages before caching starts\n            unsupported_model_behavior=\"warn\",  # \"ignore\", \"warn\", or \"raise\"\n        )\n    ),\n)\n\nprint(\"✓ Model configured with Anthropic prompt caching\")\nprint(f\"  Caching enabled: {model_config.agent_middleware.prompt_caching.enabled}\")\nprint(f\"  TTL: {model_config.agent_middleware.prompt_caching.ttl}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Options\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `enabled` | `True` | Enable/disable prompt caching |\n",
    "| `ttl` | `\"5m\"` | Cache time-to-live: `\"5m\"` or `\"1h\"` |\n",
    "| `min_messages_to_cache` | `0` | Minimum messages before caching activates |\n",
    "| `unsupported_model_behavior` | `\"warn\"` | Behavior for non-Anthropic models |\n",
    "\n",
    "### Disabling Prompt Caching\n",
    "\n",
    "To disable prompt caching for Anthropic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable prompt caching\n",
    "no_cache_model = ModelConfig(\n",
    "    id=\"no-cache-claude\",\n",
    "    model_provider=\"anthropic\",\n",
    "    model_name=\"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\",\n",
    "    mcp_urls_dict={\"biocontext\": \"https://mcp.biocontext.ai/mcp/\"},\n",
    "    agent_middleware=AgentMiddlewareConfig(prompt_caching=PromptCachingConfig(enabled=False)),\n",
    ")\n",
    "\n",
    "print(\"✓ Model configured with prompt caching disabled\")\n",
    "print(f\"  Caching enabled: {no_cache_model.agent_middleware.prompt_caching.enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "- **Provider**: `anthropic` only\n",
    "- **Interface**: `langchain` only\n",
    "- **Dependency**: `langchain-anthropic` must be installed\n",
    "\n",
    "Prompt caching does **not** provide conversation memory - it only reduces API costs by caching tokens. For conversation persistence, use a checkpointer.\n",
    "\n",
    "See the [LangChain documentation](https://docs.langchain.com/oss/python/integrations/middleware/anthropic#prompt-caching) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Server Configuration\n",
    "\n",
    "**Do**:\n",
    "\n",
    "- Validate MCP server before verification\n",
    "- Use HTTPS in production\n",
    "- Implement authentication for MCP servers\n",
    "- Set appropriate timeout limits\n",
    "- Log tool invocations for debugging\n",
    "\n",
    "**Don't**:\n",
    "\n",
    "- Expose MCP servers publicly without authentication\n",
    "- Allow unrestricted file system access\n",
    "- Skip server validation before use\n",
    "- Use untrusted MCP servers\n",
    "\n",
    "### Tool Design\n",
    "\n",
    "**Do**:\n",
    "\n",
    "- Provide clear tool descriptions\n",
    "- Use typed parameters with JSON schema\n",
    "- Return structured data\n",
    "- Handle errors gracefully\n",
    "- Document tool capabilities\n",
    "\n",
    "**Don't**:\n",
    "\n",
    "- Create tools with side effects (prefer read-only)\n",
    "- Skip parameter validation\n",
    "- Return unstructured text\n",
    "- Allow dangerous operations without safeguards\n",
    "\n",
    "### Security\n",
    "\n",
    "**Do**:\n",
    "\n",
    "- Validate all tool parameters\n",
    "- Restrict tool permissions (principle of least privilege)\n",
    "- Implement rate limiting\n",
    "- Monitor tool usage\n",
    "- Use network firewalls\n",
    "\n",
    "**Don't**:\n",
    "\n",
    "- Trust tool input without validation\n",
    "- Grant excessive permissions\n",
    "- Skip logging\n",
    "- Ignore security warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n### Issue: MCP Server Not Reachable\n\n**Error**: `Connection refused` or timeout errors\n\n**Cause**: MCP server not running or wrong URL.\n\n**Solutions**:\n```bash\n# Check server is running\ncurl http://localhost:3000/mcp/health\n\n# Verify port and URL\nps aux | grep mcp\n\n# Check firewall rules\n```\n\n### Issue: Tool Not Available\n\n**Error**: `Tool 'web_search' not found`\n\n**Cause**: Tool not registered in MCP server.\n\n**Solution**:\n```bash\n# List available tools\ncurl http://localhost:3000/mcp/tools\n\n# Verify tool name spelling matches exactly\n```\n\n### Issue: Tool Invocation Fails\n\n**Error**: `Tool invocation failed: invalid parameters`\n\n**Cause**: Parameters don't match tool schema.\n\n**Solution**:\n```python\n# Check available tools from MCP server\nfrom karenina.utils.mcp import sync_fetch_tool_descriptions\n\ntool_descriptions = sync_fetch_tool_descriptions({\"local\": \"http://localhost:3000/mcp\"})\nfor name, desc in tool_descriptions.items():\n    print(f\"{name}: {desc}\")\n```\n\n### Issue: Verification Slower with MCP\n\n**Symptom**: Verification takes much longer with MCP enabled.\n\n**Cause**: Tool invocations add latency.\n\n**Solution**:\n\n- Use faster MCP servers (local is better than remote)\n- Cache tool results when possible\n- Reduce network latency\n- Set appropriate timeouts"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "**Current Limitations**:\n",
    "\n",
    "- MCP integration primarily designed for server/GUI deployment\n",
    "- Standalone library support is experimental\n",
    "- Tool invocation tracking may be limited\n",
    "- Some providers may not support function calling\n",
    "\n",
    "**Best Use**:\n",
    "\n",
    "- Use with karenina-server and karenina-gui for full features\n",
    "- Standalone library works but with reduced visibility into tool usage\n",
    "- Consider manual traces for reproducible testing instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "MCP integration enables:\n",
    "\n",
    "1. **Tool access** - LLMs can use external tools during verification\n",
    "2. **Current data** - Access real-time information beyond training data\n",
    "3. **Database queries** - Query structured databases\n",
    "4. **File operations** - Read data from files\n",
    "5. **API calls** - Integrate with external APIs\n",
    "\n",
    "**Configure** by setting `mcp_urls_dict` in `ModelConfig` and ensure MCP server is running and accessible.\n",
    "\n",
    "**Note**: MCP integration is most powerful when used with the full karenina-server and karenina-gui stack. Standalone library support exists but is more limited."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
