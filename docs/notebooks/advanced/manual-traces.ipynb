{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        self.subunits = kwargs.get(\"subunits\", 4)\n",
    "        self.diseases = kwargs.get(\"diseases\", [\"asthma\", \"bronchitis\", \"pneumonia\"])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def create_mock_verification_result(question_id: str, question_text: str, answer: str, passed: bool = True):\n",
    "    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=f\"The answer is {answer}.\",\n",
    "        parsed_llm_response={\"value\": answer},\n",
    "        parsed_gt_response={\"value\": answer},\n",
    "        verify_result=passed,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80},\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "    )\n",
    "\n",
    "    rubric = VerificationResultRubric(\n",
    "        rubric_evaluation_performed=True,\n",
    "        llm_trait_scores={\n",
    "            \"Conciseness\": 4,\n",
    "            \"Clarity\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=answer,\n",
    "        answering_model=\"gpt-4.1-mini\",\n",
    "        parsing_model=\"gpt-4.1-mini\",\n",
    "        execution_time=1.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n",
    "    )\n",
    "\n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "\n",
    "# Store original run_verification\n",
    "_original_run_verification = None\n",
    "\n",
    "\n",
    "def mock_run_verification(self, config):\n",
    "    \"\"\"Mock run_verification that returns realistic results.\"\"\"\n",
    "    global _original_run_verification\n",
    "\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "\n",
    "    if len(finished) == 0:\n",
    "        if _original_run_verification:\n",
    "            return _original_run_verification(self, config)\n",
    "        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "\n",
    "    results = []\n",
    "    mock_data = [\n",
    "        {\"keywords\": [\"chromosomes\"], \"answer\": \"46\", \"passed\": True},\n",
    "        {\"keywords\": [\"venetoclax\", \"bcl2\"], \"answer\": \"BCL2\", \"passed\": True},\n",
    "        {\"keywords\": [\"hemoglobin\", \"subunits\"], \"answer\": \"4\", \"passed\": True},\n",
    "        {\"keywords\": [\"inflammatory\", \"lung\"], \"answer\": \"asthma, bronchitis, pneumonia\", \"passed\": True},\n",
    "        {\"keywords\": [\"2+2\", \"two plus two\"], \"answer\": \"4\", \"passed\": True},\n",
    "        {\"keywords\": [\"3+3\", \"three plus three\"], \"answer\": \"6\", \"passed\": True},\n",
    "    ]\n",
    "\n",
    "    for question in finished:\n",
    "        q_id = question[\"id\"]\n",
    "        q_text = question[\"question\"]\n",
    "        raw_answer = question.get(\"raw_answer\", \"\")\n",
    "\n",
    "        passed = True\n",
    "        mock_ans = raw_answer\n",
    "        q_text_lower = q_text.lower()\n",
    "\n",
    "        for data in mock_data:\n",
    "            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n",
    "                passed = data[\"passed\"]\n",
    "                mock_ans = data[\"answer\"]\n",
    "                break\n",
    "\n",
    "        results.append(\n",
    "            create_mock_verification_result(question_id=q_id, question_text=q_text, answer=mock_ans, passed=passed)\n",
    "        )\n",
    "\n",
    "    template_results = TemplateResults(results=results)\n",
    "\n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Patch Benchmark.run_verification\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(\"✓ Mock verification results enabled - examples will show realistic output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Trace System\n",
    "\n",
    "This guide explains how to use the Manual Trace System to evaluate pre-generated LLM responses without making live API calls during verification.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Manual Trace System enables you to provide pre-generated answer traces directly to the Karenina verification engine, bypassing the real-time LLM answer generation step. This feature is designed for standalone backend usage, allowing you to:\n",
    "\n",
    "- Evaluate pre-recorded LLM responses (from previous experiments, other systems, or manual collection)\n",
    "- Compare different answer generation approaches without re-running models\n",
    "- Test verification/rubric systems with controlled answers\n",
    "- Integrate external LLM outputs into Karenina's evaluation framework\n",
    "\n",
    "The system supports both simple string traces and rich LangChain message lists with automatic tool call metrics extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Capabilities\n",
    "\n",
    "- **Programmatic Trace Management**: `ManualTraces` class for managing traces tied to benchmarks\n",
    "- **Flexible Registration**: Register traces by question hash (MD5) or question text with automatic mapping\n",
    "- **Dual Format Support**:\n",
    "  - Simple string traces (plain text answers)\n",
    "  - LangChain message lists (AIMessage, ToolMessage, etc.) with automatic preprocessing\n",
    "- **Agent Metrics Extraction**: Automatic tool call counting, failure detection from message lists\n",
    "- **Batch Registration**: Efficient bulk trace registration with `register_traces()`\n",
    "- **Post-Config Population**: Populate traces after `ModelConfig` creation for flexible workflows\n",
    "- **Preset Compatibility**: Manual configs work with preset system (traces excluded from serialization)\n",
    "- **Session-Based Storage**: Thread-safe, time-bounded trace storage with automatic cleanup\n",
    "- **Backward Compatible**: Maintains compatibility with existing GUI-based manual trace upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "### Basic Usage with String Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.benchmark import Benchmark\n",
    "from karenina.infrastructure.llm.manual_traces import ManualTraces\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Note: Using finished=True instead of answer_template for notebook examples\n",
    "# This avoids source code extraction issues with dynamically defined classes\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark(\"my_experiment\")\n",
    "benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)  # Question marked as finished\n",
    "\n",
    "# Initialize manual traces\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "\n",
    "# Register trace by question text\n",
    "manual_traces.register_trace(\"What is 2+2?\", \"The answer is 4. I computed this by adding 2 and 2.\", map_to_id=True)\n",
    "\n",
    "print(f\"Registered {len(manual_traces._benchmark._questions_cache)} traces\")\n",
    "print(f\"Question text maps to hash: {list(manual_traces._benchmark._questions_cache.keys())[0][:32]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manual config\n",
    "manual_config = ModelConfig(interface=\"manual\", manual_traces=manual_traces)\n",
    "\n",
    "# Create judge config\n",
    "judge_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,\n",
    "    interface=\"langchain\",\n",
    "    system_prompt=\"You are an expert judge.\",\n",
    ")\n",
    "\n",
    "print(f\"Manual config ID: {manual_config.id}\")\n",
    "print(f\"Manual config interface: {manual_config.interface}\")\n",
    "print(f\"Judge config: {judge_config.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create verification config\n",
    "config = VerificationConfig(answering_models=[manual_config], parsing_models=[judge_config])\n",
    "\n",
    "# Run verification\n",
    "results = benchmark.run_verification(config)\n",
    "\n",
    "print(\"Verification complete!\")\n",
    "print(f\"Results: {len(results.results)} question(s) evaluated\")\n",
    "print(f\"Passed: {sum(1 for r in results.results if r.template.verify_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "### Core Components\n",
    "\n",
    "#### 1. `ManualTraces` Class\n",
    "**Location**: `karenina/src/karenina/infrastructure/llm/manual_traces.py`\n",
    "\n",
    "**Purpose**: High-level API for managing manual traces for a specific benchmark\n",
    "\n",
    "**Key Methods**:\n",
    "- `__init__(benchmark)` - Initialize with benchmark for question mapping\n",
    "- `register_trace(question_identifier, trace, map_to_id=False)` - Register single trace\n",
    "- `register_traces(traces_dict, map_to_id=False)` - Batch register traces\n",
    "- `_question_text_to_hash(question_text)` - Convert text to MD5 hash with validation\n",
    "- `_preprocess_trace(trace)` - Handle both string and LangChain message formats\n",
    "\n",
    "#### 2. `ManualTraceManager` Class\n",
    "**Location**: `karenina/src/karenina/infrastructure/llm/manual_traces.py`\n",
    "\n",
    "**Purpose**: Session-based thread-safe storage for manual traces\n",
    "\n",
    "**Key Features**:\n",
    "- Thread-safe storage with `threading.RLock()`\n",
    "- Session timeout (default: 1 hour) with automatic cleanup\n",
    "- Storage for both traces and agent metrics\n",
    "- MD5 hash validation\n",
    "\n",
    "#### 3. `ManualLLM` Class\n",
    "**Location**: `karenina/src/karenina/infrastructure/llm/manual_llm.py`\n",
    "\n",
    "**Purpose**: LangChain-compatible LLM that returns precomputed traces\n",
    "\n",
    "**Key Methods**:\n",
    "- `invoke(messages)` - Return precomputed trace as AIMessage\n",
    "- `get_agent_metrics()` - Retrieve agent metrics for trace\n",
    "- `with_structured_output(schema)` - Compatibility method\n",
    "\n",
    "#### 4. `ModelConfig` Integration\n",
    "\n",
    "**New Field**: `manual_traces: Any = Field(default=None, exclude=True)`\n",
    "\n",
    "**Validation**:\n",
    "- Enforces `manual_traces` requirement for manual interface\n",
    "- Auto-sets `id=\"manual\"` and `model_name=\"manual\"` for manual interface\n",
    "- Validates that MCP tools are not used with manual interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Workflows\n",
    "\n",
    "### Workflow 1: Basic String Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.benchmark import Benchmark\n",
    "from karenina.infrastructure.llm.manual_traces import ManualTraces\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark(\"simple_example\")\n",
    "benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)\n",
    "benchmark.add_question(question=\"What is 3+3?\", raw_answer=\"6\", finished=True)\n",
    "\n",
    "# Initialize manual traces\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "\n",
    "# Register traces by question text\n",
    "manual_traces.register_trace(\"What is 2+2?\", \"The answer is 4. I added 2 and 2 to get 4.\", map_to_id=True)\n",
    "\n",
    "manual_traces.register_trace(\"What is 3+3?\", \"The answer is 6. I added 3 and 3 to get 6.\", map_to_id=True)\n",
    "\n",
    "print(f\"Registered {len(list(benchmark.get_finished_questions()))} traces\")\n",
    "for q in benchmark.get_finished_questions(ids_only=False):\n",
    "    print(f\"  - {q['question'][:30]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manual config\n",
    "manual_config = ModelConfig(interface=\"manual\", manual_traces=manual_traces)\n",
    "\n",
    "# Create judge config\n",
    "judge_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.0, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# Run verification\n",
    "config = VerificationConfig(answering_models=[manual_config], parsing_models=[judge_config])\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Evaluated {len(results.results)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow 2: LangChain Message Lists with Tool Calls\n",
    "\n",
    "When you use LangChain message lists (with `AIMessage`, `ToolMessage`, etc.), the system automatically extracts agent metrics like tool call counts and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "from karenina.infrastructure.llm.manual_traces import ManualTraces\n",
    "\n",
    "# Create a new benchmark for this example\n",
    "benchmark = Benchmark(\"agent_workflow\")\n",
    "benchmark.add_question(question=\"What is 6 times 7?\", raw_answer=\"42\", finished=True)\n",
    "\n",
    "# Initialize manual traces\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "\n",
    "# Register trace with tool calls\n",
    "messages = [\n",
    "    AIMessage(content=\"I need to calculate this\"),\n",
    "    ToolMessage(name=\"calculator\", content=\"Result: 42\", tool_call_id=\"call_calc_001\"),\n",
    "    ToolMessage(name=\"validator\", content=\"Validation passed\", tool_call_id=\"call_valid_002\"),\n",
    "    AIMessage(content=\"The answer is 42. I verified this using a calculator and validator.\"),\n",
    "]\n",
    "\n",
    "manual_traces.register_trace(\"What is 6 times 7?\", messages, map_to_id=True)\n",
    "\n",
    "print(\"Registered trace with tool calls\")\n",
    "print(\"Agent metrics automatically extracted:\")\n",
    "print(\"  - tool_calls: 2\")\n",
    "print(\"  - unique_tools_used: 2 (calculator, validator)\")\n",
    "print(\"  - iterations: 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow 3: Batch Registration\n",
    "\n",
    "For efficiency, you can register multiple traces at once using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark(\"batch_example\")\n",
    "benchmark.add_question(question=\"Question 1?\", raw_answer=\"Answer 1\", finished=True)\n",
    "benchmark.add_question(question=\"Question 2?\", raw_answer=\"Answer 2\", finished=True)\n",
    "benchmark.add_question(question=\"Question 3?\", raw_answer=\"Answer 3\", finished=True)\n",
    "\n",
    "# Initialize manual traces\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "\n",
    "# Prepare traces dictionary\n",
    "traces = {\n",
    "    \"Question 1?\": \"Answer 1 with explanation.\",\n",
    "    \"Question 2?\": [\n",
    "        AIMessage(content=\"Thinking...\"),\n",
    "        ToolMessage(name=\"tool\", content=\"data\", tool_call_id=\"call_1\"),\n",
    "        AIMessage(content=\"Answer 2 with context.\"),\n",
    "    ],\n",
    "    \"Question 3?\": \"Answer 3 with details.\",\n",
    "}\n",
    "\n",
    "# Batch register all at once\n",
    "manual_traces.register_traces(traces, map_to_id=True)\n",
    "\n",
    "print(f\"Batch registered {len(traces)} traces\")\n",
    "print(\"All traces now available for verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow 4: Register by Question Hash\n",
    "\n",
    "You can also register traces using the MD5 hash directly instead of question text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark\n",
    "benchmark = Benchmark(\"hash_example\")\n",
    "benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)\n",
    "\n",
    "# Initialize manual traces\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "\n",
    "# Compute hash manually or get from CSV mapper export\n",
    "question_hash = hashlib.md5(b\"What is 2+2?\").hexdigest()\n",
    "\n",
    "print(f\"Question hash: {question_hash}\")\n",
    "\n",
    "# Register by hash (map_to_id=False, default)\n",
    "manual_traces.register_trace(question_hash, \"The answer is 4.\", map_to_id=False)\n",
    "\n",
    "print(\"Registered trace using hash directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow 5: Populate Traces After Config Creation\n",
    "\n",
    "You can create the config structure first and populate traces later, enabling flexible workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark\n",
    "benchmark = Benchmark(\"delayed_population\")\n",
    "benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)\n",
    "\n",
    "# 1. Create ManualTraces and ModelConfig upfront\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "manual_config = ModelConfig(interface=\"manual\", manual_traces=manual_traces)\n",
    "\n",
    "print(\"Created config with empty traces\")\n",
    "\n",
    "\n",
    "# 2. Later, populate traces (e.g., from file, database, API)\n",
    "def load_traces_from_source():\n",
    "    return {\"What is 2+2?\": \"The answer is 4.\"}\n",
    "\n",
    "\n",
    "for question_text, trace_content in load_traces_from_source().items():\n",
    "    manual_traces.register_trace(question_text, trace_content, map_to_id=True)\n",
    "\n",
    "print(\"Populated traces from external source\")\n",
    "\n",
    "# 3. Run verification with populated traces\n",
    "judge_config = ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "\n",
    "config = VerificationConfig(answering_models=[manual_config], parsing_models=[judge_config])\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Verification complete: {len(results.results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### Question Mapping\n",
    "\n",
    "**Hash Generation**:\n",
    "- Uses MD5 hash of UTF-8 encoded question text: `hashlib.md5(question_text.encode(\"utf-8\")).hexdigest()`\n",
    "- Same algorithm as `Question.id` property in `schemas/domain/question.py`\n",
    "- Results in 32-character hexadecimal string\n",
    "\n",
    "**Validation**:\n",
    "- When `map_to_id=True`, question text is searched in benchmark's `_questions_cache`\n",
    "- Raises `ValueError` if question not found, with computed hash and available count\n",
    "- Exact text matching (case-sensitive, including whitespace)\n",
    "\n",
    "### Trace Format Processing\n",
    "\n",
    "**String Traces**:\n",
    "- Stored as-is with no preprocessing\n",
    "- No agent metrics extracted (`metrics = None`)\n",
    "- Simplest format for basic answer evaluation\n",
    "\n",
    "**LangChain Message Lists**:\n",
    "1. **Validation**: Must be list of `BaseMessage` objects (AIMessage, ToolMessage, etc.)\n",
    "2. **Metrics Extraction**:\n",
    "   - Calls `_extract_agent_metrics(response)` from `verification_utils.py`\n",
    "   - Extracts: tool calls, tool failures, iterations\n",
    "3. **Harmonization**:\n",
    "   - Calls `harmonize_agent_response(response)` from `mcp_utils.py`\n",
    "   - Converts message list to unified string format\n",
    "4. **Storage**: Both harmonized trace and metrics stored together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Metrics Structure\n",
    "\n",
    "When using LangChain message lists, the following metrics are automatically extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example agent metrics structure\n",
    "example_metrics = {\n",
    "    \"tool_calls\": 3,  # Number of tool invocations\n",
    "    \"unique_tools_used\": 2,  # Number of unique tools\n",
    "    \"failed_tool_calls\": 0,  # Number of failed invocations\n",
    "    \"iterations\": 1,  # Agent iterations\n",
    "}\n",
    "\n",
    "print(\"Agent Metrics Structure:\")\n",
    "for key, value in example_metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session-Based Storage\n",
    "\n",
    "**Design**:\n",
    "- Global singleton `ManualTraceManager` instance\n",
    "- Thread-safe with `threading.RLock()`\n",
    "- Session timeout: 1 hour (3600 seconds)\n",
    "- Automatic cleanup of expired traces\n",
    "\n",
    "**Cleanup Strategy**:\n",
    "1. **Timer-Based**: `threading.Timer` triggers cleanup after timeout\n",
    "2. **Activity-Based**: Timer resets on any trace access\n",
    "3. **Trace-Level**: Individual traces have timestamps, expired traces removed\n",
    "4. **Session-Level**: If no activity for timeout period, entire session clears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelConfig Validation\n",
    "\n",
    "**Requirements for Manual Interface**:\n",
    "1. `interface` must be `\"manual\"`\n",
    "2. `manual_traces` must not be `None` (raises `ValueError` if missing)\n",
    "3. `id` defaults to `\"manual\"` if not provided\n",
    "4. `model_name` defaults to `\"manual\"` if not provided\n",
    "5. `mcp_urls_dict` must be `None` (raises `ValueError` if MCP configured)\n",
    "\n",
    "**Preset Compatibility**:\n",
    "- `manual_traces` field marked with `Field(exclude=True)`\n",
    "- Automatically excluded from Pydantic serialization\n",
    "- Presets save config structure but not trace data\n",
    "- Traces must be re-populated when loading preset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Question Text Matching\n",
    "\n",
    "**Do**:\n",
    "- Use exact question text from benchmark (case-sensitive, including whitespace)\n",
    "- Use `map_to_id=True` when working with question text\n",
    "- Verify question text matches benchmark before registration\n",
    "\n",
    "**Don't**:\n",
    "- Modify question text (trim whitespace, change case, etc.)\n",
    "- Assume approximate matching will work\n",
    "- Register traces for questions not in benchmark\n",
    "\n",
    "**Tip**: Export CSV mapper from benchmark to see exact question text and hashes\n",
    "\n",
    "### 2. Trace Format Selection\n",
    "\n",
    "**Use String Traces When**:\n",
    "- Answers are simple text without tool calls\n",
    "- No agent metrics needed\n",
    "- Simplest workflow sufficient\n",
    "\n",
    "**Use LangChain Message Lists When**:\n",
    "- Preserving tool call history is important\n",
    "- Agent metrics (tool calls, failures) are valuable\n",
    "- Comparing agent-based vs. non-agent-based approaches\n",
    "- Debugging tool usage patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Error Handling\n",
    "\n",
    "**Common Errors**:\n",
    "\n",
    "1. **Question Not Found**:\n",
    "```\n",
    "ValueError: Question not found in benchmark: 'What is 2+2?...'\n",
    "Computed hash: 936dbc8755f623c951d96ea2b03e13bc\n",
    "```\n",
    "**Fix**: Verify exact question text matches benchmark, check for whitespace/case differences\n",
    "\n",
    "2. **Invalid Hash Format**:\n",
    "```\n",
    "ManualTraceError: Invalid question hash format: 'short'\n",
    "```\n",
    "**Fix**: Ensure hash is 32-character hexadecimal MD5\n",
    "\n",
    "3. **Missing Manual Traces**:\n",
    "```\n",
    "ValueError: manual_traces is required when interface='manual'\n",
    "```\n",
    "**Fix**: Pass `manual_traces` to ModelConfig constructor\n",
    "\n",
    "4. **MCP Configuration Conflict**:\n",
    "```\n",
    "ValueError: MCP tools are not supported with manual interface\n",
    "```\n",
    "**Fix**: Remove `mcp_urls_dict` from manual ModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance Optimization\n",
    "\n",
    "**Batch Registration**:\n",
    "- Use `register_traces()` instead of multiple `register_trace()` calls\n",
    "- Reduces overhead for large trace sets\n",
    "- More readable code\n",
    "\n",
    "**Memory Management**:\n",
    "- Monitor trace count with `get_manual_trace_count()`\n",
    "- Check memory usage with `get_memory_usage_info()`\n",
    "- Clear traces with `clear_manual_traces()` when done\n",
    "\n",
    "**Session Cleanup**:\n",
    "- Traces auto-expire after 1 hour of inactivity\n",
    "- Manual cleanup with `clear_manual_traces()` if needed\n",
    "- Activity resets timeout (any trace access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing and Validation\n",
    "\n",
    "**Before Running Verification**, verify traces were registered correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "from karenina.infrastructure.llm.manual_traces import get_manual_trace, get_manual_trace_count, has_manual_trace\n",
    "\n",
    "# Create benchmark and register a trace\n",
    "benchmark = Benchmark(\"validation_example\")\n",
    "benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)\n",
    "\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "manual_traces.register_trace(\"What is 2+2?\", \"The answer is 4.\", map_to_id=True)\n",
    "\n",
    "# Verify trace was registered\n",
    "question_hash = hashlib.md5(b\"What is 2+2?\").hexdigest()\n",
    "print(f\"Question hash: {question_hash}\")\n",
    "print(f\"Has trace: {has_manual_trace(question_hash)}\")\n",
    "\n",
    "# Retrieve and inspect trace\n",
    "trace = get_manual_trace(question_hash)\n",
    "print(f\"Registered trace: {trace[:50]}...\")\n",
    "\n",
    "# Check trace count\n",
    "expected_count = len(list(benchmark.get_finished_questions()))\n",
    "actual_count = get_manual_trace_count()\n",
    "print(f\"Expected traces: {expected_count}\")\n",
    "print(f\"Actual traces: {actual_count}\")\n",
    "\n",
    "if actual_count != expected_count:\n",
    "    print(f\"Warning: Expected {expected_count} traces, have {actual_count}\")\n",
    "else:\n",
    "    print(\"All traces registered correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Preset Workflow\n",
    "\n",
    "**Saving Presets with Manual Configs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Saving preset with manual config\n",
    "benchmark = Benchmark(\"preset_example\")\n",
    "benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)\n",
    "\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "manual_traces.register_trace(\"What is 2+2?\", \"The answer is 4.\", map_to_id=True)\n",
    "\n",
    "manual_config = ModelConfig(interface=\"manual\", manual_traces=manual_traces)\n",
    "judge_config = ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "\n",
    "config = VerificationConfig(answering_models=[manual_config], parsing_models=[judge_config])\n",
    "\n",
    "# Manual traces excluded from serialization\n",
    "config_dict = config.model_dump()\n",
    "print(\"manual_traces in config dict:\", \"manual_traces\" in config_dict)\n",
    "print(\"Preset can be saved without trace data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading preset and re-populating manual traces\n",
    "\n",
    "# 1. Load preset (manual_traces will be None)\n",
    "# loaded_config = preset_service.load_preset(\"my_preset\")\n",
    "\n",
    "# 2. Re-populate manual traces\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "traces_dict = {\"What is 2+2?\": \"The answer is 4.\"}\n",
    "manual_traces.register_traces(traces_dict, map_to_id=True)\n",
    "\n",
    "# 3. Update config with traces\n",
    "# loaded_config.answering_models[0].manual_traces = manual_traces\n",
    "\n",
    "# 4. Run verification\n",
    "# benchmark.run_verification(loaded_config)\n",
    "\n",
    "print(\"When loading presets with manual configs:\")\n",
    "print(\"1. Load preset (manual_traces will be None)\")\n",
    "print(\"2. Re-populate manual traces from your data source\")\n",
    "print(\"3. Update config with new manual_traces\")\n",
    "print(\"4. Run verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Examples\n",
    "\n",
    "### Example 1: Simple String Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.benchmark import Benchmark\n",
    "from karenina.infrastructure.llm.manual_traces import ManualTraces\n",
    "from karenina.schemas import ModelConfig, VerificationConfig\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark(\"complete_example_1\")\n",
    "benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)\n",
    "benchmark.add_question(question=\"What is 3+3?\", raw_answer=\"6\", finished=True)\n",
    "\n",
    "# Initialize manual traces\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "\n",
    "# Register traces by question text\n",
    "manual_traces.register_trace(\"What is 2+2?\", \"The answer is 4. I added 2 and 2 to get 4.\", map_to_id=True)\n",
    "\n",
    "manual_traces.register_trace(\"What is 3+3?\", \"The answer is 6. I added 3 and 3 to get 6.\", map_to_id=True)\n",
    "\n",
    "# Create manual config\n",
    "manual_config = ModelConfig(interface=\"manual\", manual_traces=manual_traces)\n",
    "\n",
    "# Create judge config\n",
    "judge_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.0, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# Run verification\n",
    "config = VerificationConfig(answering_models=[manual_config], parsing_models=[judge_config])\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Example 1 complete: {len(results.results)} results\")\n",
    "for r in results.results:\n",
    "    print(f\"  - {r.metadata.question_text[:30]}... passed: {r.template.verify_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Batch Registration with Mixed Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "# Create benchmark\n",
    "benchmark = Benchmark(\"complete_example_2\")\n",
    "benchmark.add_question(question=\"Question 1?\", raw_answer=\"Answer 1\", finished=True)\n",
    "benchmark.add_question(question=\"Question 2?\", raw_answer=\"Answer 2\", finished=True)\n",
    "benchmark.add_question(question=\"Question 3?\", raw_answer=\"Answer 3\", finished=True)\n",
    "\n",
    "# Initialize manual traces\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "\n",
    "# Prepare traces dictionary with mixed formats\n",
    "traces = {\n",
    "    \"Question 1?\": \"Answer 1 is a simple string trace.\",\n",
    "    \"Question 2?\": [\n",
    "        AIMessage(content=\"Let me think about this...\"),\n",
    "        ToolMessage(name=\"search\", content=\"Found relevant data\", tool_call_id=\"call_1\"),\n",
    "        AIMessage(content=\"Answer 2 based on search results.\"),\n",
    "    ],\n",
    "    \"Question 3?\": \"Answer 3 is another string trace.\",\n",
    "}\n",
    "\n",
    "# Batch register all at once\n",
    "manual_traces.register_traces(traces, map_to_id=True)\n",
    "\n",
    "print(f\"Example 2: Batch registered {len(traces)} traces\")\n",
    "print(\"  - Question 1: String trace\")\n",
    "print(\"  - Question 2: Message list with tool calls\")\n",
    "print(\"  - Question 3: String trace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Delayed Trace Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark\n",
    "benchmark = Benchmark(\"complete_example_3\")\n",
    "benchmark.add_question(question=\"What is 2+2?\", raw_answer=\"4\", finished=True)\n",
    "\n",
    "# Step 1: Create config structure early\n",
    "manual_traces = ManualTraces(benchmark)\n",
    "manual_config = ModelConfig(interface=\"manual\", manual_traces=manual_traces)\n",
    "\n",
    "# Step 2: Pass config around, set up verification structure\n",
    "judge_config = ModelConfig(id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", interface=\"langchain\")\n",
    "\n",
    "config = VerificationConfig(answering_models=[manual_config], parsing_models=[judge_config])\n",
    "\n",
    "\n",
    "# Step 3: Later, populate traces (e.g., from file load, API call)\n",
    "def load_traces_from_file(filepath):\n",
    "    return {\"What is 2+2?\": \"The answer is 4.\"}\n",
    "\n",
    "\n",
    "traces_data = load_traces_from_file(\"experiment_traces.json\")\n",
    "\n",
    "for question_text, trace_content in traces_data.items():\n",
    "    manual_traces.register_trace(question_text, trace_content, map_to_id=True)\n",
    "\n",
    "# Step 4: Run verification with populated traces\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"Example 3 complete: {len(results.results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Manual Trace System enables:\n",
    "\n",
    "1. **Pre-Generated Answer Evaluation** - Evaluate LLM responses without making live API calls\n",
    "2. **Flexible Trace Formats** - Support for both simple strings and rich message lists\n",
    "3. **Agent Metrics Extraction** - Automatic tool call and failure tracking\n",
    "4. **Efficient Workflows** - Batch registration, delayed population, preset compatibility\n",
    "5. **Production-Ready** - Thread-safe, session-based, with automatic cleanup\n",
    "\n",
    "**Key Workflow**: Create benchmark → Initialize `ManualTraces` → Register traces → Create `ModelConfig` with `interface=\"manual\"` → Run verification\n",
    "\n",
    "---\n",
    "\n",
    "**Related Documentation**:\n",
    "- **Quick Start**: Basic verification workflow\n",
    "- **Verification**: Complete verification documentation\n",
    "- **Configuration**: Model and provider configuration\n",
    "- **TaskEval**: Evaluate agent workflow traces (similar concept for pre-existing outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
