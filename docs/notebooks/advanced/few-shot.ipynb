{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mock-setup",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock Setup - Hidden in rendered documentation\n",
    "# This cell is tagged with \"hide-cell\" in notebook metadata\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add karenina to path\n",
    "sys.path.insert(0, \"/Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "\n",
    "# Temporary directory for file operations\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"karenina_docs_\"))\n",
    "\n",
    "# Import after path is set\n",
    "from karenina.schemas.workflow.template_results import TemplateResults\n",
    "from karenina.schemas.workflow.verification.result import VerificationResult\n",
    "from karenina.schemas.workflow.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "from karenina.schemas.workflow.verification_result_set import VerificationResultSet\n",
    "\n",
    "\n",
    "# Mock LLM response generator\n",
    "class MockLLMResponse:\n",
    "    \"\"\"Mock response object that mimics LangChain message structure.\"\"\"\n",
    "\n",
    "    def __init__(self, content: str = \"Mock response\"):\n",
    "        self.content = content\n",
    "        self.response_metadata = {\"token_usage\": {\"total_tokens\": 50}}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "class MockStructuredOutput:\n",
    "    \"\"\"Mock structured output response that adapts to any template.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Set common attributes with realistic defaults\n",
    "        self.count = kwargs.get(\"count\", 46)\n",
    "        self.target = kwargs.get(\"target\", \"BCL2\")\n",
    "        self.subunits = kwargs.get(\"subunits\", 4)\n",
    "        self.diseases = kwargs.get(\"diseases\", [\"asthma\", \"bronchitis\", \"pneumonia\"])\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def dict(self):\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n",
    "\n",
    "    def model_dump(self):\n",
    "        return self.dict()\n",
    "\n",
    "\n",
    "def create_mock_chat_model():\n",
    "    \"\"\"Create a mock chat model that returns predictable responses.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    mock.ainvoke.return_value = MockLLMResponse(\"46 chromosomes\")\n",
    "    structured_mock = MagicMock()\n",
    "    structured_mock.invoke.return_value = MockStructuredOutput()\n",
    "    structured_mock.ainvoke.return_value = MockStructuredOutput()\n",
    "    mock.with_structured_output.return_value = structured_mock\n",
    "    mock.bind_tools.return_value = mock\n",
    "    return mock\n",
    "\n",
    "\n",
    "def compute_result_id(question_id: str, answering_model: str, parsing_model: str, timestamp: str) -> str:\n",
    "    \"\"\"Compute deterministic 16-char SHA256 hash.\"\"\"\n",
    "    data = {\n",
    "        \"answering_mcp_servers\": [],\n",
    "        \"answering_model\": answering_model,\n",
    "        \"parsing_model\": parsing_model,\n",
    "        \"question_id\": question_id,\n",
    "        \"replicate\": None,\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)\n",
    "    hash_obj = hashlib.sha256(json_str.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def create_mock_verification_result(question_id: str, question_text: str, answer: str, passed: bool = True):\n",
    "    \"\"\"Create a mock VerificationResult for testing.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    template_id = hashlib.md5(str(question_id).encode()).hexdigest()[:32]\n",
    "\n",
    "    # Create mock template result\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=f\"The answer is {answer}.\",\n",
    "        parsed_llm_response={\"value\": answer},\n",
    "        parsed_gt_response={\"value\": answer},\n",
    "        verify_result=passed,\n",
    "        template_verification_performed=True,\n",
    "        usage_metadata={\n",
    "            \"answer_generation\": {\"total_tokens\": 50},\n",
    "            \"parsing\": {\"total_tokens\": 30},\n",
    "            \"total\": {\"total_tokens\": 80},\n",
    "        },\n",
    "        abstention_check_performed=True,\n",
    "        abstention_detected=False,\n",
    "    )\n",
    "\n",
    "    # Create mock rubric result\n",
    "    rubric = VerificationResultRubric(\n",
    "        rubric_evaluation_performed=True,\n",
    "        llm_trait_scores={\n",
    "            \"Conciseness\": 4,\n",
    "            \"Clarity\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Create metadata with all required fields\n",
    "    metadata = VerificationResultMetadata(\n",
    "        question_id=question_id,\n",
    "        template_id=template_id,\n",
    "        completed_without_errors=True,\n",
    "        question_text=question_text,\n",
    "        raw_answer=answer,\n",
    "        answering_model=\"gpt-4.1-mini\",\n",
    "        parsing_model=\"gpt-4.1-mini\",\n",
    "        execution_time=1.5,\n",
    "        timestamp=timestamp,\n",
    "        result_id=compute_result_id(question_id, \"gpt-4.1-mini\", \"gpt-4.1-mini\", timestamp),\n",
    "    )\n",
    "\n",
    "    return VerificationResult(\n",
    "        metadata=metadata,\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "\n",
    "# Store original methods\n",
    "_original_run_verification = None\n",
    "_original_generate_all_templates = None\n",
    "\n",
    "\n",
    "def mock_generate_all_templates(self, *args, **kwargs):\n",
    "    \"\"\"Mock generate_all_templates - just print message and return.\"\"\"\n",
    "    # In a real environment, this would generate templates\n",
    "    # For the notebook demo, we just return success\n",
    "    return {}\n",
    "\n",
    "\n",
    "def mock_run_verification(self, config):\n",
    "    \"\"\"Mock run_verification that returns realistic results.\"\"\"\n",
    "    global _original_run_verification\n",
    "\n",
    "    # Get all finished questions\n",
    "    finished = self.get_finished_questions(ids_only=False)\n",
    "\n",
    "    if len(finished) == 0:\n",
    "        # If no finished questions, return empty results\n",
    "        return VerificationResultSet(results=[], template_results=TemplateResults(results=[]))\n",
    "\n",
    "    results = []\n",
    "    # Map question keywords to expected answers\n",
    "    mock_data = [\n",
    "        {\"keywords\": [\"chromosomes\"], \"answer\": \"46\", \"passed\": True},\n",
    "        {\"keywords\": [\"venetoclax\", \"bcl2\"], \"answer\": \"BCL2\", \"passed\": True},\n",
    "        {\"keywords\": [\"hemoglobin\", \"subunits\"], \"answer\": \"4\", \"passed\": True},\n",
    "        {\"keywords\": [\"inflammatory\", \"lung\"], \"answer\": \"asthma, bronchitis, pneumonia\", \"passed\": True},\n",
    "    ]\n",
    "\n",
    "    for question in finished:\n",
    "        q_id = question[\"id\"]\n",
    "        q_text = question[\"question\"]\n",
    "        raw_answer = question.get(\"raw_answer\", \"\")\n",
    "\n",
    "        passed = True\n",
    "        mock_ans = raw_answer\n",
    "        q_text_lower = q_text.lower()\n",
    "\n",
    "        for data in mock_data:\n",
    "            if any(kw in q_text_lower for kw in data[\"keywords\"]):\n",
    "                passed = data[\"passed\"]\n",
    "                mock_ans = data[\"answer\"]\n",
    "                break\n",
    "\n",
    "        results.append(\n",
    "            create_mock_verification_result(question_id=q_id, question_text=q_text, answer=mock_ans, passed=passed)\n",
    "        )\n",
    "\n",
    "    template_results = TemplateResults(results=results)\n",
    "\n",
    "    return VerificationResultSet(\n",
    "        results=results,\n",
    "        template_results=template_results,\n",
    "        rubric_results=None,\n",
    "    )\n",
    "\n",
    "\n",
    "# Patch all LLM providers before any imports\n",
    "_llm_patches = [\n",
    "    patch(\"langchain_openai.ChatOpenAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_anthropic.ChatAnthropic\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\"langchain_google_genai.ChatGoogleGenerativeAI\", side_effect=lambda **kwargs: create_mock_chat_model()),\n",
    "    patch(\n",
    "        \"karenina.infrastructure.llm.interface.init_chat_model_unified\",\n",
    "        side_effect=lambda **kwargs: create_mock_chat_model(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in _llm_patches:\n",
    "    p.start()\n",
    "\n",
    "# Patch Benchmark methods\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "_original_run_verification = Benchmark.run_verification\n",
    "_original_generate_all_templates = Benchmark.generate_all_templates\n",
    "Benchmark.run_verification = mock_run_verification\n",
    "Benchmark.generate_all_templates = mock_generate_all_templates\n",
    "\n",
    "\n",
    "def temp_path(filename: str) -> Path:\n",
    "    \"\"\"Helper to create paths in temp directory.\"\"\"\n",
    "    return TEMP_DIR / filename\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "import atexit\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _cleanup():\n",
    "    Benchmark.run_verification = _original_run_verification\n",
    "    Benchmark.generate_all_templates = _original_generate_all_templates\n",
    "    for p in _llm_patches:\n",
    "        try:\n",
    "            p.stop()\n",
    "        except:\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"✓ Mock setup complete\")\n",
    "print(f\"✓ Temp directory: {TEMP_DIR}\")\n",
    "print(\"✓ Karenina package loaded from: /Users/carli/Projects/karenina-monorepo/karenina/src\")\n",
    "print(\"✓ Mock verification results enabled - examples will show realistic output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting is a technique where example question-answer pairs are provided to the LLM before asking the main question, helping guide responses toward expected formats, styles, and content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-is",
   "metadata": {},
   "source": [
    "## What is Few-Shot Prompting?\n",
    "\n",
    "**Few-shot prompting** provides the LLM with examples of the task before asking it to perform the same task. For example:\n",
    "\n",
    "```\n",
    "Question: What is the approved drug target of Venetoclax?\n",
    "Answer: BCL2\n",
    "\n",
    "Question: How many chromosomes are in a human somatic cell?\n",
    "Answer: 46\n",
    "\n",
    "Question: How many protein subunits does hemoglobin A have?\n",
    "Answer: [Model will answer here]\n",
    "```\n",
    "\n",
    "This technique can significantly improve:\n",
    "\n",
    "- **Response quality**: Models learn from good examples\n",
    "- **Consistency**: Responses follow demonstrated patterns\n",
    "- **Format adherence**: Models match example structure\n",
    "- **Accuracy**: Examples clarify expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-use",
   "metadata": {},
   "source": [
    "## Why Use Few-Shot Prompting?\n",
    "\n",
    "### 1. Improve Answer Quality\n",
    "\n",
    "Models perform better when shown examples:\n",
    "\n",
    "- Without few-shot: Verbose answer like \"Hemoglobin A is a tetrameric protein consisting of two alpha and two beta subunits...\"\n",
    "- With few-shot: Concise answer like \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without few-shot: Verbose answer\n",
    "verbose_answer = \"Hemoglobin A is a tetrameric protein consisting of two alpha and two beta subunits...\"\n",
    "\n",
    "# With few-shot: Concise answer (like examples)\n",
    "concise_answer = \"4\"\n",
    "\n",
    "print(f\"Verbose: {verbose_answer[:50]}...\")\n",
    "print(f\"Concise: {concise_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-format",
   "metadata": {},
   "source": [
    "### 2. Enforce Formatting\n",
    "\n",
    "Guide models to specific answer formats by showing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples show concise numerical answers\n",
    "few_shot_examples = [\n",
    "    {\"question\": \"How many chromosomes...\", \"answer\": \"46\"},\n",
    "    {\"question\": \"How many subunits...\", \"answer\": \"4\"},\n",
    "]\n",
    "\n",
    "print(\"Few-shot examples that demonstrate format:\")\n",
    "for ex in few_shot_examples:\n",
    "    print(f\"  Q: {ex['question']}\")\n",
    "    print(f\"  A: {ex['answer']}\")\n",
    "print(\"\\nModel learns to give brief numerical answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-style",
   "metadata": {},
   "source": [
    "### 3. Demonstrate Style\n",
    "\n",
    "Show models the desired response style (e.g., technical nomenclature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "style-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples show technical nomenclature\n",
    "few_shot_examples = [\n",
    "    {\"question\": \"What is the target of Venetoclax?\", \"answer\": \"BCL2\"},\n",
    "    {\"question\": \"What does TP53 encode?\", \"answer\": \"tumor protein p53\"},\n",
    "]\n",
    "\n",
    "print(\"Examples that demonstrate technical nomenclature:\")\n",
    "for ex in few_shot_examples:\n",
    "    print(f\"  {ex['answer']}\")\n",
    "print(\"\\nModel learns to use standard nomenclature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-config",
   "metadata": {},
   "source": [
    "## Basic Configuration\n",
    "\n",
    "### Simple K-Shot Mode\n",
    "\n",
    "Use the same number of examples for all questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k-shot-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "from karenina.schemas import FewShotConfig, ModelConfig, VerificationConfig\n",
    "\n",
    "# Create few-shot config with k=3 (use 3 examples per question)\n",
    "few_shot_config = FewShotConfig(enabled=True, global_mode=\"k-shot\", global_k=3)\n",
    "\n",
    "# Create verification config with few-shot\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.0, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config], parsing_models=[model_config], few_shot_config=few_shot_config\n",
    ")\n",
    "\n",
    "print(f\"Few-shot enabled: {few_shot_config.enabled}\")\n",
    "print(f\"Global mode: {few_shot_config.global_mode}\")\n",
    "print(f\"Global k: {few_shot_config.global_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "all-mode",
   "metadata": {},
   "source": [
    "### Use All Available Examples\n",
    "\n",
    "Use every available example for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "all-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure to use all examples\n",
    "few_shot_config_all = FewShotConfig(enabled=True, global_mode=\"all\")\n",
    "\n",
    "print(f\"Global mode: {few_shot_config_all.global_mode}\")\n",
    "print(\"\\nWhen to use: Maximum context, small number of high-quality examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adding-examples",
   "metadata": {},
   "source": [
    "## Adding Examples to Questions\n",
    "\n",
    "### When Creating Questions\n",
    "\n",
    "Add few-shot examples when creating questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-examples-create",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.create(name=\"Genomics Benchmark\")\n",
    "\n",
    "# Add question with few-shot examples\n",
    "benchmark.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\",\n",
    "    raw_answer=\"BCL2\",\n",
    "    few_shot_examples=[\n",
    "        {\"question\": \"What is the approved drug target of Imatinib?\", \"answer\": \"BCR-ABL tyrosine kinase\"},\n",
    "        {\"question\": \"What is the approved drug target of Trastuzumab?\", \"answer\": \"HER2\"},\n",
    "        {\"question\": \"What is the approved drug target of Rituximab?\", \"answer\": \"CD20\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Question added with few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adding-examples-later",
   "metadata": {},
   "source": [
    "### Adding Examples Later\n",
    "\n",
    "Add examples to existing questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-examples-later",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding few-shot examples to an existing question\n",
    "# (In practice, you would load your benchmark first)\n",
    "\n",
    "# Load benchmark\n",
    "# benchmark = Benchmark.load(\"genomics_benchmark.jsonld\")\n",
    "\n",
    "# Get question\n",
    "# question_id = list(benchmark.questions.keys())[0]\n",
    "# question = benchmark.get_question(result.question_id)\n",
    "\n",
    "# Add few-shot examples\n",
    "# question.few_shot_examples = [\n",
    "#     {\"question\": \"How many autosomal chromosome pairs...\", \"answer\": \"22\"},\n",
    "#     {\"question\": \"How many sex chromosomes...\", \"answer\": \"2\"},\n",
    "# ]\n",
    "\n",
    "# Save updated benchmark\n",
    "# benchmark.save(\"genomics_benchmark.jsonld\")\n",
    "\n",
    "print(\"Examples shown above demonstrate adding examples to existing questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-example",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "\n",
    "Here's an end-to-end workflow using few-shot prompting with a genomics benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "end-to-end",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina import Benchmark\n",
    "from karenina.schemas import FewShotConfig, ModelConfig, VerificationConfig\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Create benchmark with genomics questions\n",
    "# ============================================================\n",
    "\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Genomics Knowledge Benchmark\",\n",
    "    description=\"Testing LLM knowledge of genomics with few-shot prompting\",\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "print(\"Step 1: Created benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2-add-questions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Add questions with few-shot examples\n",
    "# ============================================================\n",
    "\n",
    "# Question 1: Drug target with similar drug examples\n",
    "benchmark.add_question(\n",
    "    question=\"What is the approved drug target of Venetoclax?\",\n",
    "    raw_answer=\"BCL2\",\n",
    "    author={\"name\": \"Pharma Curator\"},\n",
    "    few_shot_examples=[\n",
    "        {\"question\": \"What is the approved drug target of Imatinib?\", \"answer\": \"BCR-ABL tyrosine kinase\"},\n",
    "        {\"question\": \"What is the approved drug target of Trastuzumab?\", \"answer\": \"HER2\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Question 2: Numerical answer with similar numerical examples\n",
    "benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\",\n",
    "    raw_answer=\"46\",\n",
    "    author={\"name\": \"Genetics Curator\"},\n",
    "    few_shot_examples=[\n",
    "        {\"question\": \"How many autosomal chromosome pairs are in humans?\", \"answer\": \"22\"},\n",
    "        {\"question\": \"How many sex chromosomes are in humans?\", \"answer\": \"2\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Question 3: Protein structure with similar structure examples\n",
    "benchmark.add_question(\n",
    "    question=\"How many protein subunits does hemoglobin A have?\",\n",
    "    raw_answer=\"4\",\n",
    "    author={\"name\": \"Biochemistry Curator\"},\n",
    "    few_shot_examples=[\n",
    "        {\"question\": \"How many subunits does RNA polymerase have?\", \"answer\": \"5\"},\n",
    "        {\"question\": \"How many catalytic subunits does DNA polymerase III have?\", \"answer\": \"3\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Step 2: Added 3 questions with few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-generate-templates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Generate templates\n",
    "# ============================================================\n",
    "\n",
    "print(\"Step 3: Generating templates...\")\n",
    "benchmark.generate_all_templates(model=\"gpt-4.1-mini\", model_provider=\"openai\", temperature=0.0, interface=\"langchain\")\n",
    "print(\"✓ Templates generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-configure-fewshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: Configure few-shot prompting\n",
    "# ============================================================\n",
    "\n",
    "# Create model config for verification\n",
    "model_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\", model_provider=\"openai\", model_name=\"gpt-4.1-mini\", temperature=0.0, interface=\"langchain\"\n",
    ")\n",
    "\n",
    "# Option A: Use k-shot mode (same number of examples per question)\n",
    "few_shot_config = FewShotConfig(\n",
    "    enabled=True,\n",
    "    global_mode=\"k-shot\",\n",
    "    global_k=2,  # Use 2 examples per question\n",
    ")\n",
    "\n",
    "print(\"Step 4: Configured few-shot (k-shot, k=2)\")\n",
    "print(f\"  Enabled: {few_shot_config.enabled}\")\n",
    "print(f\"  Mode: {few_shot_config.global_mode}\")\n",
    "print(f\"  K value: {few_shot_config.global_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5-run-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: Run verification with few-shot\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nStep 5: Running verification with few-shot prompting...\")\n",
    "config = VerificationConfig(\n",
    "    answering_models=[model_config], parsing_models=[model_config], few_shot_config=few_shot_config\n",
    ")\n",
    "\n",
    "results = benchmark.run_verification(config)\n",
    "print(f\"✓ Verification complete: {len(results.results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6-analyze-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6: Analyze results\n",
    "# ============================================================\n",
    "\n",
    "# Check if we have results (in mock environment, questions may not be \"finished\")\n",
    "if len(results.results) > 0:\n",
    "    passed = sum(1 for r in results.results if r.verify_result)\n",
    "    print(f\"\\nPass rate: {passed}/{len(results.results)} ({passed / len(results.results) * 100:.1f}%)\")\n",
    "\n",
    "    # Show individual results\n",
    "    for result in results.results:\n",
    "        question = benchmark.get_question(result.question_id)\n",
    "        print(f\"\\nQuestion: {question['question']}\")\n",
    "        print(f\"  Expected: {question['raw_answer']}\")\n",
    "        print(f\"  Model answer: {result.template.raw_llm_response[:50]}...\")\n",
    "        print(f\"  Correct: {'✓' if result.verify_result else '✗'}\")\n",
    "else:\n",
    "    print(\"\\nNo results to display - in production, this would show verification results\")\n",
    "    print(\"Mock environment: Questions need templates to be verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step7-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark with results\n",
    "save_path = temp_path(\"genomics_benchmark_few_shot.jsonld\")\n",
    "benchmark.save(str(save_path))\n",
    "print(f\"\\n✓ Benchmark saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-configs",
   "metadata": {},
   "source": [
    "## Advanced Configurations\n",
    "\n",
    "### Different K Values Per Question\n",
    "\n",
    "Use different numbers of examples for different questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k-per-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure different k values per question\n",
    "few_shot_config_per_question = FewShotConfig.k_shot_for_questions(\n",
    "    question_k_mapping={\n",
    "        \"question_id_1\": 5,  # Use 5 examples for complex question\n",
    "        \"question_id_2\": 2,  # Use 2 examples for simple question\n",
    "        \"question_id_3\": 3,  # Use 3 examples\n",
    "    },\n",
    "    global_k=3,  # Fallback for questions not in mapping\n",
    ")\n",
    "\n",
    "print(\"Per-question k-shot configuration:\")\n",
    "for qid, cfg in few_shot_config_per_question.question_configs.items():\n",
    "    print(f\"  {qid}: k={cfg.k}\")\n",
    "print(f\"\\nGlobal fallback k: {few_shot_config_per_question.global_k}\")\n",
    "print(\"\\nWhen to use: Questions have varying complexity levels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "index-selection",
   "metadata": {},
   "source": [
    "### Custom Example Selection by Index\n",
    "\n",
    "Manually select specific examples by their position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific examples by index (0-based)\n",
    "# In practice, get question IDs from your benchmark\n",
    "# question_ids = list(benchmark.questions.keys())\n",
    "\n",
    "# Example configuration (using placeholder IDs)\n",
    "few_shot_config_custom = FewShotConfig.from_index_selections(\n",
    "    {\n",
    "        \"question_1\": [0, 1],  # Use first 2 examples\n",
    "        \"question_2\": [0, 2],  # Use 1st and 3rd examples\n",
    "        \"question_3\": [1, 2, 3],  # Use 2nd, 3rd, and 4th examples\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Custom index selection configuration:\")\n",
    "for qid, cfg in few_shot_config_custom.question_configs.items():\n",
    "    print(f\"  {qid}: indices={cfg.selected_examples}\")\n",
    "print(\"\\nWhen to use: Fine-grained control over which examples are used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-examples",
   "metadata": {},
   "source": [
    "### Adding External Examples\n",
    "\n",
    "Add examples that aren't from the question's available pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-examples-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config with global external examples\n",
    "few_shot_config_external = FewShotConfig(\n",
    "    enabled=True,\n",
    "    global_mode=\"k-shot\",\n",
    "    global_k=2,\n",
    "    global_external_examples=[\n",
    "        {\"question\": \"What is the molecular weight of glucose?\", \"answer\": \"180.16 g/mol\"},\n",
    "        {\"question\": \"What is the pH of neutral water?\", \"answer\": \"7.0\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Global external examples: {len(few_shot_config_external.global_external_examples)}\")\n",
    "for i, ex in enumerate(few_shot_config_external.global_external_examples, 1):\n",
    "    print(f\"  {i}. Q: {ex['question'][:40]}...\")\n",
    "    print(f\"     A: {ex['answer']}\")\n",
    "print(\"\\nWhen to use: Want to include domain-specific high-quality examples for all questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modes-overview",
   "metadata": {},
   "source": [
    "## Modes Overview\n",
    "\n",
    "### \"k-shot\" Mode\n",
    "\n",
    "Use the first k examples for each question. Best for consistent number of examples across all questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modes-k-shot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-shot mode\n",
    "k_shot_mode = FewShotConfig(\n",
    "    global_mode=\"k-shot\",\n",
    "    global_k=3,  # Use 3 examples\n",
    ")\n",
    "print(f\"Mode: {k_shot_mode.global_mode}\")\n",
    "print(f\"K value: {k_shot_mode.global_k}\")\n",
    "print(\"Best for: Consistent number of examples across all questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modes-all",
   "metadata": {},
   "source": [
    "### \"all\" Mode\n",
    "\n",
    "Use all available examples for each question. Best for small number of high-quality examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modes-all-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all mode\n",
    "all_mode = FewShotConfig(global_mode=\"all\")\n",
    "print(f\"Mode: {all_mode.global_mode}\")\n",
    "print(\"Best for: Small number of high-quality examples, maximum context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modes-custom",
   "metadata": {},
   "source": [
    "### \"custom\" Mode\n",
    "\n",
    "Manually select specific examples. Best for fine-grained control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modes-custom-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom mode via from_index_selections\n",
    "custom_mode = FewShotConfig.from_index_selections(\n",
    "    {\n",
    "        \"question_1\": [0, 2, 4],  # Select by index\n",
    "    }\n",
    ")\n",
    "print(f\"Mode: {custom_mode.global_mode}\")\n",
    "print(\"Best for: Fine-grained control, curated example selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modes-none",
   "metadata": {},
   "source": [
    "### \"none\" Mode\n",
    "\n",
    "Disable few-shot for specific questions. Best for testing impact of few-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modes-none-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# none mode for specific questions\n",
    "from karenina.schemas import QuestionFewShotConfig\n",
    "\n",
    "none_mode = FewShotConfig(\n",
    "    global_mode=\"k-shot\",\n",
    "    global_k=3,\n",
    "    question_configs={\n",
    "        \"special_question_id\": QuestionFewShotConfig(mode=\"none\")  # No examples\n",
    "    },\n",
    ")\n",
    "print(f\"Global mode: {none_mode.global_mode}\")\n",
    "print(\"Special question has mode='none'\")\n",
    "print(\"Best for: Testing impact of few-shot on specific questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-format",
   "metadata": {},
   "source": [
    "## Prompt Format\n",
    "\n",
    "Few-shot prompts are constructed in a simple Q&A format:\n",
    "\n",
    "```\n",
    "Question: What is the approved drug target of Imatinib?\n",
    "Answer: BCR-ABL tyrosine kinase\n",
    "\n",
    "Question: What is the approved drug target of Trastuzumab?\n",
    "Answer: HER2\n",
    "\n",
    "Question: What is the approved drug target of Venetoclax?\n",
    "Answer: [Model generates answer here]\n",
    "```\n",
    "\n",
    "The LLM sees the examples before answering, learning from their format and content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-use",
   "metadata": {},
   "source": [
    "## When to Use Few-Shot\n",
    "\n",
    "### ✅ Use Few-Shot When:\n",
    "\n",
    "- **Enforcing formats**: Need specific answer structure (numerical, gene symbols, etc.)\n",
    "- **Improving conciseness**: Models tend to be verbose, examples show brevity\n",
    "- **Demonstrating style**: Want technical nomenclature or specific terminology\n",
    "- **Complex tasks**: Task benefits from seeing examples\n",
    "- **Consistency matters**: Need similar answers across similar questions\n",
    "\n",
    "### ❌ Don't Use Few-Shot When:\n",
    "\n",
    "- **Simple tasks**: Model already performs well without examples\n",
    "- **Token limits**: Using large models with limited context windows\n",
    "- **No good examples**: Don't have high-quality representative examples\n",
    "- **Testing baselines**: Measuring model performance without assistance\n",
    "- **Fast iteration**: Adding complexity during initial testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Start with K-Shot Mode\n",
    "\n",
    "Begin with k-shot before moving to custom selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bp-1-start-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start simple\n",
    "simple_config = FewShotConfig(global_mode=\"k-shot\", global_k=3)\n",
    "print(\"Start with simple k-shot configuration\")\n",
    "print(f\"Mode: {simple_config.global_mode}, K: {simple_config.global_k}\")\n",
    "print(\"Can iterate to custom if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bp-2",
   "metadata": {},
   "source": [
    "### 2. Use 2-3 Examples\n",
    "\n",
    "More examples aren't always better. Start small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bp-2-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good starting point\n",
    "config_k2 = FewShotConfig(global_mode=\"k-shot\", global_k=2)\n",
    "\n",
    "# Can increase if needed\n",
    "config_k5 = FewShotConfig(global_mode=\"k-shot\", global_k=5)\n",
    "\n",
    "print(\"Recommended: Start with k=2-3\")\n",
    "print(f\"  k=2: {config_k2.global_k} examples\")\n",
    "print(f\"  k=5: {config_k5.global_k} examples (increase if needed)\")\n",
    "print(\"\\nWhy: Diminishing returns after 3-5 examples, increased token costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bp-3",
   "metadata": {},
   "source": [
    "### 3. Choose Representative Examples\n",
    "\n",
    "Select examples that represent the task well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bp-3-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Similar domain, clear answers\n",
    "good_examples = [\n",
    "    {\"question\": \"What is the target of Venetoclax?\", \"answer\": \"BCL2\"},\n",
    "    {\"question\": \"What is the target of Imatinib?\", \"answer\": \"BCR-ABL\"},\n",
    "]\n",
    "\n",
    "print(\"Good examples (similar domain, clear answers):\")\n",
    "for ex in good_examples:\n",
    "    print(f\"  {ex['answer']}\")\n",
    "\n",
    "print(\"\\nAvoid: Unrelated domain examples like math problems or general knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bp-4",
   "metadata": {},
   "source": [
    "### 4. Match Example Format to Expected Answers\n",
    "\n",
    "Examples should match the format you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bp-4-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For concise numerical answers\n",
    "numerical_examples = [\n",
    "    {\"question\": \"How many chromosomes...\", \"answer\": \"46\"},\n",
    "    {\"question\": \"How many subunits...\", \"answer\": \"4\"},\n",
    "]\n",
    "\n",
    "print(\"Numerical format examples:\")\n",
    "for ex in numerical_examples:\n",
    "    print(f\"  {ex['answer']}\")\n",
    "\n",
    "print(\"\\nTip: Match example style to desired output style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bp-5",
   "metadata": {},
   "source": [
    "### 5. Test With and Without Few-Shot\n",
    "\n",
    "Measure the impact of few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bp-5-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of comparing with and without few-shot\n",
    "# In practice, you would run both verifications\n",
    "\n",
    "print(\"Testing methodology:\")\n",
    "print(\"\")\n",
    "print(\"1. Baseline (no few-shot):\")\n",
    "print(\"   config_baseline = VerificationConfig(\")\n",
    "print(\"       answering_models=[model_config],\")\n",
    "print(\"       parsing_models=[model_config]\")\n",
    "print(\"   )\")\n",
    "print(\"\")\n",
    "print(\"2. With few-shot:\")\n",
    "print(\"   config_few_shot = VerificationConfig(\")\n",
    "print(\"       answering_models=[model_config],\")\n",
    "print(\"       parsing_models=[model_config],\")\n",
    "print(\"       few_shot_config=FewShotConfig(global_mode='k-shot', global_k=3)\")\n",
    "print(\"   )\")\n",
    "print(\"\")\n",
    "print(\"3. Compare pass rates to measure improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bp-6",
   "metadata": {},
   "source": [
    "### 6. Monitor Token Usage\n",
    "\n",
    "More examples consume more tokens:\n",
    "\n",
    "- Each example: ~50-200 tokens (depending on length)\n",
    "- 3 examples: ~150-600 tokens\n",
    "- 10 examples: ~500-2000 tokens\n",
    "\n",
    "**Watch for**: Context window limits, increased API costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bp-7",
   "metadata": {},
   "source": [
    "### 7. Use External Examples Sparingly\n",
    "\n",
    "Only add external examples when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bp-7-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Add domain-specific high-quality examples\n",
    "good_external = FewShotConfig(\n",
    "    global_external_examples=[{\"question\": \"High-quality domain example\", \"answer\": \"Perfect answer\"}]\n",
    ")\n",
    "\n",
    "print(\"Good: 1-2 high-quality external examples\")\n",
    "print(\"Bad: Too many unrelated external examples (50+)\")\n",
    "print(\"\")\n",
    "print(f\"External examples count: {len(good_external.global_external_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Issue 1: Examples Not Being Used\n",
    "\n",
    "**Symptom**: Few-shot enabled but no improvement in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify few-shot is enabled\n",
    "config_check = FewShotConfig(enabled=True, global_mode=\"k-shot\", global_k=3)\n",
    "\n",
    "print(f\"Few-shot enabled: {config_check.enabled}\")\n",
    "print(f\"Global mode: {config_check.global_mode}\")\n",
    "print(f\"Global k: {config_check.global_k}\")\n",
    "\n",
    "print(\"\\nSolutions:\")\n",
    "print(\"1. Verify few-shot is enabled\")\n",
    "print(\"2. Check questions have examples\")\n",
    "print(\"3. Check mode isn't 'none'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshoot-2",
   "metadata": {},
   "source": [
    "### Issue 2: Too Many Examples\n",
    "\n",
    "**Symptom**: LLM context limit exceeded, slow responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Reduce k value\n",
    "reduced_config = FewShotConfig(global_mode=\"k-shot\", global_k=2)\n",
    "\n",
    "print(\"Solution: Reduce k value\")\n",
    "print(f\"Reduced k to: {reduced_config.global_k}\")\n",
    "print(\"\")\n",
    "print(\"Alternative: Switch to custom selection to pick specific examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshoot-3",
   "metadata": {},
   "source": [
    "### Issue 3: Poor Example Quality\n",
    "\n",
    "**Symptom**: Few-shot makes results worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Use custom selection to pick better examples\n",
    "better_config = FewShotConfig.from_index_selections(\n",
    "    {\n",
    "        \"question_1\": [0, 2],  # Skip poor example at index 1\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Solutions:\")\n",
    "print(\"1. Review example quality\")\n",
    "print(\"2. Use custom selection to skip poor examples\")\n",
    "print(\"3. Add external high-quality examples\")\n",
    "print(\"\")\n",
    "print(f\"Custom mode: {better_config.global_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshoot-4",
   "metadata": {},
   "source": [
    "### Issue 4: Inconsistent Results\n",
    "\n",
    "**Symptom**: Results vary between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshoot-4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Set temperature to 0\n",
    "deterministic_config = ModelConfig(\n",
    "    id=\"gpt-4.1-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,  # Deterministic\n",
    "    interface=\"langchain\",\n",
    ")\n",
    "\n",
    "print(\"Solutions:\")\n",
    "print(\"1. Set temperature to 0\")\n",
    "print(f\"   Temperature: {deterministic_config.temperature}\")\n",
    "print(\"\")\n",
    "print(\"2. Use deterministic example selection\")\n",
    "print(\"   K-shot mode uses question ID as seed for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "### Token Usage\n",
    "\n",
    "Few-shot prompting increases token consumption:\n",
    "\n",
    "| Examples | Estimated Tokens (Input) | Cost Impact |\n",
    "|----------|-------------------------|-------------|\n",
    "| 0 (no few-shot) | Baseline | Baseline |\n",
    "| 2 examples | +100-400 tokens | +5-10% |\n",
    "| 5 examples | +250-1000 tokens | +10-20% |\n",
    "| 10 examples | +500-2000 tokens | +20-40% |\n",
    "\n",
    "### Latency\n",
    "\n",
    "More examples slightly increase latency:\n",
    "\n",
    "- Token generation time: ~50-100ms per 100 tokens\n",
    "- 3 examples: +50-200ms additional latency\n",
    "\n",
    "**Recommendation:** Start with k=2-3 to balance quality and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once you have few-shot prompting configured, you can:\n",
    "\n",
    "- [Verification](../using-karenina/verification.md) - Run verifications with few-shot\n",
    "- [Presets](presets.md) - Save few-shot configurations in presets\n",
    "- [Deep-Judgment](deep-judgment.md) - Combine with deep-judgment parsing\n",
    "- [Templates](../using-karenina/templates.md) - Design templates that work with few-shot\n",
    "\n",
    "## Related Documentation\n",
    "\n",
    "- [Verification](../using-karenina/verification.md) - Core verification workflow\n",
    "- [Adding Questions](../using-karenina/adding-questions.md) - How to add questions with examples\n",
    "- [Presets](presets.md) - Save few-shot configurations\n",
    "- [Templates](../using-karenina/templates.md) - Template creation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}