{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c27942",
   "metadata": {},
   "source": [
    "# Loading a Benchmark\n",
    "\n",
    "Before running verification, you need to load your benchmark from a checkpoint file (JSON-LD) or a database. This page covers `Benchmark.load()`, inspecting questions, templates, and rubrics, and preparing for verification.\n",
    "\n",
    "For background on the checkpoint format, see [Checkpoints](../04-core-concepts/checkpoints.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3b2550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:45.983915Z",
     "iopub.status.busy": "2026-02-06T00:49:45.983799Z",
     "iopub.status.idle": "2026-02-06T00:49:45.988043Z",
     "shell.execute_reply": "2026-02-06T00:49:45.987677Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell (hidden in rendered docs).\n",
    "# No mocking needed — all examples load from the test checkpoint file.\n",
    "import os as _os\n",
    "\n",
    "_os.chdir(_os.path.dirname(_os.path.abspath(\"__file__\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46abb09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loading from a File\n",
    "\n",
    "Use `Benchmark.load()` to load a checkpoint from a JSON-LD file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e667848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:45.989892Z",
     "iopub.status.busy": "2026-02-06T00:49:45.989743Z",
     "iopub.status.idle": "2026-02-06T00:49:46.304848Z",
     "shell.execute_reply": "2026-02-06T00:49:46.304587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        Documentation Test Benchmark\n",
      "Description: Sample benchmark with 5 questions for use in documentation examples\n",
      "Version:     1.0.0\n",
      "Creator:     Karenina Documentation\n",
      "Questions:   5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from karenina.benchmark import Benchmark\n",
    "\n",
    "benchmark = Benchmark.load(Path(\"test_checkpoint.jsonld\"))\n",
    "\n",
    "print(f\"Name:        {benchmark.name}\")\n",
    "print(f\"Description: {benchmark.description}\")\n",
    "print(f\"Version:     {benchmark.version}\")\n",
    "print(f\"Creator:     {benchmark.creator}\")\n",
    "print(f\"Questions:   {benchmark.question_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357780b",
   "metadata": {},
   "source": [
    "`Benchmark.load()` parses the JSON-LD structure, validates it, and rebuilds the internal question/template/rubric caches. The returned `Benchmark` object is ready for inspection and verification.\n",
    "\n",
    "---\n",
    "\n",
    "## Loading from a Database\n",
    "\n",
    "If your benchmark is stored in a database (via `save_to_db()`), load it by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba6e9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.305964Z",
     "iopub.status.busy": "2026-02-06T00:49:46.305878Z",
     "iopub.status.idle": "2026-02-06T00:49:46.307392Z",
     "shell.execute_reply": "2026-02-06T00:49:46.307156Z"
    }
   },
   "outputs": [],
   "source": [
    "# benchmark = Benchmark.load_from_db(\"My Benchmark\", storage=\"sqlite:///benchmarks.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3033048",
   "metadata": {},
   "source": [
    "The `storage` parameter is a SQLAlchemy connection string. This is useful when benchmarks are managed through the karenina-server or GUI.\n",
    "\n",
    "---\n",
    "\n",
    "## Inspecting Questions\n",
    "\n",
    "### Listing Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f4fa58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.308324Z",
     "iopub.status.busy": "2026-02-06T00:49:46.308272Z",
     "iopub.status.idle": "2026-02-06T00:49:46.309591Z",
     "shell.execute_reply": "2026-02-06T00:49:46.309408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question IDs (5):\n",
      "  urn:uuid:question-what-is-the-capital-of-france-cb0b4aaf\n",
      "  urn:uuid:question-what-is-6-multiplied-by-7-aba99ec8\n",
      "  urn:uuid:question-what-element-has-the-atomic-number-8-provide-both--8bcf4ce2\n",
      "  urn:uuid:question-is-17-a-prime-number-7abec4b4\n",
      "  urn:uuid:question-explain-the-concept-of-machine-learning-in-simple--de73020c\n"
     ]
    }
   ],
   "source": [
    "# Get all question IDs\n",
    "question_ids = benchmark.get_question_ids()\n",
    "print(f\"Question IDs ({len(question_ids)}):\")\n",
    "for qid in question_ids:\n",
    "    print(f\"  {qid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0bd3ca",
   "metadata": {},
   "source": [
    "### Getting Question Details\n",
    "\n",
    "Each question is returned as a dictionary with all its fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831e4b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.310589Z",
     "iopub.status.busy": "2026-02-06T00:49:46.310522Z",
     "iopub.status.idle": "2026-02-06T00:49:46.312240Z",
     "shell.execute_reply": "2026-02-06T00:49:46.312052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:   What is the capital of France?\n",
      "Answer:     Paris\n",
      "Finished:   True\n",
      "Has rubric: True\n"
     ]
    }
   ],
   "source": [
    "# Get the first question\n",
    "qid = benchmark.get_question_ids()[0]\n",
    "question = benchmark.get_question(qid)\n",
    "\n",
    "print(f\"Question:   {question['question']}\")\n",
    "print(f\"Answer:     {question['raw_answer']}\")\n",
    "print(f\"Finished:   {question.get('finished', False)}\")\n",
    "print(f\"Has rubric: {'question_rubric' in question and question['question_rubric'] is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce07a4",
   "metadata": {},
   "source": [
    "### Getting All Questions\n",
    "\n",
    "Use `get_all_questions()` for bulk access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3c5ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.313112Z",
     "iopub.status.busy": "2026-02-06T00:49:46.313059Z",
     "iopub.status.idle": "2026-02-06T00:49:46.314849Z",
     "shell.execute_reply": "2026-02-06T00:49:46.314652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 5\n",
      "  - What is the capital of France?...\n",
      "  - What is 6 multiplied by 7?...\n",
      "  - What element has the atomic number 8? Provide both the name ...\n",
      "  - Is 17 a prime number?...\n",
      "  - Explain the concept of machine learning in simple terms....\n",
      "\n",
      "IDs only: 5 questions\n"
     ]
    }
   ],
   "source": [
    "# Full question dictionaries\n",
    "all_questions = benchmark.get_all_questions()\n",
    "print(f\"Total questions: {len(all_questions)}\")\n",
    "for q in all_questions:\n",
    "    print(f\"  - {q['question'][:60]}...\")\n",
    "\n",
    "# IDs only (lighter)\n",
    "ids = benchmark.get_all_questions(ids_only=True)\n",
    "print(f\"\\nIDs only: {len(ids)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb110b",
   "metadata": {},
   "source": [
    "### Question Metadata\n",
    "\n",
    "For a structured metadata summary including template and rubric status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295c3ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.315700Z",
     "iopub.status.busy": "2026-02-06T00:49:46.315653Z",
     "iopub.status.idle": "2026-02-06T00:49:46.317256Z",
     "shell.execute_reply": "2026-02-06T00:49:46.317072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:     What is the capital of France?...\n",
      "Has template: True\n",
      "Has rubric:   True\n",
      "Finished:     True\n",
      "Created:      2026-02-05T23:06:59.237875\n"
     ]
    }
   ],
   "source": [
    "qid = benchmark.get_question_ids()[0]\n",
    "meta = benchmark.get_question_metadata(qid)\n",
    "\n",
    "print(f\"Question:     {meta['question'][:50]}...\")\n",
    "print(f\"Has template: {meta['has_template']}\")\n",
    "print(f\"Has rubric:   {meta['has_rubric']}\")\n",
    "print(f\"Finished:     {meta['finished']}\")\n",
    "print(f\"Created:      {meta['date_created']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c47ed0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inspecting Templates\n",
    "\n",
    "Templates define how answers are parsed and verified. Not every question needs a template — questions without templates can still be evaluated using rubric-only mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458ac080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.318132Z",
     "iopub.status.busy": "2026-02-06T00:49:46.318079Z",
     "iopub.status.idle": "2026-02-06T00:49:46.319592Z",
     "shell.execute_reply": "2026-02-06T00:49:46.319436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[template] What is the capital of France?...\n",
      "[template] What is 6 multiplied by 7?...\n",
      "[template] What element has the atomic number 8? Provide both...\n",
      "[template] Is 17 a prime number?...\n",
      "[none]     Explain the concept of machine learning in simple ...\n"
     ]
    }
   ],
   "source": [
    "# Check which questions have templates\n",
    "for qid in benchmark.get_question_ids():\n",
    "    has_it = benchmark.has_template(qid)\n",
    "    q = benchmark.get_question(qid)\n",
    "    print(f\"{'[template]' if has_it else '[none]    '} {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b1baf",
   "metadata": {},
   "source": [
    "### Viewing Template Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3550a96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.320502Z",
     "iopub.status.busy": "2026-02-06T00:49:46.320427Z",
     "iopub.status.idle": "2026-02-06T00:49:46.321884Z",
     "shell.execute_reply": "2026-02-06T00:49:46.321734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pydantic import Field\n",
      "from karenina.schemas.entities import BaseAnswer\n",
      "\n",
      "class Answer(BaseAnswer):\n",
      "    capital: str = Field(description=\"The capital city mentioned in the response\")\n",
      "\n",
      "    def model_post_init(self, __context):\n",
      "        self.correct = {\"capital\": \"Paris\"}\n",
      "\n",
      "    def verify(self) -> bool:\n",
      "        return self.capital.strip().lower() == self.correct[\"capital\"].lower()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the template code for a question that has one\n",
    "qid = benchmark.get_question_ids()[0]\n",
    "if benchmark.has_template(qid):\n",
    "    code = benchmark.get_template(qid)\n",
    "    print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a2fb8",
   "metadata": {},
   "source": [
    "### Finished Templates\n",
    "\n",
    "The `get_finished_templates()` method returns templates that are ready for verification (questions marked as finished with non-default templates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f0ceaa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.322715Z",
     "iopub.status.busy": "2026-02-06T00:49:46.322662Z",
     "iopub.status.idle": "2026-02-06T00:49:46.324206Z",
     "shell.execute_reply": "2026-02-06T00:49:46.324030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Templates ready for verification: 4\n",
      "  - What is the capital of France?: 383 chars\n",
      "  - What is 6 multiplied by 7?: 345 chars\n",
      "  - What element has the atomic number 8? Provide both the name and chemical symbol.: 527 chars\n",
      "  - Is 17 a prime number?: 247 chars\n"
     ]
    }
   ],
   "source": [
    "finished = benchmark.get_finished_templates()\n",
    "print(f\"Templates ready for verification: {len(finished)}\")\n",
    "for ft in finished:\n",
    "    print(f\"  - {ft.question_preview}: {len(ft.template_code)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463f26a",
   "metadata": {},
   "source": [
    "### Missing Templates\n",
    "\n",
    "Find questions that still need templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654217d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.325105Z",
     "iopub.status.busy": "2026-02-06T00:49:46.325052Z",
     "iopub.status.idle": "2026-02-06T00:49:46.326525Z",
     "shell.execute_reply": "2026-02-06T00:49:46.326364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions without templates: 1\n",
      "  - Explain the concept of machine learning in simple terms....\n"
     ]
    }
   ],
   "source": [
    "missing = benchmark.get_missing_templates()\n",
    "print(f\"Questions without templates: {len(missing)}\")\n",
    "for m in missing:\n",
    "    print(f\"  - {m['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8209b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inspecting Rubrics\n",
    "\n",
    "Rubrics evaluate response quality through traits. They can be **global** (applied to all questions) or **question-specific**.\n",
    "\n",
    "### Global Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca00151e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.327384Z",
     "iopub.status.busy": "2026-02-06T00:49:46.327320Z",
     "iopub.status.idle": "2026-02-06T00:49:46.328896Z",
     "shell.execute_reply": "2026-02-06T00:49:46.328738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No global rubric defined\n"
     ]
    }
   ],
   "source": [
    "global_rubric = benchmark.get_global_rubric()\n",
    "if global_rubric:\n",
    "    print(f\"Global LLM traits:      {len(global_rubric.llm_traits)}\")\n",
    "    print(f\"Global regex traits:    {len(global_rubric.regex_traits)}\")\n",
    "    print(f\"Global callable traits: {len(global_rubric.callable_traits)}\")\n",
    "    print(f\"Global metric traits:   {len(global_rubric.metric_traits)}\")\n",
    "else:\n",
    "    print(\"No global rubric defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c465202a",
   "metadata": {},
   "source": [
    "### Question-Specific Rubrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "524ae453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.329781Z",
     "iopub.status.busy": "2026-02-06T00:49:46.329733Z",
     "iopub.status.idle": "2026-02-06T00:49:46.331410Z",
     "shell.execute_reply": "2026-02-06T00:49:46.331236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?... — 1 trait(s)\n",
      "What is 6 multiplied by 7?... — 1 trait(s)\n",
      "What element has the atomic number 8? Pr... — 1 trait(s)\n",
      "Is 17 a prime number?... — no question rubric\n",
      "Explain the concept of machine learning ... — no question rubric\n"
     ]
    }
   ],
   "source": [
    "for qid in benchmark.get_question_ids():\n",
    "    q = benchmark.get_question(qid)\n",
    "    rubric = q.get(\"question_rubric\")\n",
    "    if rubric:\n",
    "        trait_count = sum(\n",
    "            len(rubric.get(k, [])) for k in [\"llm_traits\", \"regex_traits\", \"callable_traits\", \"metric_traits\"]\n",
    "        )\n",
    "        print(f\"{q['question'][:40]}... — {trait_count} trait(s)\")\n",
    "    else:\n",
    "        print(f\"{q['question'][:40]}... — no question rubric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63b40c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark Status\n",
    "\n",
    "Use the status properties to understand the overall state of a loaded benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be376a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:49:46.332185Z",
     "iopub.status.busy": "2026-02-06T00:49:46.332140Z",
     "iopub.status.idle": "2026-02-06T00:49:46.333747Z",
     "shell.execute_reply": "2026-02-06T00:49:46.333573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:          Documentation Test Benchmark\n",
      "Questions:     5\n",
      "Finished:      4\n",
      "Empty:         False\n",
      "Complete:      False\n",
      "Progress:      80.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Name:          {benchmark.name}\")\n",
    "print(f\"Questions:     {benchmark.question_count}\")\n",
    "print(f\"Finished:      {benchmark.finished_count}\")\n",
    "print(f\"Empty:         {benchmark.is_empty}\")\n",
    "print(f\"Complete:      {benchmark.is_complete}\")\n",
    "print(f\"Progress:      {benchmark.get_progress():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a217d02",
   "metadata": {},
   "source": [
    "A benchmark is **complete** when all questions are finished and have templates. The `get_progress()` method returns a percentage based on the ratio of finished questions to total questions.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Configure verification settings](verification-config.md) — set up models, evaluation mode, and feature flags\n",
    "- [Run verification via Python API](python-api.md) — full end-to-end example\n",
    "- [Run verification via CLI](cli.md) — command-line workflow\n",
    "- [Analyze results](../07-analyzing-results/index.md) — inspect and export verification outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
