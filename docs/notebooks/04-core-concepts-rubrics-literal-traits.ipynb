{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da1ea94",
   "metadata": {},
   "source": [
    "# Literal Traits\n",
    "\n",
    "Literal traits are a kind of `LLMRubricTrait` that perform **ordered categorical classification**. Instead of a binary yes/no (boolean) or a numeric scale (score), the parsing model classifies the response into one of several predefined categories. The result is the **class index** — an integer indicating which category was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f460256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:40:59.725331Z",
     "iopub.status.busy": "2026-02-06T01:40:59.725131Z",
     "iopub.status.idle": "2026-02-06T01:40:59.729908Z",
     "shell.execute_reply": "2026-02-06T01:40:59.729456Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mock cell: ensures examples execute without live API keys.\n",
    "# This cell is hidden in rendered documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877a51d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A literal trait is created by setting `kind=\"literal\"` on `LLMRubricTrait` and providing a `classes` dictionary. The classes define the available categories and their descriptions, which are shown to the parsing model.\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `name` | `str` | Human-readable identifier for the trait |\n",
    "| `description` | `str \\| None` | Detailed description shown to the parsing model |\n",
    "| `kind` | `\"literal\"` | Must be `\"literal\"` for categorical classification |\n",
    "| `classes` | `dict[str, str]` | Class name → description mapping (2-20 classes, order matters) |\n",
    "| `higher_is_better` | `bool` | Whether higher class indices indicate better performance |\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- **Ordered**: Dictionary order determines indices (0, 1, 2, ...)\n",
    "- **Auto-ranged**: `min_score` is set to 0, `max_score` to `len(classes) - 1` automatically\n",
    "- **Descriptive**: Each class has a name and a description to guide the parsing model\n",
    "- **2-20 classes**: Must have at least 2 and at most 20 categories\n",
    "\n",
    "## Tone Classification\n",
    "\n",
    "A common use case is classifying the tone or style of a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c01f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:40:59.732051Z",
     "iopub.status.busy": "2026-02-06T01:40:59.731907Z",
     "iopub.status.idle": "2026-02-06T01:41:00.060467Z",
     "shell.execute_reply": "2026-02-06T01:41:00.060200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kind: literal\n",
      "Classes: ['overly_simple', 'accessible', 'technical']\n",
      "Score range: 0 to 2\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait\n",
    "\n",
    "tone_trait = LLMRubricTrait(\n",
    "    name=\"Response Tone\",\n",
    "    description=\"Classify the overall tone of this response.\",\n",
    "    kind=\"literal\",\n",
    "    classes={\n",
    "        \"overly_simple\": \"Uses childish language, oversimplifies to the point of inaccuracy\",\n",
    "        \"accessible\": \"Clear and approachable while remaining accurate\",\n",
    "        \"technical\": \"Uses domain-specific jargon, assumes background knowledge\",\n",
    "    },\n",
    "    higher_is_better=False,  # Context-dependent — no inherent \"better\" direction\n",
    ")\n",
    "\n",
    "print(f\"Kind: {tone_trait.kind}\")\n",
    "print(f\"Classes: {list(tone_trait.classes.keys())}\")\n",
    "print(f\"Score range: {tone_trait.min_score} to {tone_trait.max_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674e0dc",
   "metadata": {},
   "source": [
    "The parsing model receives the class names and descriptions, then selects the one that best fits the response. The result is the class index:\n",
    "\n",
    "- `0` → \"overly_simple\"\n",
    "- `1` → \"accessible\"\n",
    "- `2` → \"technical\"\n",
    "\n",
    "## Quality Tiers\n",
    "\n",
    "Another common pattern is defining quality levels where order is meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee39287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:41:00.061578Z",
     "iopub.status.busy": "2026-02-06T01:41:00.061502Z",
     "iopub.status.idle": "2026-02-06T01:41:00.063364Z",
     "shell.execute_reply": "2026-02-06T01:41:00.063144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['poor', 'acceptable', 'good', 'excellent']\n",
      "Score range: 0 to 3\n",
      "higher_is_better: True\n"
     ]
    }
   ],
   "source": [
    "quality_trait = LLMRubricTrait(\n",
    "    name=\"Answer Quality\",\n",
    "    description=\"Rate the overall quality of this answer.\",\n",
    "    kind=\"literal\",\n",
    "    classes={\n",
    "        \"poor\": \"Incorrect, misleading, or largely irrelevant\",\n",
    "        \"acceptable\": \"Broadly correct but missing important details\",\n",
    "        \"good\": \"Correct and well-structured with adequate detail\",\n",
    "        \"excellent\": \"Comprehensive, precise, and well-organized\",\n",
    "    },\n",
    "    higher_is_better=True,  # Higher index = better quality\n",
    ")\n",
    "\n",
    "print(f\"Classes: {list(quality_trait.classes.keys())}\")\n",
    "print(f\"Score range: {quality_trait.min_score} to {quality_trait.max_score}\")\n",
    "print(f\"higher_is_better: {quality_trait.higher_is_better}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278379e",
   "metadata": {},
   "source": [
    "Here `higher_is_better=True` because later classes (higher indices) represent better quality:\n",
    "\n",
    "- `0` → \"poor\" (worst)\n",
    "- `1` → \"acceptable\"\n",
    "- `2` → \"good\"\n",
    "- `3` → \"excellent\" (best)\n",
    "\n",
    "## How `higher_is_better` Works\n",
    "\n",
    "The `higher_is_better` field tells Karenina which direction is \"better\" when interpreting scores:\n",
    "\n",
    "| `higher_is_better` | Interpretation | Example |\n",
    "|---------------------|---------------|---------|\n",
    "| `True` | Higher class indices are better | Quality: poor(0) → excellent(3) |\n",
    "| `False` | Lower class indices are better — or no inherent direction | Tone: classification without preference |\n",
    "\n",
    "This field does **not** change how the parsing model classifies — it only affects how results are interpreted in summaries and comparisons.\n",
    "\n",
    "## Working with Class Names\n",
    "\n",
    "`LLMRubricTrait` provides helper methods for converting between class names and indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ccb3fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:41:00.064309Z",
     "iopub.status.busy": "2026-02-06T01:41:00.064255Z",
     "iopub.status.idle": "2026-02-06T01:41:00.065681Z",
     "shell.execute_reply": "2026-02-06T01:41:00.065494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['poor', 'acceptable', 'good', 'excellent']\n",
      "Index of 'good': 2\n",
      "Index of 'poor': 0\n",
      "Index of 'unknown': -1\n"
     ]
    }
   ],
   "source": [
    "# Get the ordered list of class names\n",
    "print(f\"Class names: {quality_trait.get_class_names()}\")\n",
    "\n",
    "# Get the index for a specific class\n",
    "print(f\"Index of 'good': {quality_trait.get_class_index('good')}\")\n",
    "print(f\"Index of 'poor': {quality_trait.get_class_index('poor')}\")\n",
    "\n",
    "# Invalid class names return -1\n",
    "print(f\"Index of 'unknown': {quality_trait.get_class_index('unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce154f2c",
   "metadata": {},
   "source": [
    "The `get_class_index()` method returns `-1` for unrecognized class names. This value is also accepted by `validate_score()` as a valid error state for literal traits.\n",
    "\n",
    "## Score Validation\n",
    "\n",
    "Literal traits validate scores the same way as score traits — the value must be an integer within the auto-derived range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15836c9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:41:00.066532Z",
     "iopub.status.busy": "2026-02-06T01:41:00.066481Z",
     "iopub.status.idle": "2026-02-06T01:41:00.068093Z",
     "shell.execute_reply": "2026-02-06T01:41:00.067898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 0 valid? True\n",
      "Is 3 valid? True\n",
      "Is -1 valid? True\n",
      "Is 4 valid? False\n",
      "Is True valid? False\n"
     ]
    }
   ],
   "source": [
    "# Valid scores\n",
    "print(f\"Is 0 valid? {quality_trait.validate_score(0)}\")   # First class\n",
    "print(f\"Is 3 valid? {quality_trait.validate_score(3)}\")   # Last class\n",
    "print(f\"Is -1 valid? {quality_trait.validate_score(-1)}\")  # Error state\n",
    "\n",
    "# Invalid scores\n",
    "print(f\"Is 4 valid? {quality_trait.validate_score(4)}\")    # Out of range\n",
    "print(f\"Is True valid? {quality_trait.validate_score(True)}\")  # Boolean rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27d6bfd",
   "metadata": {},
   "source": [
    "## Writing Good Class Descriptions\n",
    "\n",
    "The quality of class descriptions directly affects how well the parsing model classifies responses. Good descriptions are:\n",
    "\n",
    "- **Mutually exclusive**: Each class should be clearly distinct from the others\n",
    "- **Observable**: Describe what the model should look for in the response\n",
    "- **Ordered consistently**: If using `higher_is_better`, ensure the natural ordering matches\n",
    "\n",
    "**Good** — clear criteria the model can evaluate:\n",
    "\n",
    "    \"poor\": \"Incorrect, misleading, or largely irrelevant to the question\"\n",
    "    \"acceptable\": \"Broadly correct but missing important details or nuance\"\n",
    "    \"good\": \"Correct and well-structured with adequate supporting detail\"\n",
    "    \"excellent\": \"Comprehensive, precise, well-organized, and addresses edge cases\"\n",
    "\n",
    "**Weak** — vague or overlapping:\n",
    "\n",
    "    \"bad\": \"A bad answer\"\n",
    "    \"ok\": \"An okay answer\"\n",
    "    \"good\": \"A good answer\"\n",
    "\n",
    "## Deep Judgment\n",
    "\n",
    "Like boolean and score traits, literal traits support [deep judgment](llm-traits.md#deep-judgment-optional) for evidence-based evaluation. When enabled, the parsing model extracts verbatim excerpts from the response and provides reasoning for its classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b16891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:41:00.069047Z",
     "iopub.status.busy": "2026-02-06T01:41:00.068977Z",
     "iopub.status.idle": "2026-02-06T01:41:00.070677Z",
     "shell.execute_reply": "2026-02-06T01:41:00.070493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep judgment enabled: True\n",
      "Excerpt extraction: True\n"
     ]
    }
   ],
   "source": [
    "deep_quality_trait = LLMRubricTrait(\n",
    "    name=\"Answer Quality (Deep)\",\n",
    "    description=\"Rate the overall quality of this answer.\",\n",
    "    kind=\"literal\",\n",
    "    classes={\n",
    "        \"poor\": \"Incorrect, misleading, or largely irrelevant\",\n",
    "        \"acceptable\": \"Broadly correct but missing important details\",\n",
    "        \"good\": \"Correct and well-structured with adequate detail\",\n",
    "        \"excellent\": \"Comprehensive, precise, and well-organized\",\n",
    "    },\n",
    "    higher_is_better=True,\n",
    "    deep_judgment_enabled=True,\n",
    "    deep_judgment_excerpt_enabled=True,\n",
    ")\n",
    "\n",
    "print(f\"Deep judgment enabled: {deep_quality_trait.deep_judgment_enabled}\")\n",
    "print(f\"Excerpt extraction: {deep_quality_trait.deep_judgment_excerpt_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e2b35",
   "metadata": {},
   "source": [
    "See [LLM Traits — Deep Judgment](llm-traits.md#deep-judgment-optional) for configuration details including retry attempts, fuzzy match thresholds, and search-enhanced detection.\n",
    "\n",
    "## Using Literal Traits in a Rubric\n",
    "\n",
    "Literal traits are added to rubrics just like other trait types — as global or question-specific traits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba9ae3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T01:41:00.071547Z",
     "iopub.status.busy": "2026-02-06T01:41:00.071498Z",
     "iopub.status.idle": "2026-02-06T01:41:00.073133Z",
     "shell.execute_reply": "2026-02-06T01:41:00.072957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rubric has 2 literal traits\n",
      "  Response Tone: 3 classes, range 0-2\n",
      "  Answer Quality: 4 classes, range 0-3\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import Rubric\n",
    "\n",
    "rubric = Rubric(\n",
    "    llm_traits=[\n",
    "        LLMRubricTrait(\n",
    "            name=\"Response Tone\",\n",
    "            description=\"Classify the overall tone.\",\n",
    "            kind=\"literal\",\n",
    "            classes={\n",
    "                \"overly_simple\": \"Oversimplifies, childish language\",\n",
    "                \"accessible\": \"Clear and accurate without jargon\",\n",
    "                \"technical\": \"Domain-specific, assumes expertise\",\n",
    "            },\n",
    "            higher_is_better=False,\n",
    "        ),\n",
    "        LLMRubricTrait(\n",
    "            name=\"Answer Quality\",\n",
    "            description=\"Rate the overall quality.\",\n",
    "            kind=\"literal\",\n",
    "            classes={\n",
    "                \"poor\": \"Incorrect or irrelevant\",\n",
    "                \"acceptable\": \"Broadly correct, missing details\",\n",
    "                \"good\": \"Correct and well-structured\",\n",
    "                \"excellent\": \"Comprehensive and precise\",\n",
    "            },\n",
    "            higher_is_better=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Rubric has {len(rubric.llm_traits)} literal traits\")\n",
    "for trait in rubric.llm_traits:\n",
    "    print(f\"  {trait.name}: {len(trait.classes)} classes, range 0-{trait.max_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e500b54",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [LLM Traits](llm-traits.md) — boolean and score kinds of `LLMRubricTrait`\n",
    "- [Regex Traits](regex-traits.md) — pattern matching on raw response text\n",
    "- [Callable Traits](callable-traits.md) — custom Python evaluation functions\n",
    "- [Rubrics Overview](index.md) — when to use each trait type\n",
    "- [Defining Rubrics](../../05-creating-benchmarks/defining-rubrics.md) — adding traits to a benchmark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
