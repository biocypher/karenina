{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Custom Templates: A Progressive Tutorial\n",
    "\n",
    "This notebook demonstrates how to write custom answer templates in Karenina, progressing from simple patterns to advanced multi-field evaluation. Templates define how a Judge LLM should parse model responses and how correctness is verified.\n",
    "\n",
    "For conceptual background, see [Answer Templates](../04-core-concepts/answer-templates.md). For the full writing guide, see [Writing Custom Templates](../05-creating-benchmarks/writing-templates.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell: ensures examples execute without live API keys.\n",
    "# This cell is hidden in rendered documentation.\n",
    "from pydantic import Field\n",
    "\n",
    "from karenina.schemas.entities import BaseAnswer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Template Basics\n",
    "\n",
    "Every template is a Pydantic model named `Answer` that extends `BaseAnswer`. It defines:\n",
    "\n",
    "- **Attributes** — what the Judge LLM should extract from the response\n",
    "- **Ground truth** — the expected correct values (set in `model_post_init`)\n",
    "- **`verify()`** — comparison logic returning `True` (pass) or `False` (fail)\n",
    "\n",
    "### Case-Insensitive String Matching\n",
    "\n",
    "The simplest pattern: normalize strings before comparison to handle common variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    gene_symbol: str = Field(\n",
    "        description=\"The gene symbol mentioned in the response\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"gene_symbol\": \"TP53\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.gene_symbol.strip().upper() == self.correct[\"gene_symbol\"].upper()\n",
    "\n",
    "\n",
    "# Test with common variations a Judge LLM might extract\n",
    "for variant in [\"tp53\", \" TP53 \", \"Tp53\", \"P53\"]:\n",
    "    parsed = Answer(gene_symbol=variant)\n",
    "    print(f\"{variant!r:12s} → verify(): {parsed.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Numeric Tolerance\n",
    "\n",
    "Exact numeric comparison is often too strict. Use absolute or relative tolerance depending on the value range.\n",
    "\n",
    "### Absolute Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    temperature: float = Field(\n",
    "        description=\"The boiling point temperature in degrees Celsius\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"temperature\": 100.0}\n",
    "        self.tolerance = 0.5  # Accept within ±0.5°C\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return abs(self.temperature - self.correct[\"temperature\"]) <= self.tolerance\n",
    "\n",
    "\n",
    "for temp in [100.0, 99.8, 100.3, 101.0]:\n",
    "    parsed = Answer(temperature=temp)\n",
    "    print(f\"{temp:6.1f}°C → verify(): {parsed.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y5z6a7b8",
   "metadata": {},
   "source": [
    "### Percentage-Based Tolerance\n",
    "\n",
    "For values spanning wide ranges, relative tolerance adapts to the magnitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    population: int = Field(\n",
    "        description=\"The estimated population of the city\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"population\": 8_336_817}\n",
    "        self.tolerance_pct = 10  # Accept within 10%\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        expected = self.correct[\"population\"]\n",
    "        threshold = expected * (self.tolerance_pct / 100)\n",
    "        return abs(self.population - expected) <= threshold\n",
    "\n",
    "\n",
    "for pop in [8_336_817, 8_000_000, 9_000_000, 7_000_000]:\n",
    "    parsed = Answer(population=pop)\n",
    "    print(f\"{pop:>10,} → verify(): {parsed.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g3h4i5j6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## List Comparison\n",
    "\n",
    "When evaluating lists, decide whether order matters and whether extra items are acceptable.\n",
    "\n",
    "### Set-Based (Order Doesn't Matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k7l8m9n0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    symptoms: list[str] = Field(\n",
    "        description=\"The symptoms of the condition listed in the response\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"symptoms\": [\"fever\", \"cough\", \"fatigue\"]}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        extracted = {s.strip().lower() for s in self.symptoms}\n",
    "        expected = {s.lower() for s in self.correct[\"symptoms\"]}\n",
    "        return extracted == expected\n",
    "\n",
    "\n",
    "# Order doesn't matter; normalization handles case\n",
    "parsed = Answer(symptoms=[\"Fatigue\", \"fever\", \"Cough\"])\n",
    "print(f\"Extracted: {parsed.symptoms}\")\n",
    "print(f\"verify():  {parsed.verify()}\")\n",
    "\n",
    "# Missing items fail\n",
    "parsed2 = Answer(symptoms=[\"fever\", \"cough\"])\n",
    "print(f\"\\nMissing item: {parsed2.symptoms}\")\n",
    "print(f\"verify():     {parsed2.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1p2q3r4",
   "metadata": {},
   "source": [
    "### Subset Matching (Extra Items OK)\n",
    "\n",
    "Sometimes you want to check that required items are present, but extra items are acceptable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5t6u7v8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    proteins: list[str] = Field(\n",
    "        description=\"Proteins involved in the signaling pathway mentioned in the response\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"required_proteins\": [\"EGFR\", \"RAS\", \"RAF\"]}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        extracted = {p.strip().upper() for p in self.proteins}\n",
    "        required = {p.upper() for p in self.correct[\"required_proteins\"]}\n",
    "        return required.issubset(extracted)\n",
    "\n",
    "\n",
    "# Extra proteins are fine\n",
    "parsed = Answer(proteins=[\"EGFR\", \"RAS\", \"RAF\", \"MEK\", \"ERK\"])\n",
    "print(f\"Extracted: {parsed.proteins}\")\n",
    "print(f\"verify():  {parsed.verify()}\")\n",
    "\n",
    "# Missing a required protein fails\n",
    "parsed2 = Answer(proteins=[\"EGFR\", \"MEK\"])\n",
    "print(f\"\\nMissing required: {parsed2.proteins}\")\n",
    "print(f\"verify():         {parsed2.verify()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w9x0y1z2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-Field Templates with Partial Credit\n",
    "\n",
    "For templates with multiple attributes, implement both `verify()` (all-or-nothing) and `verify_granular()` (partial credit as a float from 0.0 to 1.0). The verification pipeline automatically calls `verify_granular()` when present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    drug_name: str = Field(\n",
    "        description=\"The name of the drug mentioned in the response\"\n",
    "    )\n",
    "    target: str = Field(\n",
    "        description=\"The protein target of the drug\"\n",
    "    )\n",
    "    mechanism: str = Field(\n",
    "        description=\"The mechanism of action (e.g., inhibitor, agonist)\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\n",
    "            \"drug_name\": \"venetoclax\",\n",
    "            \"target\": \"BCL2\",\n",
    "            \"mechanism\": \"inhibitor\",\n",
    "        }\n",
    "\n",
    "    def _check_drug_name(self) -> bool:\n",
    "        return self.drug_name.strip().lower() == self.correct[\"drug_name\"].lower()\n",
    "\n",
    "    def _check_target(self) -> bool:\n",
    "        extracted = self.target.strip().upper().replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "        expected = self.correct[\"target\"].upper().replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "        return extracted == expected\n",
    "\n",
    "    def _check_mechanism(self) -> bool:\n",
    "        return self.mechanism.strip().lower() == self.correct[\"mechanism\"].lower()\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self._check_drug_name() and self._check_target() and self._check_mechanism()\n",
    "\n",
    "    def verify_granular(self) -> float:\n",
    "        checks = [self._check_drug_name(), self._check_target(), self._check_mechanism()]\n",
    "        return sum(checks) / len(checks)\n",
    "\n",
    "\n",
    "# 2 out of 3 correct\n",
    "parsed = Answer(drug_name=\"Venetoclax\", target=\"Bcl-2\", mechanism=\"agonist\")\n",
    "print(f\"Drug:      {parsed._check_drug_name()}\")\n",
    "print(f\"Target:    {parsed._check_target()}\")\n",
    "print(f\"Mechanism: {parsed._check_mechanism()}\")\n",
    "print(f\"verify():          {parsed.verify()}\")\n",
    "print(f\"verify_granular(): {parsed.verify_granular():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8g9h0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Boolean Attribute Pattern\n",
    "\n",
    "Instead of extracting text and matching strings, ask the Judge LLM to make boolean judgments directly. This avoids string normalization pitfalls entirely — the LLM answers yes/no for each concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1j2k3l4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseAnswer):\n",
    "    mentions_bcl2: bool = Field(\n",
    "        description=\"True if the response identifies BCL2 (or BCL-2, Bcl-2) as the target\"\n",
    "    )\n",
    "    mentions_inhibition: bool = Field(\n",
    "        description=\"True if the response describes inhibition as the mechanism of action\"\n",
    "    )\n",
    "    mentions_apoptosis: bool = Field(\n",
    "        description=\"True if the response mentions apoptosis or programmed cell death\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\n",
    "            \"mentions_bcl2\": True,\n",
    "            \"mentions_inhibition\": True,\n",
    "            \"mentions_apoptosis\": True,\n",
    "        }\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return all(\n",
    "            getattr(self, field) == self.correct[field]\n",
    "            for field in self.correct\n",
    "        )\n",
    "\n",
    "    def verify_granular(self) -> float:\n",
    "        matches = sum(\n",
    "            1 for field in self.correct\n",
    "            if getattr(self, field) == self.correct[field]\n",
    "        )\n",
    "        return matches / len(self.correct)\n",
    "\n",
    "\n",
    "# Simulates a response that missed one concept\n",
    "parsed = Answer(mentions_bcl2=True, mentions_inhibition=True, mentions_apoptosis=False)\n",
    "print(f\"verify():          {parsed.verify()}\")\n",
    "print(f\"verify_granular(): {parsed.verify_granular():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5n6o7p8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Adding Templates to a Benchmark\n",
    "\n",
    "Templates are added to benchmarks as **code strings** — not class objects. This is the most reliable method and works in all environments (scripts, notebooks, CI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q9r0s1t2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.create(name=\"Custom Templates Demo\")\n",
    "\n",
    "# Option 1: Provide template when adding the question\n",
    "template_code = '''class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target mentioned\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCR-ABL\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        extracted = self.target.strip().upper().replace(\"-\", \"\")\n",
    "        expected = self.correct[\"target\"].upper().replace(\"-\", \"\")\n",
    "        return extracted == expected\n",
    "'''\n",
    "\n",
    "q1_id = benchmark.add_question(\n",
    "    question=\"What is the molecular target of imatinib?\",\n",
    "    raw_answer=\"BCR-ABL\",\n",
    "    answer_template=template_code,\n",
    ")\n",
    "print(f\"Q1 added with template: {q1_id[:50]}...\")\n",
    "\n",
    "# Option 2: Add template to an existing question\n",
    "q2_id = benchmark.add_question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\",\n",
    "    raw_answer=\"46\",\n",
    ")\n",
    "\n",
    "count_template = '''class Answer(BaseAnswer):\n",
    "    count: int = Field(description=\"The number of chromosomes mentioned\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"count\": 46}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.count == self.correct[\"count\"]\n",
    "'''\n",
    "\n",
    "benchmark.add_answer_template(q2_id, count_template)\n",
    "print(f\"Q2 template added: {q2_id[:50]}...\")\n",
    "\n",
    "print(f\"\\nTotal questions: {benchmark.question_count}\")\n",
    "print(f\"With templates:  {len(benchmark.get_finished_templates())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u3v4w5x6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Template Design Guidelines\n",
    "\n",
    "| Guideline | Why |\n",
    "|-----------|-----|\n",
    "| **Keep `verify()` deterministic** | Same input must always produce the same result — no randomness, no network calls |\n",
    "| **Normalize before comparing** | Strip whitespace, standardize case, handle hyphens/underscores |\n",
    "| **Use helper methods** | Extract each check into `_check_*()` methods for `verify_granular()` |\n",
    "| **Write clear field descriptions** | The Judge LLM only sees name, type, and description — be specific |\n",
    "| **Test locally first** | Instantiate your template and call `verify()` before running full verification |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Generating Templates](../05-creating-benchmarks/generating-templates.md) — Automatic template generation for common patterns\n",
    "- [Defining Rubrics](../05-creating-benchmarks/defining-rubrics.md) — Quality assessment alongside correctness\n",
    "- [Running Verification](../06-running-verification/index.md) — Execute verification with your templates\n",
    "- [Answer Templates](../04-core-concepts/answer-templates.md) — Conceptual foundation (field types, naming requirement)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
