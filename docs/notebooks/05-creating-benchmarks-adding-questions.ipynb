{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0bb684",
   "metadata": {},
   "source": [
    "# Adding Questions\n",
    "\n",
    "Questions are the core content of any benchmark. Each question pairs a prompt with an expected answer, and optionally includes a template, metadata, and few-shot examples. This page covers the three ways to add questions and how to work with question metadata.\n",
    "\n",
    "For background on how questions are stored in checkpoint files, see [Checkpoints](../04-core-concepts/checkpoints.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2cff2f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:38:10.968941Z",
     "iopub.status.busy": "2026-02-06T00:38:10.968691Z",
     "iopub.status.idle": "2026-02-06T00:38:10.974005Z",
     "shell.execute_reply": "2026-02-06T00:38:10.973456Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell (hidden in rendered docs).\n",
    "# No mocking needed — all examples create Benchmark objects locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf9da3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Adding a Question with Strings\n",
    "\n",
    "The simplest way to add a question is by passing the question text and expected answer as strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01aec25f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:38:10.976506Z",
     "iopub.status.busy": "2026-02-06T00:38:10.976342Z",
     "iopub.status.idle": "2026-02-06T00:38:11.298596Z",
     "shell.execute_reply": "2026-02-06T00:38:11.298387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: urn:uuid:question-what-is-the-chemical-symbol-for-gold-6a70bd93\n",
      "Question count: 1\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark(name=\"Science Quiz\")\n",
    "\n",
    "q_id = benchmark.add_question(\n",
    "    question=\"What is the chemical symbol for gold?\",\n",
    "    raw_answer=\"Au\",\n",
    ")\n",
    "\n",
    "print(f\"Question ID: {q_id}\")\n",
    "print(f\"Question count: {benchmark.question_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc789659",
   "metadata": {},
   "source": [
    "The `add_question()` method returns a **question ID** — a deterministic URN based on the question text (e.g., `urn:uuid:question-what-is-the-chemical-...-a1b2c3d4`). This ID can be used to reference the question later.\n",
    "\n",
    "## Adding a Question with a Template\n",
    "\n",
    "You can attach an answer template at the same time. Templates can be passed as **Python code strings** or as **Answer classes** (in scripts where `inspect.getsource()` works).\n",
    "\n",
    "### As a Code String\n",
    "\n",
    "The most portable approach — works in notebooks and scripts alike:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8189882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:38:11.299710Z",
     "iopub.status.busy": "2026-02-06T00:38:11.299632Z",
     "iopub.status.idle": "2026-02-06T00:38:11.301388Z",
     "shell.execute_reply": "2026-02-06T00:38:11.301184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: urn:uuid:question-what-element-has-the-symbol-au-51e79554\n",
      "Finished count: 1\n"
     ]
    }
   ],
   "source": [
    "template_code = \"\"\"class Answer(BaseAnswer):\n",
    "    symbol: str = Field(description=\"The chemical symbol for gold\")\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self.symbol.strip().upper() == \"AU\"\n",
    "\"\"\"\n",
    "\n",
    "q_id = benchmark.add_question(\n",
    "    question=\"What element has the symbol Au?\",\n",
    "    raw_answer=\"Gold\",\n",
    "    answer_template=template_code,\n",
    ")\n",
    "\n",
    "print(f\"Question ID: {q_id}\")\n",
    "print(f\"Finished count: {benchmark.finished_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e477b",
   "metadata": {},
   "source": [
    "When you pass an `answer_template`, the question is automatically marked as **finished** (`finished=True`).\n",
    "\n",
    "### As an Answer Class\n",
    "\n",
    "In Python scripts (not notebooks), you can pass a class directly. The class name is automatically normalized to `\"Answer\"` internally:\n",
    "\n",
    "    from karenina.schemas.entities import BaseAnswer\n",
    "    from pydantic import Field\n",
    "\n",
    "    class GoldAnswer(BaseAnswer):\n",
    "        symbol: str = Field(description=\"The chemical symbol for gold\")\n",
    "\n",
    "        def verify(self) -> bool:\n",
    "            return self.symbol.strip().upper() == \"AU\"\n",
    "\n",
    "    q_id = benchmark.add_question(\n",
    "        question=\"What element has the symbol Au?\",\n",
    "        raw_answer=\"Gold\",\n",
    "        answer_template=GoldAnswer,\n",
    "    )\n",
    "\n",
    "!!! note\n",
    "    Passing a class requires that `inspect.getsource()` can access the class source code. This works in `.py` files but not in interactive environments (notebooks, REPL). In those contexts, use a code string instead.\n",
    "\n",
    "For more on writing templates, see [Writing Templates](writing-templates.md) and [Answer Templates](../04-core-concepts/answer-templates.md).\n",
    "\n",
    "## Adding a Question with a Question Object\n",
    "\n",
    "For programmatic workflows, create a `Question` object first and pass it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2a7ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:38:11.302316Z",
     "iopub.status.busy": "2026-02-06T00:38:11.302262Z",
     "iopub.status.idle": "2026-02-06T00:38:11.303998Z",
     "shell.execute_reply": "2026-02-06T00:38:11.303778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: 3e6df3f90776cb0bb27fbbb91ea194d1\n",
      "Total questions: 3\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas.entities import Question\n",
    "\n",
    "q_obj = Question(\n",
    "    question=\"How many chromosomes are in a human somatic cell?\",\n",
    "    raw_answer=\"46\",\n",
    "    tags=[\"genetics\", \"cell-biology\"],\n",
    "    few_shot_examples=[\n",
    "        {\"question\": \"How many chromosomes does a fruit fly have?\", \"answer\": \"8\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "q_id = benchmark.add_question(q_obj)\n",
    "\n",
    "print(f\"Question ID: {q_id}\")\n",
    "print(f\"Total questions: {benchmark.question_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a2bb1",
   "metadata": {},
   "source": [
    "When you pass a `Question` object, its `tags` and `few_shot_examples` are automatically extracted and stored in the benchmark.\n",
    "\n",
    "### Question Fields\n",
    "\n",
    "| Field | Type | Required | Description |\n",
    "|-------|------|----------|-------------|\n",
    "| `question` | `str` | Yes | The question text |\n",
    "| `raw_answer` | `str` | Yes | The expected answer |\n",
    "| `tags` | `list[str]` | No | Tags for categorization and filtering |\n",
    "| `few_shot_examples` | `list[dict[str, str]]` | No | Few-shot examples as `{\"question\": ..., \"answer\": ...}` pairs |\n",
    "| `id` | `str` | Auto | MD5 hash of the question text (computed, read-only) |\n",
    "\n",
    "## Optional Metadata\n",
    "\n",
    "All three usage patterns support additional metadata parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc25ee3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:38:11.304932Z",
     "iopub.status.busy": "2026-02-06T00:38:11.304882Z",
     "iopub.status.idle": "2026-02-06T00:38:11.306491Z",
     "shell.execute_reply": "2026-02-06T00:38:11.306308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added question with metadata: urn:uuid:question-what-is-the-mechanism-of-action-of-imatinib-f2c541ee\n"
     ]
    }
   ],
   "source": [
    "q_id = benchmark.add_question(\n",
    "    question=\"What is the mechanism of action of imatinib?\",\n",
    "    raw_answer=\"Tyrosine kinase inhibitor targeting BCR-ABL\",\n",
    "    author={\"name\": \"Dr. Smith\", \"email\": \"smith@example.com\"},\n",
    "    sources=[\n",
    "        {\"name\": \"DrugBank\", \"url\": \"https://go.drugbank.com/drugs/DB00619\"},\n",
    "    ],\n",
    "    custom_metadata={\"difficulty\": \"hard\", \"domain\": \"oncology\"},\n",
    ")\n",
    "\n",
    "print(f\"Added question with metadata: {q_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e19e8",
   "metadata": {},
   "source": [
    "### Metadata Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `author` | `dict[str, Any]` | Author information (name, email, affiliation, etc.) |\n",
    "| `sources` | `list[dict[str, Any]]` | Source documents or references |\n",
    "| `custom_metadata` | `dict[str, Any]` | Arbitrary key-value pairs stored in the checkpoint |\n",
    "| `question_id` | `str` | Override the auto-generated ID (use with care) |\n",
    "| `finished` | `bool` | Override the auto-set finished flag |\n",
    "| `few_shot_examples` | `list[dict[str, str]]` | Few-shot examples (also available via Question object) |\n",
    "\n",
    "## Extracting Questions from Files\n",
    "\n",
    "For larger benchmarks, you can extract questions from Excel, CSV, or TSV files using the `extract_questions_from_file` utility:\n",
    "\n",
    "    from karenina.benchmark.authoring.questions import extract_questions_from_file\n",
    "\n",
    "    # Extract from a spreadsheet\n",
    "    questions = extract_questions_from_file(\n",
    "        file_path=\"questions.xlsx\",\n",
    "        question_column=\"Question\",\n",
    "        answer_column=\"Answer\",\n",
    "        author_name_column=\"Author\",           # optional\n",
    "        keywords_columns=[                     # optional\n",
    "            {\"column\": \"Keywords\", \"separator\": \",\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Add extracted questions to benchmark\n",
    "    for question, metadata in questions:\n",
    "        benchmark.add_question(question, **metadata)\n",
    "\n",
    "The function returns a list of `(Question, dict)` tuples. Each tuple contains the `Question` object and a metadata dictionary that can be unpacked directly into `add_question()`.\n",
    "\n",
    "**Supported formats**: `.xlsx`, `.xls`, `.csv`, `.tsv`\n",
    "\n",
    "## Working with Questions\n",
    "\n",
    "After adding questions, you can inspect them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39bbe403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:38:11.307438Z",
     "iopub.status.busy": "2026-02-06T00:38:11.307370Z",
     "iopub.status.idle": "2026-02-06T00:38:11.309048Z",
     "shell.execute_reply": "2026-02-06T00:38:11.308863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question IDs: 4 total\n",
      "  - What is the chemical symbol for gold?...\n",
      "  - What element has the symbol Au?...\n",
      "\n",
      "First question: What is the chemical symbol for gold?...\n",
      "Expected answer: Au...\n"
     ]
    }
   ],
   "source": [
    "# List all question IDs\n",
    "ids = benchmark.get_question_ids()\n",
    "print(f\"Question IDs: {len(ids)} total\")\n",
    "\n",
    "# Access all questions\n",
    "questions = benchmark.get_all_questions()\n",
    "for q in questions[:2]:\n",
    "    print(f\"  - {q['question'][:50]}...\")\n",
    "\n",
    "# Get a specific question by ID\n",
    "first_id = ids[0]\n",
    "question_data = benchmark.get_question(first_id)\n",
    "print(f\"\\nFirst question: {question_data['question'][:60]}...\")\n",
    "print(f\"Expected answer: {question_data['raw_answer'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04d4cf",
   "metadata": {},
   "source": [
    "## add_question() Reference\n",
    "\n",
    "Full method signature:\n",
    "\n",
    "    benchmark.add_question(\n",
    "        question,                  # str or Question object (required)\n",
    "        raw_answer=None,           # str — required if question is str\n",
    "        answer_template=None,      # str, type, or None\n",
    "        question_id=None,          # str — auto-generated if None\n",
    "        finished=<auto>,           # bool — True if template provided, False otherwise\n",
    "        author=None,               # dict — author information\n",
    "        sources=None,              # list[dict] — source references\n",
    "        custom_metadata=None,      # dict — arbitrary metadata\n",
    "        few_shot_examples=None,    # list[dict] — few-shot examples\n",
    "    ) -> str                       # returns question ID\n",
    "\n",
    "**Key behaviors**:\n",
    "\n",
    "- **Auto-finished**: Providing an `answer_template` automatically sets `finished=True` unless explicitly overridden\n",
    "- **Deterministic IDs**: Question IDs are generated from the question text using MD5 hashing. Duplicate texts get suffixed (`-1`, `-2`, etc.)\n",
    "- **Class renaming**: Answer template classes are automatically renamed to `\"Answer\"` (required by the verification system)\n",
    "- **Default templates**: If no template is provided, a minimal default template is generated that always returns `False` from `verify()`\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Writing Templates](writing-templates.md) — define evaluation criteria for your questions\n",
    "- [Defining Rubrics](defining-rubrics.md) — add quality assessment traits\n",
    "- [Saving Benchmarks](saving-benchmarks.md) — persist your benchmark to JSON-LD or database\n",
    "- [Answer Templates](../04-core-concepts/answer-templates.md) — concept guide for templates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
