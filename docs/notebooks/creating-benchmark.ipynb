{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db166a51",
   "metadata": {},
   "source": [
    "# Creating a Benchmark: End-to-End Workflow\n",
    "\n",
    "This notebook walks through the complete benchmark creation workflow — from creating an empty checkpoint to saving a fully-defined benchmark with questions, templates, and rubrics.\n",
    "\n",
    "For conceptual background, see the [Creating Benchmarks](../05-creating-benchmarks/index.md) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020f2160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:37.916589Z",
     "iopub.status.busy": "2026-02-06T05:23:37.916343Z",
     "iopub.status.idle": "2026-02-06T05:23:37.921085Z",
     "shell.execute_reply": "2026-02-06T05:23:37.920679Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell (hidden in rendered docs).\n",
    "# No mocking needed — all examples create objects locally without API calls.\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary directory for saving the benchmark\n",
    "_tmpdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b785478",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Create a Benchmark\n",
    "\n",
    "Every benchmark starts with the `Benchmark` constructor. Provide a name and optional metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b1ef99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:37.923104Z",
     "iopub.status.busy": "2026-02-06T05:23:37.922952Z",
     "iopub.status.idle": "2026-02-06T05:23:38.242568Z",
     "shell.execute_reply": "2026-02-06T05:23:38.242241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        Drug Discovery Knowledge Benchmark\n",
      "Version:     1.0.0\n",
      "Questions:   0\n",
      "Is empty:    True\n"
     ]
    }
   ],
   "source": [
    "from karenina import Benchmark\n",
    "\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Drug Discovery Knowledge Benchmark\",\n",
    "    description=\"Evaluates LLM knowledge of drug targets, mechanisms, and molecular biology\",\n",
    "    version=\"1.0.0\",\n",
    "    creator=\"Pharmacology Research Lab\",\n",
    ")\n",
    "\n",
    "print(f\"Name:        {benchmark.name}\")\n",
    "print(f\"Version:     {benchmark.version}\")\n",
    "print(f\"Questions:   {benchmark.question_count}\")\n",
    "print(f\"Is empty:    {benchmark.is_empty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc557f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Add Questions\n",
    "\n",
    "Questions pair a prompt with an expected answer. You can add them as simple strings, with templates attached, or as `Question` objects.\n",
    "\n",
    "### Simple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68ae072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.244132Z",
     "iopub.status.busy": "2026-02-06T05:23:38.244031Z",
     "iopub.status.idle": "2026-02-06T05:23:38.245802Z",
     "shell.execute_reply": "2026-02-06T05:23:38.245622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2 questions\n"
     ]
    }
   ],
   "source": [
    "q1_id = benchmark.add_question(\n",
    "    question=\"What is the putative target of imatinib?\",\n",
    "    raw_answer=\"BCR-ABL\",\n",
    ")\n",
    "\n",
    "q2_id = benchmark.add_question(\n",
    "    question=\"What is the boiling point of water in degrees Celsius?\",\n",
    "    raw_answer=\"100\",\n",
    ")\n",
    "\n",
    "print(f\"Added {benchmark.question_count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f865626",
   "metadata": {},
   "source": [
    "### Questions with Templates\n",
    "\n",
    "Attach an answer template as a code string to define how the response should be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b568d047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.246794Z",
     "iopub.status.busy": "2026-02-06T05:23:38.246736Z",
     "iopub.status.idle": "2026-02-06T05:23:38.248538Z",
     "shell.execute_reply": "2026-02-06T05:23:38.248359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question with template: urn:uuid:question-what-is-the-molecular-target-of-...\n",
      "Finished count: 1\n"
     ]
    }
   ],
   "source": [
    "template_code = '''class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target of the drug mentioned in the response\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCR-ABL\"}\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        extracted = self.target.strip().upper().replace(\"-\", \"\")\n",
    "        expected = self.correct[\"target\"].upper().replace(\"-\", \"\")\n",
    "        return extracted == expected\n",
    "'''\n",
    "\n",
    "q3_id = benchmark.add_question(\n",
    "    question=\"What is the molecular target of imatinib in chronic myeloid leukemia?\",\n",
    "    raw_answer=\"BCR-ABL tyrosine kinase\",\n",
    "    answer_template=template_code,\n",
    ")\n",
    "\n",
    "print(f\"Question with template: {q3_id[:50]}...\")\n",
    "print(f\"Finished count: {benchmark.finished_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925260a",
   "metadata": {},
   "source": [
    "### Questions with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7d02c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.249462Z",
     "iopub.status.busy": "2026-02-06T05:23:38.249407Z",
     "iopub.status.idle": "2026-02-06T05:23:38.251117Z",
     "shell.execute_reply": "2026-02-06T05:23:38.250915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 4\n",
      "With templates:  1\n"
     ]
    }
   ],
   "source": [
    "q4_id = benchmark.add_question(\n",
    "    question=\"What is the mechanism of action of venetoclax?\",\n",
    "    raw_answer=\"Selective BCL-2 inhibitor that triggers apoptosis\",\n",
    "    author={\"name\": \"Dr. Smith\", \"email\": \"smith@example.com\"},\n",
    "    sources=[{\"name\": \"DrugBank\", \"url\": \"https://go.drugbank.com/drugs/DB11581\"}],\n",
    "    custom_metadata={\"difficulty\": \"medium\", \"domain\": \"oncology\"},\n",
    ")\n",
    "\n",
    "print(f\"Total questions: {benchmark.question_count}\")\n",
    "print(f\"With templates:  {benchmark.finished_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa07efc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Write Custom Templates\n",
    "\n",
    "Templates define evaluation criteria. Let's add templates to the questions that don't have them yet.\n",
    "\n",
    "### Numeric Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84831da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.252048Z",
     "iopub.status.busy": "2026-02-06T05:23:38.251999Z",
     "iopub.status.idle": "2026-02-06T05:23:38.253852Z",
     "shell.execute_reply": "2026-02-06T05:23:38.253672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added numeric tolerance template to Q2\n"
     ]
    }
   ],
   "source": [
    "temperature_template = '''class Answer(BaseAnswer):\n",
    "    temperature: float = Field(\n",
    "        description=\"The boiling point temperature in degrees Celsius\"\n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"temperature\": 100.0}\n",
    "        self.tolerance = 0.5\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return abs(self.temperature - self.correct[\"temperature\"]) <= self.tolerance\n",
    "'''\n",
    "\n",
    "benchmark.add_answer_template(q2_id, temperature_template)\n",
    "print(f\"Added numeric tolerance template to Q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a395f",
   "metadata": {},
   "source": [
    "### Multi-Field with Partial Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04bca301",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.254794Z",
     "iopub.status.busy": "2026-02-06T05:23:38.254730Z",
     "iopub.status.idle": "2026-02-06T05:23:38.256784Z",
     "shell.execute_reply": "2026-02-06T05:23:38.256587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added multi-field template to Q4\n",
      "Finished count: 1 of 4\n"
     ]
    }
   ],
   "source": [
    "venetoclax_template = '''class Answer(BaseAnswer):\n",
    "    target: str = Field(description=\"The protein target of the drug\")\n",
    "    mechanism: str = Field(description=\"The mechanism of action (e.g., inhibitor, agonist)\")\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.correct = {\"target\": \"BCL2\", \"mechanism\": \"inhibitor\"}\n",
    "\n",
    "    def _check_target(self) -> bool:\n",
    "        extracted = self.target.strip().upper().replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "        expected = self.correct[\"target\"].upper().replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "        return extracted == expected\n",
    "\n",
    "    def _check_mechanism(self) -> bool:\n",
    "        return self.mechanism.strip().lower() == self.correct[\"mechanism\"].lower()\n",
    "\n",
    "    def verify(self) -> bool:\n",
    "        return self._check_target() and self._check_mechanism()\n",
    "\n",
    "    def verify_granular(self) -> float:\n",
    "        checks = [self._check_target(), self._check_mechanism()]\n",
    "        return sum(checks) / len(checks)\n",
    "'''\n",
    "\n",
    "benchmark.add_answer_template(q4_id, venetoclax_template)\n",
    "print(f\"Added multi-field template to Q4\")\n",
    "print(f\"Finished count: {benchmark.finished_count} of {benchmark.question_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa709391",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Define Rubric Traits\n",
    "\n",
    "Rubrics evaluate **response quality** independently of correctness. Add traits at global scope (every question) or per-question scope.\n",
    "\n",
    "### Global Traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1807e7e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.257713Z",
     "iopub.status.busy": "2026-02-06T05:23:38.257663Z",
     "iopub.status.idle": "2026-02-06T05:23:38.259982Z",
     "shell.execute_reply": "2026-02-06T05:23:38.259798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rubric traits added:\n",
      "  - Safe Response\n",
      "  - No Hedging\n",
      "  - Minimum Length\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas import LLMRubricTrait, RegexTrait, CallableTrait\n",
    "\n",
    "# Boolean LLM trait — evaluated on every question\n",
    "safety_trait = LLMRubricTrait(\n",
    "    name=\"Safe Response\",\n",
    "    description=(\n",
    "        \"Answer True if the response is safe and appropriate for a general audience, \"\n",
    "        \"with no harmful, misleading, or offensive content. Answer False otherwise.\"\n",
    "    ),\n",
    "    kind=\"boolean\",\n",
    "    higher_is_better=True,\n",
    ")\n",
    "benchmark.add_global_rubric_trait(safety_trait)\n",
    "\n",
    "# Regex trait — check for hedging language\n",
    "no_hedging = RegexTrait(\n",
    "    name=\"No Hedging\",\n",
    "    description=\"Response should not contain hedging phrases.\",\n",
    "    pattern=r\"\\b(I think|I believe|I guess|probably)\\b\",\n",
    "    case_sensitive=False,\n",
    "    invert_result=True,\n",
    "    higher_is_better=True,\n",
    ")\n",
    "benchmark.add_global_rubric_trait(no_hedging)\n",
    "\n",
    "# Callable trait — minimum word count\n",
    "min_words = CallableTrait.from_callable(\n",
    "    name=\"Minimum Length\",\n",
    "    func=lambda text: len(text.split()) >= 10,\n",
    "    kind=\"boolean\",\n",
    "    description=\"Response must contain at least 10 words.\",\n",
    "    higher_is_better=True,\n",
    ")\n",
    "benchmark.add_global_rubric_trait(min_words)\n",
    "\n",
    "print(\"Global rubric traits added:\")\n",
    "global_rubric = benchmark.get_global_rubric()\n",
    "for name in global_rubric.get_trait_names():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8985c6d",
   "metadata": {},
   "source": [
    "### Question-Specific Traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9843f0a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.260956Z",
     "iopub.status.busy": "2026-02-06T05:23:38.260906Z",
     "iopub.status.idle": "2026-02-06T05:23:38.262495Z",
     "shell.execute_reply": "2026-02-06T05:23:38.262295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added question-specific score trait to venetoclax question\n"
     ]
    }
   ],
   "source": [
    "# Score trait — only on the venetoclax question\n",
    "clarity_trait = LLMRubricTrait(\n",
    "    name=\"Mechanism Clarity\",\n",
    "    description=(\n",
    "        \"Rate how clearly the response explains the drug's mechanism of action. \"\n",
    "        \"1 = vague or missing, 5 = precise and well-explained.\"\n",
    "    ),\n",
    "    kind=\"score\",\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    "    higher_is_better=True,\n",
    ")\n",
    "benchmark.add_question_rubric_trait(q4_id, clarity_trait)\n",
    "print(f\"Added question-specific score trait to venetoclax question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb75e59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Save the Benchmark\n",
    "\n",
    "Save the completed benchmark as a JSON-LD checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c75d8db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.263385Z",
     "iopub.status.busy": "2026-02-06T05:23:38.263337Z",
     "iopub.status.idle": "2026-02-06T05:23:38.265576Z",
     "shell.execute_reply": "2026-02-06T05:23:38.265396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: drug_discovery_benchmark.jsonld\n",
      "File size: 11,579 bytes\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path(_tmpdir) / \"drug_discovery_benchmark.jsonld\"\n",
    "benchmark.save(checkpoint_path)\n",
    "\n",
    "print(f\"Saved to: {checkpoint_path.name}\")\n",
    "print(f\"File size: {checkpoint_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86d6c1",
   "metadata": {},
   "source": [
    "### Verify Round-Trip\n",
    "\n",
    "Load the saved checkpoint to confirm everything persisted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32edb66a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.266450Z",
     "iopub.status.busy": "2026-02-06T05:23:38.266402Z",
     "iopub.status.idle": "2026-02-06T05:23:38.268347Z",
     "shell.execute_reply": "2026-02-06T05:23:38.268183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:           Drug Discovery Knowledge Benchmark\n",
      "Questions:      4\n",
      "With templates: 1\n",
      "Progress:       25.0%\n",
      "Global traits:  ['Safe Response', 'No Hedging', 'Minimum Length']\n"
     ]
    }
   ],
   "source": [
    "loaded = Benchmark.load(str(checkpoint_path))\n",
    "\n",
    "print(f\"Name:           {loaded.name}\")\n",
    "print(f\"Questions:      {loaded.question_count}\")\n",
    "print(f\"With templates: {loaded.finished_count}\")\n",
    "print(f\"Progress:       {loaded.get_progress()}%\")\n",
    "\n",
    "# Check global rubric survived\n",
    "global_rubric = loaded.get_global_rubric()\n",
    "if global_rubric:\n",
    "    print(f\"Global traits:  {global_rubric.get_trait_names()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8192d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered the complete benchmark creation workflow:\n",
    "\n",
    "| Step | What | Key API |\n",
    "|------|------|---------|\n",
    "| 1 | Create checkpoint | `Benchmark.create()` |\n",
    "| 2 | Add questions | `benchmark.add_question()` |\n",
    "| 3 | Write templates | `benchmark.add_answer_template()` |\n",
    "| 4 | Define rubrics | `benchmark.add_global_rubric_trait()`, `add_question_rubric_trait()` |\n",
    "| 5 | Save | `benchmark.save()` |\n",
    "\n",
    "For running verification on this benchmark, see the [Running Verification](../06-running-verification/index.md) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "196422a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:23:38.269182Z",
     "iopub.status.busy": "2026-02-06T05:23:38.269122Z",
     "iopub.status.idle": "2026-02-06T05:23:38.270823Z",
     "shell.execute_reply": "2026-02-06T05:23:38.270601Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import shutil\n",
    "shutil.rmtree(_tmpdir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
