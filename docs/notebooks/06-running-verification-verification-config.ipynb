{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d072b45a",
   "metadata": {},
   "source": [
    "# Configuring Verification\n",
    "\n",
    "Every verification run is controlled by a `VerificationConfig` object. This page walks through the major configuration categories with practical examples. For an exhaustive field-by-field reference, see [VerificationConfig Reference](../10-configuration-reference/verification-config.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a077282b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.005369Z",
     "iopub.status.busy": "2026-02-06T00:56:18.005274Z",
     "iopub.status.idle": "2026-02-06T00:56:18.337917Z",
     "shell.execute_reply": "2026-02-06T00:56:18.337449Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(self)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mock cell: patches VerificationConfig validation so examples execute without live API keys.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "from unittest.mock import patch\n",
    "\n",
    "from karenina.schemas.verification import VerificationConfig\n",
    "\n",
    "_patcher_validate = patch.object(\n",
    "    VerificationConfig, \"_validate_config\", lambda self: None\n",
    ")\n",
    "_patcher_validate.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4799a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "`VerificationConfig` is the central configuration object for running verification. It controls:\n",
    "\n",
    "- **Which models** generate answers and parse responses\n",
    "- **What evaluation mode** to use (template-only, template+rubric, rubric-only)\n",
    "- **Which features** are enabled (abstention detection, embedding checks, deep judgment, etc.)\n",
    "- **Execution settings** (async parallelism, replicate count)\n",
    "\n",
    "Configuration flows through a precedence hierarchy:\n",
    "\n",
    "    CLI arguments > Preset values > Environment variables > Built-in defaults\n",
    "\n",
    "For details on this hierarchy, see [Configuration Hierarchy](../03-configuration/index.md).\n",
    "\n",
    "## Model Configuration\n",
    "\n",
    "Every verification run needs at least one **parsing model** (the Judge LLM that extracts structured data). Most runs also need an **answering model** (the model being evaluated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da6bfe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.339770Z",
     "iopub.status.busy": "2026-02-06T00:56:18.339562Z",
     "iopub.status.idle": "2026-02-06T00:56:18.342203Z",
     "shell.execute_reply": "2026-02-06T00:56:18.341984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering models: 1\n",
      "Parsing models: 1\n",
      "Interface: langchain\n"
     ]
    }
   ],
   "source": [
    "from karenina.schemas.config.models import ModelConfig\n",
    "from karenina.schemas.verification import VerificationConfig\n",
    "\n",
    "# Minimal configuration: one answering model + one parsing model\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"answering-gpt4mini\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parsing-gpt4mini\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Answering models: {len(config.answering_models)}\")\n",
    "print(f\"Parsing models: {len(config.parsing_models)}\")\n",
    "print(f\"Interface: {config.answering_models[0].interface}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b11289",
   "metadata": {},
   "source": [
    "### ModelConfig Fields\n",
    "\n",
    "Each `ModelConfig` specifies how to connect to an LLM:\n",
    "\n",
    "| Field | Type | Default | Description |\n",
    "|-------|------|---------|-------------|\n",
    "| `id` | `str` | — | Unique identifier for this model configuration (required) |\n",
    "| `model_name` | `str` | — | Model identifier (e.g., `\"gpt-4.1-mini\"`, `\"claude-sonnet-4-20250514\"`) |\n",
    "| `model_provider` | `str` | — | Provider name (e.g., `\"openai\"`, `\"anthropic\"`) |\n",
    "| `interface` | `str` | `\"langchain\"` | Adapter backend (see [Adapters](../04-core-concepts/adapters.md)) |\n",
    "| `temperature` | `float` | `0.1` | Sampling temperature |\n",
    "| `max_tokens` | `int` | `8192` | Maximum response tokens |\n",
    "| `system_prompt` | `str` | auto | Auto-set based on role (answering vs parsing) |\n",
    "\n",
    "System prompts are automatically applied if not explicitly set:\n",
    "\n",
    "- **Answering models**: *\"You are an expert assistant. Answer the question accurately and concisely.\"*\n",
    "- **Parsing models**: *\"You are a validation assistant. Parse and validate responses against the given Pydantic template.\"*\n",
    "\n",
    "### Using Different Interfaces\n",
    "\n",
    "You can mix interfaces — for example, use Claude Agent SDK for answering (to get native tool use) and LangChain for parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612512bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.343442Z",
     "iopub.status.busy": "2026-02-06T00:56:18.343359Z",
     "iopub.status.idle": "2026-02-06T00:56:18.345079Z",
     "shell.execute_reply": "2026-02-06T00:56:18.344854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering interface: claude_agent_sdk\n",
      "Parsing interface: langchain\n"
     ]
    }
   ],
   "source": [
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(\n",
    "            id=\"answering-claude\",\n",
    "            model_name=\"claude-sonnet-4-20250514\",\n",
    "            interface=\"claude_agent_sdk\",\n",
    "        )\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(\n",
    "            id=\"parsing-gpt4mini\",\n",
    "            model_name=\"gpt-4.1-mini\",\n",
    "            model_provider=\"openai\",\n",
    "            interface=\"langchain\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Answering interface: {config.answering_models[0].interface}\")\n",
    "print(f\"Parsing interface: {config.parsing_models[0].interface}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6897848",
   "metadata": {},
   "source": [
    "The six available interfaces are: `langchain` (default, multi-provider), `openrouter`, `openai_endpoint`, `claude_agent_sdk`, `claude_tool`, and `manual`. See [Adapters Overview](../04-core-concepts/adapters.md) for when to use each.\n",
    "\n",
    "## Evaluation Modes\n",
    "\n",
    "The `evaluation_mode` field determines which pipeline stages run:\n",
    "\n",
    "| Mode | Runs Templates | Runs Rubrics | Use Case |\n",
    "|------|:-:|:-:|----------|\n",
    "| `\"template_only\"` | Yes | No | Verify factual correctness (default) |\n",
    "| `\"template_and_rubric\"` | Yes | Yes | Correctness + quality assessment |\n",
    "| `\"rubric_only\"` | No | Yes | Quality-only evaluation (no templates needed) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4649fe53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.346127Z",
     "iopub.status.busy": "2026-02-06T00:56:18.346048Z",
     "iopub.status.idle": "2026-02-06T00:56:18.347984Z",
     "shell.execute_reply": "2026-02-06T00:56:18.347770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation mode: template_and_rubric\n",
      "Rubric enabled: True\n"
     ]
    }
   ],
   "source": [
    "# Template + rubric mode\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"answering\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"parsing\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    evaluation_mode=\"template_and_rubric\",\n",
    "    rubric_enabled=True,\n",
    ")\n",
    "\n",
    "print(f\"Evaluation mode: {config.evaluation_mode}\")\n",
    "print(f\"Rubric enabled: {config.rubric_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801044b",
   "metadata": {},
   "source": [
    "!!! note \"Consistency requirement\"\n",
    "    `evaluation_mode` and `rubric_enabled` must be consistent:\n",
    "\n",
    "    - `\"template_and_rubric\"` and `\"rubric_only\"` both require `rubric_enabled=True`\n",
    "    - `\"template_only\"` requires `rubric_enabled=False` (the default)\n",
    "\n",
    "For a detailed comparison of what each mode includes, see [Evaluation Modes](../04-core-concepts/evaluation-modes.md).\n",
    "\n",
    "## Feature Flags\n",
    "\n",
    "Feature flags enable optional pipeline stages. All are disabled by default.\n",
    "\n",
    "### Abstention and Sufficiency Detection\n",
    "\n",
    "These run *before* parsing, saving cost by short-circuiting evaluation for problematic responses:\n",
    "\n",
    "| Flag | Default | Effect |\n",
    "|------|---------|--------|\n",
    "| `abstention_enabled` | `False` | Detect model refusals/evasions — auto-fail if detected |\n",
    "| `sufficiency_enabled` | `False` | Detect incomplete responses — auto-fail if insufficient |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c753d253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.349005Z",
     "iopub.status.busy": "2026-02-06T00:56:18.348937Z",
     "iopub.status.idle": "2026-02-06T00:56:18.350815Z",
     "shell.execute_reply": "2026-02-06T00:56:18.350622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstention: True\n",
      "Sufficiency: True\n"
     ]
    }
   ],
   "source": [
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"answering\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"parsing\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    abstention_enabled=True,\n",
    "    sufficiency_enabled=True,\n",
    ")\n",
    "\n",
    "print(f\"Abstention: {config.abstention_enabled}\")\n",
    "print(f\"Sufficiency: {config.sufficiency_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496cb21",
   "metadata": {},
   "source": [
    "### Embedding Check\n",
    "\n",
    "Semantic similarity fallback that runs *after* template verification:\n",
    "\n",
    "| Flag | Default | Description |\n",
    "|------|---------|-------------|\n",
    "| `embedding_check_enabled` | `False` | Enable semantic similarity verification |\n",
    "| `embedding_check_model` | `\"all-MiniLM-L6-v2\"` | SentenceTransformer model name |\n",
    "| `embedding_check_threshold` | `0.85` | Similarity threshold (0.0–1.0) |\n",
    "\n",
    "These can also be set via environment variables (`EMBEDDING_CHECK`, `EMBEDDING_CHECK_MODEL`, `EMBEDDING_CHECK_THRESHOLD`).\n",
    "\n",
    "### Deep Judgment\n",
    "\n",
    "Multi-stage parsing with excerpt extraction and reasoning. Runs after standard template verification:\n",
    "\n",
    "| Flag | Default | Description |\n",
    "|------|---------|-------------|\n",
    "| `deep_judgment_enabled` | `False` | Enable deep judgment for templates |\n",
    "| `deep_judgment_max_excerpts_per_attribute` | `3` | Max excerpts extracted per attribute |\n",
    "| `deep_judgment_fuzzy_match_threshold` | `0.80` | Fuzzy match similarity threshold |\n",
    "| `deep_judgment_excerpt_retry_attempts` | `2` | Retry attempts for excerpt extraction |\n",
    "| `deep_judgment_search_enabled` | `False` | Enable web search validation for excerpts |\n",
    "\n",
    "For rubric deep judgment, see the `deep_judgment_rubric_mode` field and [Deep Judgment for Rubrics](../11-advanced-pipeline/deep-judgment-rubrics.md).\n",
    "\n",
    "## Rubric Settings\n",
    "\n",
    "When rubrics are enabled (`evaluation_mode` set to `\"template_and_rubric\"` or `\"rubric_only\"`), additional settings control rubric behavior:\n",
    "\n",
    "| Field | Default | Description |\n",
    "|-------|---------|-------------|\n",
    "| `rubric_evaluation_strategy` | `\"batch\"` | `\"batch\"` (all traits in one LLM call) or `\"sequential\"` (one-by-one) |\n",
    "| `rubric_trait_names` | `None` | Optional filter — evaluate only these trait names |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46532bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.351765Z",
     "iopub.status.busy": "2026-02-06T00:56:18.351706Z",
     "iopub.status.idle": "2026-02-06T00:56:18.353326Z",
     "shell.execute_reply": "2026-02-06T00:56:18.353078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: rubric_only\n",
      "Strategy: sequential\n",
      "Trait filter: ['safety', 'conciseness']\n"
     ]
    }
   ],
   "source": [
    "# Rubric-only mode evaluating specific traits\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"answering\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"parsing\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    evaluation_mode=\"rubric_only\",\n",
    "    rubric_enabled=True,\n",
    "    rubric_trait_names=[\"safety\", \"conciseness\"],\n",
    "    rubric_evaluation_strategy=\"sequential\",\n",
    ")\n",
    "\n",
    "print(f\"Mode: {config.evaluation_mode}\")\n",
    "print(f\"Strategy: {config.rubric_evaluation_strategy}\")\n",
    "print(f\"Trait filter: {config.rubric_trait_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77284287",
   "metadata": {},
   "source": [
    "## Async Execution\n",
    "\n",
    "Verification runs in parallel by default:\n",
    "\n",
    "| Field | Default | Description |\n",
    "|-------|---------|-------------|\n",
    "| `async_enabled` | `True` | Enable parallel execution of verification tasks |\n",
    "| `async_max_workers` | `2` | Number of concurrent workers |\n",
    "\n",
    "These can also be set via environment variables (`KARENINA_ASYNC_ENABLED`, `KARENINA_ASYNC_MAX_WORKERS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92566c18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.354299Z",
     "iopub.status.busy": "2026-02-06T00:56:18.354238Z",
     "iopub.status.idle": "2026-02-06T00:56:18.355996Z",
     "shell.execute_reply": "2026-02-06T00:56:18.355792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async: True\n",
      "Workers: 5\n"
     ]
    }
   ],
   "source": [
    "# Increase parallelism for large benchmarks\n",
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"answering\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"parsing\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    async_enabled=True,\n",
    "    async_max_workers=5,\n",
    ")\n",
    "\n",
    "print(f\"Async: {config.async_enabled}\")\n",
    "print(f\"Workers: {config.async_max_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d41d5e",
   "metadata": {},
   "source": [
    "## Trace Filtering (MCP)\n",
    "\n",
    "When using MCP-enabled agents, these flags control what portion of the agent trace is passed to evaluation:\n",
    "\n",
    "| Field | Default | Description |\n",
    "|-------|---------|-------------|\n",
    "| `use_full_trace_for_template` | `False` | Pass full agent trace to template parsing (vs final AI message only) |\n",
    "| `use_full_trace_for_rubric` | `True` | Pass full agent trace to rubric evaluation (vs final AI message only) |\n",
    "\n",
    "The full trace is always captured and stored in `raw_llm_response` regardless of these settings. The flags only control what input the parsing/evaluation models see.\n",
    "\n",
    "## Replicate Count\n",
    "\n",
    "Run each question–model combination multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ad4a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.356851Z",
     "iopub.status.busy": "2026-02-06T00:56:18.356792Z",
     "iopub.status.idle": "2026-02-06T00:56:18.358415Z",
     "shell.execute_reply": "2026-02-06T00:56:18.358221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicates: 3\n"
     ]
    }
   ],
   "source": [
    "config = VerificationConfig(\n",
    "    answering_models=[\n",
    "        ModelConfig(id=\"answering\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    parsing_models=[\n",
    "        ModelConfig(id=\"parsing\", model_name=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
    "    ],\n",
    "    replicate_count=3,\n",
    ")\n",
    "\n",
    "print(f\"Replicates: {config.replicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e5832",
   "metadata": {},
   "source": [
    "This produces 3 results per question per model combination, useful for measuring variance in LLM outputs.\n",
    "\n",
    "## Using `from_overrides`\n",
    "\n",
    "The `from_overrides` class method is the most convenient way to create a config with selective overrides. It implements the full precedence hierarchy: `overrides > base config > defaults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f49a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.359367Z",
     "iopub.status.busy": "2026-02-06T00:56:18.359306Z",
     "iopub.status.idle": "2026-02-06T00:56:18.361226Z",
     "shell.execute_reply": "2026-02-06T00:56:18.361053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering: gpt-4.1-mini\n",
      "Abstention: True\n",
      "Embedding check: True\n",
      "Replicates: 2\n"
     ]
    }
   ],
   "source": [
    "# Start from defaults, override just the models\n",
    "config = VerificationConfig.from_overrides(\n",
    "    answering_model=\"gpt-4.1-mini\",\n",
    "    answering_provider=\"openai\",\n",
    "    answering_id=\"answering\",\n",
    "    parsing_model=\"gpt-4.1-mini\",\n",
    "    parsing_provider=\"openai\",\n",
    "    parsing_id=\"parsing\",\n",
    "    abstention=True,\n",
    "    embedding_check=True,\n",
    "    replicate_count=2,\n",
    ")\n",
    "\n",
    "print(f\"Answering: {config.answering_models[0].model_name}\")\n",
    "print(f\"Abstention: {config.abstention_enabled}\")\n",
    "print(f\"Embedding check: {config.embedding_check_enabled}\")\n",
    "print(f\"Replicates: {config.replicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74dc1f",
   "metadata": {},
   "source": [
    "You can also apply overrides to a base config loaded from a preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01efb16e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.362169Z",
     "iopub.status.busy": "2026-02-06T00:56:18.362113Z",
     "iopub.status.idle": "2026-02-06T00:56:18.363566Z",
     "shell.execute_reply": "2026-02-06T00:56:18.363311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hypothetical: load preset then override specific settings\n",
    "# base = VerificationConfig.from_preset(Path(\"presets/default.json\"))\n",
    "# config = VerificationConfig.from_overrides(\n",
    "#     base,\n",
    "#     answering_model=\"claude-sonnet-4-20250514\",\n",
    "#     answering_id=\"answering-claude\",\n",
    "#     deep_judgment=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d0f8c",
   "metadata": {},
   "source": [
    "## Inspecting Configuration\n",
    "\n",
    "`VerificationConfig` has a detailed `repr` that shows all active settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2341f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.364529Z",
     "iopub.status.busy": "2026-02-06T00:56:18.364476Z",
     "iopub.status.idle": "2026-02-06T00:56:18.366061Z",
     "shell.execute_reply": "2026-02-06T00:56:18.365889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VerificationConfig(\n",
      "  === MODELS ===\n",
      "  Answering (1):\n",
      "    - gpt-4.1-mini (openai) [temp=0.1, interface=langchain]\n",
      "  Parsing (1):\n",
      "    - gpt-4.1-mini (openai) [temp=0.1, interface=langchain]\n",
      "\n",
      "  === EXECUTION ===\n",
      "  Replicates: 1\n",
      "  Async: True\n",
      "    └─ workers: 2\n",
      "  Evaluation Mode: template_only\n",
      "  Rubric Evaluation Strategy: batch\n",
      "\n",
      "  === FEATURES ===\n",
      "  Rubric: disabled\n",
      "  Abstention: enabled\n",
      "  Embedding Check: model=all-MiniLM-L6-v2, threshold=0.85\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = VerificationConfig.from_overrides(\n",
    "    answering_model=\"gpt-4.1-mini\",\n",
    "    answering_provider=\"openai\",\n",
    "    answering_id=\"answering\",\n",
    "    parsing_model=\"gpt-4.1-mini\",\n",
    "    parsing_provider=\"openai\",\n",
    "    parsing_id=\"parsing\",\n",
    "    abstention=True,\n",
    "    embedding_check=True,\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a5005",
   "metadata": {},
   "source": [
    "This prints a structured overview of models, execution settings, and enabled features — useful for verifying your configuration before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bca6979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T00:56:18.367007Z",
     "iopub.status.busy": "2026-02-06T00:56:18.366938Z",
     "iopub.status.idle": "2026-02-06T00:56:18.368357Z",
     "shell.execute_reply": "2026-02-06T00:56:18.368161Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up the mock\n",
    "_ = _patcher_validate.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730f690",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Python API Verification](python-api.md) — run verification with your config\n",
    "- [Using Presets](using-presets.md) — save and reuse configurations\n",
    "- [PromptConfig](prompt-config.md) — inject custom instructions into pipeline stages\n",
    "- [Response Quality Checks](response-quality-checks.md) — abstention and sufficiency detection\n",
    "- [VerificationConfig Reference](../10-configuration-reference/verification-config.md) — exhaustive field table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
