{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Analyzing Verification Results\n",
    "\n",
    "This notebook demonstrates how to explore and analyze verification results in Karenina.\n",
    "It covers the full workflow: inspecting individual `VerificationResult` objects,\n",
    "converting results to pandas DataFrames with all three builders, filtering and\n",
    "aggregating, and exporting.\n",
    "\n",
    "For the full reference on result structure, see\n",
    "[VerificationResult Structure](../07-analyzing-results/verification-result.md).\n",
    "For DataFrame details, see [DataFrame Analysis](../07-analyzing-results/dataframe-analysis.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell: creates mock VerificationResult objects for documentation examples.\n",
    "# This cell is hidden in the rendered documentation.\n",
    "import datetime\n",
    "\n",
    "from karenina.schemas.results import VerificationResultSet\n",
    "from karenina.schemas.verification import VerificationResult\n",
    "from karenina.schemas.verification.model_identity import ModelIdentity\n",
    "from karenina.schemas.verification.result_components import (\n",
    "    VerificationResultMetadata,\n",
    "    VerificationResultRubric,\n",
    "    VerificationResultTemplate,\n",
    ")\n",
    "\n",
    "# Two answering models and one parsing model\n",
    "_answering_gpt4o = ModelIdentity(model_name=\"gpt-4o\", interface=\"langchain\")\n",
    "_answering_claude = ModelIdentity(\n",
    "    model_name=\"claude-sonnet-4-20250514\", interface=\"claude_agent_sdk\"\n",
    ")\n",
    "_parsing = ModelIdentity(model_name=\"gpt-4o-mini\", interface=\"langchain\")\n",
    "_ts = datetime.datetime.now(tz=datetime.UTC).isoformat()\n",
    "\n",
    "\n",
    "def _make_result(\n",
    "    qid, question_text, answering, verified, response,\n",
    "    rubric_scores=None, regex_scores=None, callable_scores=None,\n",
    "    parsed_gt=None, parsed_llm=None, replicate=None,\n",
    "    exec_time=1.5,\n",
    "):\n",
    "    rid = VerificationResultMetadata.compute_result_id(\n",
    "        qid, answering, _parsing, _ts, replicate\n",
    "    )\n",
    "    template = VerificationResultTemplate(\n",
    "        raw_llm_response=response,\n",
    "        verify_result=verified,\n",
    "        template_verification_performed=True,\n",
    "        parsed_gt_response=parsed_gt or {\"answer\": response},\n",
    "        parsed_llm_response=parsed_llm or {\"answer\": response},\n",
    "    )\n",
    "    rubric = None\n",
    "    if rubric_scores or regex_scores or callable_scores:\n",
    "        rubric = VerificationResultRubric(\n",
    "            rubric_evaluation_performed=True,\n",
    "            llm_trait_scores=rubric_scores,\n",
    "            regex_trait_scores=regex_scores,\n",
    "            callable_trait_scores=callable_scores,\n",
    "        )\n",
    "    return VerificationResult(\n",
    "        metadata=VerificationResultMetadata(\n",
    "            question_id=qid,\n",
    "            template_id=\"tmpl_\" + qid[:8],\n",
    "            completed_without_errors=True,\n",
    "            question_text=question_text,\n",
    "            answering=answering,\n",
    "            parsing=_parsing,\n",
    "            execution_time=exec_time,\n",
    "            timestamp=_ts,\n",
    "            result_id=rid,\n",
    "            replicate=replicate,\n",
    "        ),\n",
    "        template=template,\n",
    "        rubric=rubric,\n",
    "    )\n",
    "\n",
    "\n",
    "# 6 results: 2 models x 3 questions\n",
    "_mock_results = [\n",
    "    _make_result(\n",
    "        \"q1\", \"What is the capital of France?\", _answering_gpt4o, True, \"Paris\",\n",
    "        rubric_scores={\"clarity\": 4, \"conciseness\": True},\n",
    "        regex_scores={\"no_hedging\": True},\n",
    "        parsed_gt={\"capital\": \"Paris\"}, parsed_llm={\"capital\": \"Paris\"},\n",
    "        exec_time=1.2,\n",
    "    ),\n",
    "    _make_result(\n",
    "        \"q2\", \"What is 6 multiplied by 7?\", _answering_gpt4o, True, \"42\",\n",
    "        rubric_scores={\"clarity\": 5, \"conciseness\": True},\n",
    "        parsed_gt={\"result\": \"42\"}, parsed_llm={\"result\": \"42\"},\n",
    "        exec_time=0.9,\n",
    "    ),\n",
    "    _make_result(\n",
    "        \"q3\", \"What element has atomic number 8?\", _answering_gpt4o, False, \"Nitrogen\",\n",
    "        rubric_scores={\"clarity\": 3, \"conciseness\": False},\n",
    "        parsed_gt={\"element\": \"Oxygen\"}, parsed_llm={\"element\": \"Nitrogen\"},\n",
    "        exec_time=1.8,\n",
    "    ),\n",
    "    _make_result(\n",
    "        \"q1\", \"What is the capital of France?\", _answering_claude, True, \"Paris\",\n",
    "        rubric_scores={\"clarity\": 5, \"conciseness\": True},\n",
    "        regex_scores={\"no_hedging\": True},\n",
    "        parsed_gt={\"capital\": \"Paris\"}, parsed_llm={\"capital\": \"Paris\"},\n",
    "        exec_time=1.4,\n",
    "    ),\n",
    "    _make_result(\n",
    "        \"q2\", \"What is 6 multiplied by 7?\", _answering_claude, True, \"42\",\n",
    "        rubric_scores={\"clarity\": 5, \"conciseness\": True},\n",
    "        parsed_gt={\"result\": \"42\"}, parsed_llm={\"result\": \"42\"},\n",
    "        exec_time=1.1,\n",
    "    ),\n",
    "    _make_result(\n",
    "        \"q3\", \"What element has atomic number 8?\", _answering_claude, True, \"Oxygen\",\n",
    "        rubric_scores={\"clarity\": 4, \"conciseness\": True},\n",
    "        parsed_gt={\"element\": \"Oxygen\"}, parsed_llm={\"element\": \"Oxygen\"},\n",
    "        exec_time=1.6,\n",
    "    ),\n",
    "]\n",
    "\n",
    "results = VerificationResultSet(results=_mock_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-explore",
   "metadata": {},
   "source": [
    "## Exploring a VerificationResult\n",
    "\n",
    "Each call to `run_verification()` returns a `VerificationResultSet` — a collection\n",
    "of `VerificationResult` objects, one per question verified. Let's start by\n",
    "inspecting a single result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first result from the set\n",
    "result = results.results[0]\n",
    "\n",
    "print(\"Sections available:\")\n",
    "print(\"  metadata:       always present\")\n",
    "print(f\"  template:       {result.template is not None}\")\n",
    "print(f\"  rubric:         {result.rubric is not None}\")\n",
    "print(f\"  deep_judgment:  {result.deep_judgment is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-metadata",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "The `metadata` section is always present and identifies the question, models,\n",
    "and execution context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = result.metadata\n",
    "\n",
    "print(\"Identification:\")\n",
    "print(f\"  question_id:  {meta.question_id}\")\n",
    "print(f\"  template_id:  {meta.template_id}\")\n",
    "print(f\"  result_id:    {meta.result_id}\")\n",
    "print(f\"  question:     {meta.question_text}\")\n",
    "\n",
    "print(\"\\nModels:\")\n",
    "print(f\"  answering:  {meta.answering_model}\")\n",
    "print(f\"  parsing:    {meta.parsing_model}\")\n",
    "\n",
    "print(\"\\nExecution:\")\n",
    "print(f\"  completed:  {meta.completed_without_errors}\")\n",
    "print(f\"  time:       {meta.execution_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-template",
   "metadata": {},
   "source": [
    "### Template Results\n",
    "\n",
    "The `template` section contains parsed responses and verification outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.template:\n",
    "    tmpl = result.template\n",
    "    print(f\"Raw LLM response: {tmpl.raw_llm_response!r}\")\n",
    "    print(f\"Parsed LLM:       {tmpl.parsed_llm_response}\")\n",
    "    print(f\"Parsed GT:        {tmpl.parsed_gt_response}\")\n",
    "    print(f\"verify() result:  {tmpl.verify_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-rubric",
   "metadata": {},
   "source": [
    "### Rubric Results\n",
    "\n",
    "The `rubric` section contains trait scores, split by type for type-safe access.\n",
    "The `get_all_trait_scores()` convenience method merges them into a flat dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-rubric",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.rubric:\n",
    "    rub = result.rubric\n",
    "    print(\"Trait scores by type:\")\n",
    "    print(f\"  LLM traits:      {rub.llm_trait_scores}\")\n",
    "    print(f\"  Regex traits:    {rub.regex_trait_scores}\")\n",
    "    print(f\"  Callable traits: {rub.callable_trait_scores}\")\n",
    "    print(f\"\\nAll scores (flat): {rub.get_all_trait_scores()}\")\n",
    "\n",
    "    # Look up a specific trait\n",
    "    match = rub.get_trait_by_name(\"clarity\")\n",
    "    if match:\n",
    "        value, trait_type = match\n",
    "        print(f\"\\nLookup 'clarity': value={value}, type={trait_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-resultset",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Working with VerificationResultSet\n",
    "\n",
    "The `VerificationResultSet` provides filtering and grouping before you convert\n",
    "to DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resultset-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total results: {len(results.results)}\")\n",
    "\n",
    "# Filter to a single model\n",
    "filtered = results.filter(answering_models=[\"langchain:gpt-4o\"])\n",
    "print(f\"GPT-4o results: {len(filtered.results)}\")\n",
    "\n",
    "# Group by model\n",
    "by_model = results.group_by_model()\n",
    "for model, group in by_model.items():\n",
    "    print(f\"  {model}: {len(group.results)} results\")\n",
    "\n",
    "# Group by question\n",
    "by_question = results.group_by_question()\n",
    "for qid, group in by_question.items():\n",
    "    print(f\"  {qid}: {len(group.results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-summary",
   "metadata": {},
   "source": [
    "### Result Set Summary\n",
    "\n",
    "`get_summary()` provides comprehensive statistics in one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resultset-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = results.get_summary()\n",
    "print(f\"Total results:    {summary['num_results']}\")\n",
    "print(f\"Completed:        {summary['num_completed']}\")\n",
    "print(f\"With templates:   {summary['num_with_template']}\")\n",
    "print(f\"With rubrics:     {summary['num_with_rubric']}\")\n",
    "print(f\"Unique questions: {summary['num_questions']}\")\n",
    "print(f\"Models used:      {summary['num_models']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-template-df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Template DataFrames\n",
    "\n",
    "`TemplateResults` creates one row per parsed field, enabling field-level\n",
    "comparison between ground truth and LLM-extracted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "template-df-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_results = results.get_template_results()\n",
    "df = template_results.to_dataframe()\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nField comparison:\")\n",
    "print(\n",
    "    df[[\"question_id\", \"answering_model\", \"field_name\",\n",
    "        \"gt_value\", \"llm_value\", \"field_match\"]].to_string(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-template-agg",
   "metadata": {},
   "source": [
    "### Pass Rate by Model and Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "template-pass-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in aggregation helpers\n",
    "print(\"Pass rate by model:\")\n",
    "for model, rate in template_results.aggregate_pass_rate(by=\"answering_model\").items():\n",
    "    print(f\"  {model}: {rate:.0%}\")\n",
    "\n",
    "print(\"\\nPass rate by question:\")\n",
    "for qid, rate in template_results.aggregate_pass_rate(by=\"question_id\").items():\n",
    "    print(f\"  {qid}: {rate:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-template-filter",
   "metadata": {},
   "source": [
    "### Filtering Template Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "template-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only failed results\n",
    "failed = template_results.filter(failed_only=True)\n",
    "print(f\"Failed results: {len(failed)}\")\n",
    "\n",
    "# Filter by model\n",
    "gpt_results = template_results.filter(answering_models=[\"langchain:gpt-4o\"])\n",
    "print(f\"GPT-4o results: {len(gpt_results)}\")\n",
    "\n",
    "# Summary statistics\n",
    "tmpl_summary = template_results.get_template_summary()\n",
    "print(f\"\\nSummary: {tmpl_summary['num_passed']} passed, {tmpl_summary['num_failed']} failed\")\n",
    "print(f\"Pass rate: {tmpl_summary['pass_rate']:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-rubric-df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rubric DataFrames\n",
    "\n",
    "`RubricResults` creates one row per trait evaluated. Filter by trait type using\n",
    "the `trait_type` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubric-df-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_results = results.get_rubrics_results()\n",
    "df_rubric = rubric_results.to_dataframe()\n",
    "\n",
    "print(\"All rubric traits:\")\n",
    "print(\n",
    "    df_rubric[[\"question_id\", \"answering_model\", \"trait_name\",\n",
    "               \"trait_score\", \"trait_type\"]].to_string(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubric-filter-type",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to LLM score traits only (numeric 1-5 scale)\n",
    "df_scores = rubric_results.to_dataframe(trait_type=\"llm_score\")\n",
    "print(f\"LLM score traits: {len(df_scores)} rows\")\n",
    "print(\n",
    "    df_scores[[\"question_id\", \"answering_model\",\n",
    "               \"trait_name\", \"trait_score\"]].to_string(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-rubric-agg",
   "metadata": {},
   "source": [
    "### Aggregating Trait Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubric-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average LLM trait scores by model\n",
    "avg_by_model = rubric_results.aggregate_llm_traits(\n",
    "    strategy=\"mean\", by=\"answering_model\"\n",
    ")\n",
    "print(\"Average LLM trait scores by model:\")\n",
    "for model, traits in avg_by_model.items():\n",
    "    print(f\"  {model}:\")\n",
    "    for trait, score in traits.items():\n",
    "        print(f\"    {trait}: {score:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubric-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "trait_summary = rubric_results.get_trait_summary()\n",
    "print(f\"Results with rubric data: {trait_summary['num_results']}\")\n",
    "print(f\"LLM traits:     {trait_summary['llm_traits']}\")\n",
    "print(f\"Regex traits:   {trait_summary['regex_traits']}\")\n",
    "print(f\"Callable traits: {trait_summary['callable_traits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-judgment",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deep Judgment DataFrames\n",
    "\n",
    "`JudgmentResults` handles deep judgment data, creating one row per\n",
    "(attribute x excerpt) pair. This is the most granular DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judgment-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "judgment_results = results.get_judgment_results()\n",
    "print(f\"Results with deep judgment: {len(judgment_results.get_results_with_judgment())}\")\n",
    "# Deep judgment was not enabled in our mock data, so this is empty.\n",
    "# When enabled, the DataFrame provides columns for excerpt text,\n",
    "# confidence scores, similarity scores, and hallucination risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-pandas",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Analysis Patterns\n",
    "\n",
    "### Model Comparison with pandas\n",
    "\n",
    "Combine template pass rates and rubric scores for a side-by-side model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pandas-model-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Template pass rates by model\n",
    "template_df = results.get_template_results().to_dataframe()\n",
    "model_pass = (\n",
    "    template_df.drop_duplicates(subset=[\"result_index\"])\n",
    "    .groupby(\"answering_model\")[\"verify_result\"]\n",
    "    .mean()\n",
    ")\n",
    "print(\"Template pass rate by model:\")\n",
    "print(model_pass.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-difficulty",
   "metadata": {},
   "source": [
    "### Question Difficulty\n",
    "\n",
    "Identify which questions are hardest by looking at pass rates across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pandas-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pass = (\n",
    "    template_df.drop_duplicates(subset=[\"result_index\"])\n",
    "    .groupby(\"question_id\")[\"verify_result\"]\n",
    "    .agg([\"mean\", \"count\"])\n",
    "    .rename(columns={\"mean\": \"pass_rate\", \"count\": \"num_runs\"})\n",
    "    .sort_values(\"pass_rate\")\n",
    ")\n",
    "print(\"Question difficulty (sorted by pass rate):\")\n",
    "print(question_pass.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-export",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exporting Results\n",
    "\n",
    "### DataFrame Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False, mode=\"w\") as f:\n",
    "    csv_path = f.name\n",
    "    template_df.to_csv(csv_path, index=False)\n",
    "    size = os.path.getsize(csv_path)\n",
    "    print(f\"Exported {len(template_df)} rows to CSV ({size} bytes)\")\n",
    "    os.unlink(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-export-rubric",
   "metadata": {},
   "source": [
    "### Exporting Rubric DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-rubric",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_df = rubric_results.to_dataframe()\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False, mode=\"w\") as f:\n",
    "    csv_path = f.name\n",
    "    rubric_df.to_csv(csv_path, index=False)\n",
    "    size = os.path.getsize(csv_path)\n",
    "    print(f\"Exported {len(rubric_df)} rubric rows to CSV ({size} bytes)\")\n",
    "    os.unlink(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-next-steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [VerificationResult Structure](../07-analyzing-results/verification-result.md) — Complete field reference\n",
    "- [DataFrame Analysis](../07-analyzing-results/dataframe-analysis.md) — Full DataFrame API details\n",
    "- [Exporting Results](../07-analyzing-results/exporting.md) — Benchmark-level export (JSON and CSV)\n",
    "- [Iterating on Benchmarks](../07-analyzing-results/iterating.md) — Improve templates based on results"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
