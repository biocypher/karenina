"""Thread-safe answer caching for verification pipeline.

This module provides caching infrastructure to prevent duplicate answer generation
when multiple judges (parsing models) evaluate the same answering model output.
"""

import logging
import threading
import time
from typing import Any

logger = logging.getLogger(__name__)


class CacheEntry:
    """Represents a cache entry for an answer generation task.

    Supports three states:
    - IN_PROGRESS: Answer is being generated by another task
    - COMPLETED: Answer generation succeeded
    - FAILED: Answer generation failed
    """

    def __init__(self, is_complete: bool = False) -> None:
        """Initialize cache entry.

        Args:
            is_complete: Whether the answer is already complete
        """
        self.is_complete = is_complete
        self.event = threading.Event()  # Signal completion to waiting tasks
        self.answer_data: dict[str, Any] | None = None
        self.error: Exception | None = None
        self.timestamp = time.time()


class AnswerTraceCache:
    """Thread-safe cache for answer traces with non-blocking synchronization.

    This cache prevents duplicate answer generation when multiple judges
    (parsing models) need to evaluate the same answering model output.

    Features:
    - Thread-safe: All operations protected by lock
    - Non-blocking: Returns immediately with status, no waiting
    - Race condition prevention: Caller handles requeuing for in-progress answers
    - Fault tolerance: Failed generations allow retry

    Example:
        ```python
        cache = AnswerTraceCache()

        # Thread 1: Generates answer
        status, answer_data = cache.get_or_reserve(key)
        if status == "MISS":
            # Generate answer...
            cache.complete(key, answer_data, error=None)

        # Thread 2: Gets IN_PROGRESS status
        status, answer_data = cache.get_or_reserve(key)
        # status == "IN_PROGRESS", caller should requeue task
        # Later, after thread 1 completes:
        status, answer_data = cache.get_or_reserve(key)
        # status == "HIT", answer_data contains the cached answer
        ```
    """

    def __init__(self) -> None:
        """Initialize answer cache."""
        self._cache: dict[str, CacheEntry] = {}
        self._lock = threading.Lock()
        self._stats_hits = 0
        self._stats_misses = 0
        self._stats_waits = 0  # Now tracks IN_PROGRESS returns
        self._stats_timeouts = 0  # Deprecated, kept for backward compatibility

    def get_or_reserve(self, key: str) -> tuple[str, dict[str, Any] | None]:
        """Get cached answer or reserve slot for generation.

        This method implements a three-state cache without blocking:
        1. COMPLETED: Return cached answer
        2. IN_PROGRESS: Return status immediately, caller should requeue
        3. MISSING: Reserve slot and signal caller to generate

        Args:
            key: Cache key (question_id, answering_model_id, replicate)

        Returns:
            Tuple of (status, answer_data):
            - If answer cached: ("HIT", data)
            - If answer in-progress: ("IN_PROGRESS", None) - caller should requeue
            - If missing: ("MISS", None) - caller should generate
        """
        with self._lock:
            entry = self._cache.get(key)

            if entry is None:
                # MISSING: Reserve slot for generation
                self._cache[key] = CacheEntry(is_complete=False)
                self._stats_misses += 1
                logger.debug(f"Cache miss: {key} - reserving slot")
                return "MISS", None

            if entry.is_complete:
                # COMPLETED or FAILED
                if entry.error:
                    # Previous generation failed, allow retry
                    logger.warning(
                        f"Previous answer generation failed for {key} (error: {entry.error}), allowing retry"
                    )
                    del self._cache[key]
                    # Treat as miss to allow retry
                    self._cache[key] = CacheEntry(is_complete=False)
                    self._stats_misses += 1
                    return "MISS", None

                # Success! Return cached answer
                self._stats_hits += 1
                logger.debug(f"Cache hit: {key}")
                return "HIT", entry.answer_data

            # IN_PROGRESS: Return immediately, let caller handle requeuing
            logger.debug(f"Answer in-progress for {key}, returning IN_PROGRESS status")
            self._stats_waits += 1
            return "IN_PROGRESS", None

    def complete(self, key: str, answer_data: dict[str, Any] | None, error: Exception | None = None) -> None:
        """Mark answer generation as complete and notify waiting tasks.

        Args:
            key: Cache key
            answer_data: Generated answer data (None if failed)
            error: Exception if generation failed
        """
        with self._lock:
            entry = self._cache.get(key)
            if entry is None:
                logger.warning(f"Attempted to complete non-existent cache entry: {key}")
                return

            entry.is_complete = True
            entry.answer_data = answer_data
            entry.error = error

            # Notify all waiting tasks
            entry.event.set()

            if error:
                logger.debug(f"Marked cache entry as failed: {key} (error: {error})")
            else:
                logger.debug(f"Marked cache entry as complete: {key}")

    def get_stats(self) -> dict[str, int]:
        """Get cache statistics.

        Returns:
            Dictionary with hit/miss/wait/timeout counts
        """
        return {
            "hits": self._stats_hits,
            "misses": self._stats_misses,
            "waits": self._stats_waits,
            "timeouts": self._stats_timeouts,
        }
