"""Thread-safe answer caching for verification pipeline.

This module provides caching infrastructure to prevent duplicate answer generation
when multiple judges (parsing models) evaluate the same answering model output.
"""

import logging
import threading
import time
from typing import Any

logger = logging.getLogger(__name__)


class CacheEntry:
    """Represents a cache entry for an answer generation task.

    Supports three states:
    - IN_PROGRESS: Answer is being generated by another task
    - COMPLETED: Answer generation succeeded
    - FAILED: Answer generation failed
    """

    def __init__(self, is_complete: bool = False) -> None:
        """Initialize cache entry.

        Args:
            is_complete: Whether the answer is already complete
        """
        self.is_complete = is_complete
        self.event = threading.Event()  # Signal completion to waiting tasks
        self.answer_data: dict[str, Any] | None = None
        self.error: Exception | None = None
        self.timestamp = time.time()


class AnswerTraceCache:
    """Thread-safe cache for answer traces with synchronization.

    This cache prevents duplicate answer generation when multiple judges
    (parsing models) need to evaluate the same answering model output.

    Features:
    - Thread-safe: All operations protected by lock
    - Race condition prevention: Tasks wait for in-progress answers
    - Timeout protection: Tasks don't wait forever
    - Fault tolerance: Failed generations don't block other tasks

    Example:
        ```python
        cache = AnswerTraceCache(wait_timeout=300.0)

        # Thread 1: Generates answer
        answer_data, should_generate = cache.get_or_reserve(key)
        if should_generate:
            # Generate answer...
            cache.complete(key, answer_data, error=None)

        # Thread 2: Waits for answer
        answer_data, should_generate = cache.get_or_reserve(key)
        # Thread 2 waits, then receives cached answer
        # should_generate = False, answer_data contains the answer
        ```
    """

    def __init__(self, wait_timeout: float = 300.0) -> None:
        """Initialize answer cache.

        Args:
            wait_timeout: Maximum seconds to wait for in-progress answer (default: 5 minutes)
        """
        self._cache: dict[str, CacheEntry] = {}
        self._lock = threading.Lock()
        self._wait_timeout = wait_timeout
        self._stats_hits = 0
        self._stats_misses = 0
        self._stats_waits = 0
        self._stats_timeouts = 0

    def get_or_reserve(self, key: str) -> tuple[dict[str, Any] | None, bool]:
        """Get cached answer or reserve slot for generation.

        This method implements a three-state cache with synchronization:
        1. COMPLETED: Return cached answer
        2. IN_PROGRESS: Wait for completion, then return answer
        3. MISSING: Reserve slot and signal caller to generate

        Args:
            key: Cache key (question_id, answering_model_id, replicate)

        Returns:
            Tuple of (answer_data, should_generate):
            - If answer cached: (data, False)
            - If answer in-progress: waits, then (data, False)
            - If missing: (None, True) and reserves slot
        """
        max_retries = 3

        for _attempt in range(max_retries):
            with self._lock:
                entry = self._cache.get(key)

                if entry is None:
                    # MISSING: Reserve slot for generation
                    self._cache[key] = CacheEntry(is_complete=False)
                    self._stats_misses += 1
                    logger.debug(f"Cache miss: {key} - reserving slot")
                    return None, True

                if entry.is_complete:
                    # COMPLETED or FAILED
                    if entry.error:
                        # Previous generation failed, allow retry
                        logger.warning(
                            f"Previous answer generation failed for {key} (error: {entry.error}), allowing retry"
                        )
                        del self._cache[key]
                        continue

                    # Success! Return cached answer
                    self._stats_hits += 1
                    logger.debug(f"Cache hit: {key}")
                    return entry.answer_data, False

            # IN_PROGRESS: Wait for completion (outside lock to avoid blocking other tasks)
            logger.info(f"Answer in-progress for {key}, waiting (timeout: {self._wait_timeout}s)")
            self._stats_waits += 1

            signaled = entry.event.wait(timeout=self._wait_timeout)

            if not signaled:
                # Timeout: Proceed with own generation to avoid deadlock
                self._stats_timeouts += 1
                logger.warning(
                    f"Timeout waiting for answer generation: {key} "
                    f"(waited {self._wait_timeout}s). Proceeding with own generation."
                )
                # Don't return cached data - let task generate its own answer
                return None, True

            # Event signaled, check cache again
            # (entry might now be complete or failed)

        # Should rarely reach here (max retries exhausted)
        logger.error(f"Failed to acquire answer after {max_retries} attempts: {key}")
        return None, True

    def complete(self, key: str, answer_data: dict[str, Any] | None, error: Exception | None = None) -> None:
        """Mark answer generation as complete and notify waiting tasks.

        Args:
            key: Cache key
            answer_data: Generated answer data (None if failed)
            error: Exception if generation failed
        """
        with self._lock:
            entry = self._cache.get(key)
            if entry is None:
                logger.warning(f"Attempted to complete non-existent cache entry: {key}")
                return

            entry.is_complete = True
            entry.answer_data = answer_data
            entry.error = error

            # Notify all waiting tasks
            entry.event.set()

            if error:
                logger.debug(f"Marked cache entry as failed: {key} (error: {error})")
            else:
                logger.debug(f"Marked cache entry as complete: {key}")

    def get_stats(self) -> dict[str, int]:
        """Get cache statistics.

        Returns:
            Dictionary with hit/miss/wait/timeout counts
        """
        return {
            "hits": self._stats_hits,
            "misses": self._stats_misses,
            "waits": self._stats_waits,
            "timeouts": self._stats_timeouts,
        }
