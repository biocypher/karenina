{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Access and Filtering Methods\n",
    "\n",
    "This notebook demonstrates the comprehensive question access, filtering, and search methods available in Karenina.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Karenina provides powerful methods to:\n",
    "- **Access** questions with a harmonized API that returns full question objects\n",
    "- **Filter** questions by system metadata (finished, has_template, author, etc.)\n",
    "- **Filter** questions by custom metadata with flexible APIs\n",
    "- **Search** questions with multi-term, regex, and multi-field support\n",
    "- **Count** and analyze question distributions\n",
    "- **Combine** methods for complex workflows\n",
    "\n",
    "All methods work with **any custom metadata structure** - no assumptions about field names!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karenina.benchmark.benchmark import Benchmark\n",
    "\n",
    "# Create a benchmark\n",
    "benchmark = Benchmark.create(\n",
    "    name=\"Question Access Demo\",\n",
    "    description=\"Demonstrating question access methods\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Created benchmark: {benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Questions\n",
    "\n",
    "Add questions with various **custom metadata** to demonstrate filtering flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define questions with diverse metadata\n",
    "questions_data = [\n",
    "    {\n",
    "        \"question\": \"What is Python?\",\n",
    "        \"raw_answer\": \"A high-level programming language\",\n",
    "        \"finished\": True,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"programming\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"tags\": [\"python\", \"basics\"],\n",
    "            \"year\": 2023\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain quantum entanglement\",\n",
    "        \"raw_answer\": \"A phenomenon where particles are correlated\",\n",
    "        \"finished\": True,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"physics\",\n",
    "            \"difficulty\": \"hard\",\n",
    "            \"tags\": [\"quantum\", \"physics\"],\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"raw_answer\": \"AI algorithms that learn from data\",\n",
    "        \"finished\": False,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"programming\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"tags\": [\"ai\", \"ml\"],\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Describe DNA replication\",\n",
    "        \"raw_answer\": \"Process of copying DNA molecules\",\n",
    "        \"finished\": True,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"biology\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"tags\": [\"biology\", \"dna\"],\n",
    "            \"year\": 2023\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does Python handle memory?\",\n",
    "        \"raw_answer\": \"Through garbage collection and reference counting\",\n",
    "        \"finished\": False,\n",
    "        \"custom_metadata\": {\n",
    "            \"category\": \"programming\",\n",
    "            \"difficulty\": \"hard\",\n",
    "            \"tags\": [\"python\", \"memory\"],\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Add questions to benchmark\n",
    "for q_data in questions_data:\n",
    "    benchmark.add_question(\n",
    "        question=q_data[\"question\"],\n",
    "        raw_answer=q_data[\"raw_answer\"],\n",
    "        finished=q_data[\"finished\"],\n",
    "        custom_metadata=q_data[\"custom_metadata\"]\n",
    "    )\n",
    "\n",
    "print(f\"✓ Added {len(benchmark)} questions to benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Get all questions\nall_questions = benchmark.get_all_questions()\nprint(f\"Total questions: {len(all_questions)}\")\n\n# Get a specific question by ID\nquestion_ids = benchmark.get_question_ids()\nfirst_q = benchmark.get_question(question_ids[0])\n\nprint(f\"\\nFirst question: {first_q['question']}\")\nprint(f\"Category: {first_q['custom_metadata']['category']}\")\nprint(f\"Difficulty: {first_q['custom_metadata']['difficulty']}\")\n\n# get_all_questions also supports ids_only parameter\nall_ids = benchmark.get_all_questions(ids_only=True)\nprint(f\"\\nAll question IDs: {len(all_ids)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all questions\n",
    "all_questions = benchmark.get_all_questions()\n",
    "print(f\"Total questions: {len(all_questions)}\")\n",
    "\n",
    "# Get a specific question by ID\n",
    "question_ids = benchmark.get_question_ids()\n",
    "first_q = benchmark.get_question(question_ids[0])\n",
    "\n",
    "print(f\"\\nFirst question: {first_q['question']}\")\n",
    "print(f\"Category: {first_q['custom_metadata']['category']}\")\n",
    "print(f\"Difficulty: {first_q['custom_metadata']['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Harmonized Access Methods\n",
    "\n",
    "These methods return question objects by default, making them consistent with other filtering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get finished questions (returns question objects)\n",
    "finished = benchmark.get_finished_questions()\n",
    "print(f\"Finished questions: {len(finished)}\")\n",
    "\n",
    "# Directly access question properties\n",
    "for q in finished:\n",
    "    print(f\"  ✓ {q['question'][:50]}...\")\n",
    "\n",
    "# Get unfinished questions\n",
    "unfinished = benchmark.get_unfinished_questions()\n",
    "print(f\"\\nUnfinished questions: {len(unfinished)}\")\n",
    "\n",
    "for q in unfinished:\n",
    "    print(f\"  ○ {q['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need just IDs, use ids_only=True\n",
    "finished_ids = benchmark.get_finished_questions(ids_only=True)\n",
    "unfinished_ids = benchmark.get_unfinished_questions(ids_only=True)\n",
    "\n",
    "print(f\"Finished IDs: {len(finished_ids)}\")\n",
    "print(f\"Unfinished IDs: {len(unfinished_ids)}\")\n",
    "print(f\"Total: {len(finished_ids) + len(unfinished_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtering by System Metadata\n",
    "\n",
    "Use `filter_questions()` for built-in Karenina fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by finished status\n",
    "finished_qs = benchmark.filter_questions(finished=True)\n",
    "print(f\"Finished: {len(finished_qs)} questions\")\n",
    "\n",
    "# Use custom lambda for complex logic\n",
    "complex = benchmark.filter_questions(\n",
    "    finished=True,\n",
    "    custom_filter=lambda q: q.get(\"custom_metadata\", {}).get(\"difficulty\") == \"hard\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFinished + Hard: {len(complex)} questions\")\n",
    "for q in complex:\n",
    "    print(f\"  - {q['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering by Custom Metadata\n",
    "\n",
    "### Method 1: `filter_by_custom_metadata()` - Simple AND logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by multiple criteria (AND logic)\n",
    "prog_hard = benchmark.filter_by_custom_metadata(\n",
    "    category=\"programming\",\n",
    "    difficulty=\"hard\"\n",
    ")\n",
    "\n",
    "print(f\"Programming AND Hard: {len(prog_hard)} questions\")\n",
    "for q in prog_hard:\n",
    "    print(f\"  - {q['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: `filter_by_metadata()` - Generic with dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match\n",
    "programming = benchmark.filter_by_metadata(\"custom_metadata.category\", \"programming\")\n",
    "print(f\"Programming questions: {len(programming)}\")\n",
    "\n",
    "# Contains match\n",
    "bio_related = benchmark.filter_by_metadata(\n",
    "    \"custom_metadata.category\",\n",
    "    \"bio\",\n",
    "    match_mode=\"contains\"\n",
    ")\n",
    "print(f\"Bio-related questions: {len(bio_related)}\")\n",
    "\n",
    "# List membership (in)\n",
    "python_tagged = benchmark.filter_by_metadata(\n",
    "    \"custom_metadata.tags\",\n",
    "    \"python\",\n",
    "    match_mode=\"in\"\n",
    ")\n",
    "print(f\"Python-tagged questions: {len(python_tagged)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Search Methods\n",
    "\n",
    "The `search_questions()` method supports multi-term, regex, and multi-field searches."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Key Takeaways\n\n✅ **Harmonized API** - All access methods return question objects by default  \n✅ **Generic APIs** - Work with any custom metadata structure  \n✅ **Flexible Filtering** - Lambda functions for complex logic  \n✅ **Powerful Search** - Multi-term, regex, multi-field support  \n✅ **Statistics** - Count by any field with dot notation  \n✅ **Composable** - Combine methods for complex workflows  \n\n## Method Reference\n\n### Access (Returns question objects by default)\n- `get_all_questions(ids_only=False)` - All questions (objects by default, IDs if `ids_only=True`)\n- `get_question(id)` - Single question by ID\n- `get_question_ids()` - All question IDs (convenience wrapper)\n- `get_finished_questions(ids_only=False)` - Finished questions\n- `get_unfinished_questions(ids_only=False)` - Unfinished questions\n- `get_missing_templates(ids_only=False)` - Questions without templates\n\n### Filtering\n- `filter_questions(finished, has_template, has_rubric, author, custom_filter)` - System metadata + lambda\n- `filter_by_custom_metadata(**criteria)` - Simple AND logic\n- `filter_by_metadata(field_path, value, match_mode)` - Generic with dot notation\n\n### Search\n- `search_questions(query, match_all, fields, case_sensitive, regex)` - Unified search API\n\n### Statistics\n- `count_by_field(field_path, questions)` - Count distribution by any field\n\nSee the [documentation](../docs/using-karenina/accessing-filtering.md) for more details!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-term AND search\n",
    "ml_questions = benchmark.search_questions(\n",
    "    [\"machine\", \"learning\"],\n",
    "    match_all=True\n",
    ")\n",
    "print(f\"'machine' AND 'learning': {len(ml_questions)} questions\")\n",
    "\n",
    "# Multi-term OR search\n",
    "science = benchmark.search_questions(\n",
    "    [\"DNA\", \"quantum\", \"physics\"],\n",
    "    match_all=False\n",
    ")\n",
    "print(f\"DNA OR quantum OR physics: {len(science)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex search\n",
    "what_questions = benchmark.search_questions(r\"^What\", regex=True)\n",
    "print(f\"Questions starting with 'What': {len(what_questions)}\")\n",
    "for q in what_questions:\n",
    "    print(f\"  - {q['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistics with `count_by_field()`\n",
    "\n",
    "Get distribution statistics for any field using dot notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by category\n",
    "category_dist = benchmark.count_by_field(\"custom_metadata.category\")\n",
    "print(\"Category Distribution:\")\n",
    "for category, count in sorted(category_dist.items()):\n",
    "    print(f\"  {category}: {count} questions\")\n",
    "\n",
    "# Count by difficulty\n",
    "difficulty_dist = benchmark.count_by_field(\"custom_metadata.difficulty\")\n",
    "print(\"\\nDifficulty Distribution:\")\n",
    "for difficulty, count in sorted(difficulty_dist.items()):\n",
    "    print(f\"  {difficulty}: {count} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count on a filtered subset\n",
    "programming_qs = benchmark.filter_by_custom_metadata(category=\"programming\")\n",
    "prog_difficulty = benchmark.count_by_field(\n",
    "    \"custom_metadata.difficulty\",\n",
    "    questions=programming_qs\n",
    ")\n",
    "\n",
    "print(\"Programming Questions by Difficulty:\")\n",
    "for difficulty, count in sorted(prog_difficulty.items()):\n",
    "    print(f\"  {difficulty}: {count} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Realistic Workflow: Progressive Filtering\n",
    "\n",
    "Combine multiple methods for complex analysis pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get all programming questions\n",
    "prog_qs = benchmark.filter_by_custom_metadata(category=\"programming\")\n",
    "print(f\"Step 1: Found {len(prog_qs)} programming questions\")\n",
    "\n",
    "# Step 2: Analyze difficulty distribution\n",
    "difficulty_counts = benchmark.count_by_field(\n",
    "    \"custom_metadata.difficulty\",\n",
    "    questions=prog_qs\n",
    ")\n",
    "print(f\"\\nStep 2: Difficulty distribution\")\n",
    "for diff, count in difficulty_counts.items():\n",
    "    print(f\"  {diff}: {count}\")\n",
    "\n",
    "# Step 3: Filter to hard programming questions\n",
    "hard_prog = benchmark.filter_by_custom_metadata(\n",
    "    category=\"programming\",\n",
    "    difficulty=\"hard\"\n",
    ")\n",
    "print(f\"\\nStep 3: {len(hard_prog)} hard programming questions\")\n",
    "\n",
    "# Step 4: Search for Python-specific hard questions\n",
    "python_hard = [\n",
    "    q for q in hard_prog\n",
    "    if \"python\" in q[\"question\"].lower()\n",
    "]\n",
    "print(f\"\\nStep 4: {len(python_hard)} hard Python questions\")\n",
    "for q in python_hard:\n",
    "    print(f\"  - {q['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Benchmark Summary\n",
    "\n",
    "Generate a comprehensive overview using all the methods we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal Questions: {len(benchmark)}\")\n",
    "\n",
    "# Status breakdown\n",
    "finished = benchmark.get_finished_questions()\n",
    "unfinished = benchmark.get_unfinished_questions()\n",
    "print(f\"Finished: {len(finished)}\")\n",
    "print(f\"Unfinished: {len(unfinished)}\")\n",
    "\n",
    "# Category distribution\n",
    "print(\"\\nBy Category:\")\n",
    "for cat, count in sorted(benchmark.count_by_field(\"custom_metadata.category\").items()):\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# Difficulty distribution\n",
    "print(\"\\nBy Difficulty:\")\n",
    "for diff, count in sorted(benchmark.count_by_field(\"custom_metadata.difficulty\").items()):\n",
    "    print(f\"  {diff}: {count}\")\n",
    "\n",
    "# Year distribution\n",
    "print(\"\\nBy Year:\")\n",
    "for year, count in sorted(benchmark.count_by_field(\"custom_metadata.year\").items()):\n",
    "    print(f\"  {year}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✅ **Harmonized API** - All access methods return question objects by default  \n",
    "✅ **Generic APIs** - Work with any custom metadata structure  \n",
    "✅ **Flexible Filtering** - Lambda functions for complex logic  \n",
    "✅ **Powerful Search** - Multi-term, regex, multi-field support  \n",
    "✅ **Statistics** - Count by any field with dot notation  \n",
    "✅ **Composable** - Combine methods for complex workflows  \n",
    "\n",
    "## Method Reference\n",
    "\n",
    "### Access (Returns question objects by default)\n",
    "- `get_all_questions()` - All questions\n",
    "- `get_question(id)` - Single question by ID\n",
    "- `get_finished_questions(ids_only=False)` - Finished questions\n",
    "- `get_unfinished_questions(ids_only=False)` - Unfinished questions\n",
    "- `get_missing_templates(ids_only=False)` - Questions without templates\n",
    "\n",
    "### Filtering\n",
    "- `filter_questions(finished, has_template, has_rubric, author, custom_filter)` - System metadata + lambda\n",
    "- `filter_by_custom_metadata(**criteria)` - Simple AND logic\n",
    "- `filter_by_metadata(field_path, value, match_mode)` - Generic with dot notation\n",
    "\n",
    "### Search\n",
    "- `search_questions(query, match_all, fields, case_sensitive, regex)` - Unified search API\n",
    "\n",
    "### Statistics\n",
    "- `count_by_field(field_path, questions)` - Count distribution by any field\n",
    "\n",
    "See the [documentation](../docs/using-karenina/accessing-filtering.md) for more details!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
