[
  {
    "id": "task-001",
    "category": "Infrastructure",
    "description": "Delete existing test files and create clean test directory structure",
    "depends_on": [],
    "steps": [
      "Implement: Delete all files in tests/ directory except __init__.py",
      "Implement: Create tests/unit/ with subdirectories: benchmark/, schemas/, domain/, infrastructure/, integrations/, storage/, utils/, cli/",
      "Implement: Create tests/integration/ with subdirectories: verification/, templates/, rubrics/, storage/, cli/",
      "Implement: Create tests/e2e/",
      "Implement: Create tests/fixtures/ with subdirectories: llm_responses/claude-haiku-4-5/, checkpoints/, templates/",
      "Implement: Create __init__.py in each new directory for pytest discovery",
      "Verify: Run 'find tests -name \"test_*.py\"' returns empty (all old tests deleted)",
      "Verify: Run 'ls -R tests/' confirms new directory structure exists"
    ],
    "status": "completed",
    "additional notes": "This is a destructive operation that removes all existing tests. The old tests were inconsistently organized and will be replaced with properly structured tests written from scratch based on source code analysis."
  },
  {
    "id": "task-002",
    "category": "Infrastructure",
    "description": "Configure pytest markers and test settings in pyproject.toml",
    "depends_on": [],
    "steps": [
      "Implement: Add [tool.pytest.ini_options] section to pyproject.toml",
      "Implement: Add markers: unit (pure logic, no I/O), integration (multiple components), e2e (full workflows), slow (>1s), pipeline, rubric, storage, cli",
      "Implement: Add testpaths = ['tests']",
      "Implement: Add filterwarnings to suppress known warnings",
      "Verify: Run 'uv run pytest --markers' shows all custom markers with descriptions",
      "Verify: Run 'uv run pytest --collect-only tests/' completes without errors"
    ],
    "status": "completed",
    "additional notes": "Markers enable running subsets of tests (e.g., 'pytest -m unit' for fast CI, 'pytest -m \"not slow\"' for local dev)."
  },
  {
    "id": "task-003",
    "category": "Infrastructure",
    "description": "Implement FixtureBackedLLMClient for deterministic LLM test replay",
    "depends_on": ["task-001"],
    "steps": [
      "Implement: Analyze src/karenina/infrastructure/llm/ to understand LLM client interface",
      "Implement: Create tests/conftest.py with FixtureBackedLLMClient class",
      "Implement: Add invoke(messages, **kwargs) method that matches real LLM client interface",
      "Implement: Add _hash_messages(messages) using SHA256 on JSON-serialized content",
      "Implement: Add _load_fixture(prompt_hash) that searches fixtures/llm_responses/ recursively",
      "Implement: Raise ValueError with regeneration command when fixture not found",
      "Implement: Add MockResponse class with .content, .id, .model, .usage attributes",
      "Verify: Write test that FixtureBackedLLMClient raises ValueError for unknown prompt",
      "Verify: Create dummy fixture, verify invoke() returns correct MockResponse"
    ],
    "status": "completed",
    "additional notes": "The client must match how template_evaluator.py and rubric_evaluator.py call the LLM. Check actual method signatures in the infrastructure module."
  },
  {
    "id": "task-004",
    "category": "Infrastructure",
    "description": "Create shared pytest fixtures in root conftest.py",
    "depends_on": ["task-003"],
    "steps": [
      "Implement: Add fixtures_dir() fixture returning Path to tests/fixtures/",
      "Implement: Add llm_fixtures_dir(fixtures_dir) returning path to LLM response fixtures",
      "Implement: Add llm_client(llm_fixtures_dir) returning FixtureBackedLLMClient instance",
      "Implement: Add sample_trace() with realistic LLM response text",
      "Implement: Add tmp_benchmark(tmp_path) creating minimal Benchmark with 1 question",
      "Verify: Create test file using each fixture, confirm all work",
      "Verify: Run 'uv run pytest tests/test_fixtures_work.py -v' passes"
    ],
    "status": "completed",
    "additional notes": "These fixtures are imported by all test layers. Keep them minimal. Analyze src/karenina/benchmark/__init__.py to understand Benchmark creation API."
  },
  {
    "id": "task-005",
    "category": "Infrastructure",
    "description": "Create fixture capture script CLI structure",
    "depends_on": [],
    "steps": [
      "Implement: Create scripts/capture_fixtures.py with argparse CLI",
      "Implement: Add --scenario option (template_parsing, rubric_evaluation, abstention, generation)",
      "Implement: Add --all flag to capture all scenarios",
      "Implement: Add --list flag to show available scenarios with descriptions",
      "Implement: Add --force flag for regeneration",
      "Implement: Add --dry-run flag to show what would be captured without calling API",
      "Verify: Run 'python scripts/capture_fixtures.py --help' shows all options",
      "Verify: Run 'python scripts/capture_fixtures.py --list' shows scenario descriptions"
    ],
    "status": "pending",
    "additional notes": "This task creates CLI structure only. Capture logic implementation is task-006. Analyze src/karenina/domain/verification/ to find all LLM call sites using 'grep -rn \".invoke(\" src/karenina'."
  },
  {
    "id": "task-006",
    "category": "Infrastructure",
    "description": "Implement fixture capture logic in capture script",
    "depends_on": ["task-001", "task-005"],
    "steps": [
      "Implement: Add capture logic that runs real pipeline stages and intercepts LLM calls",
      "Implement: Save fixtures with metadata (model, captured_at, prompt_hash, source_file, source_line)",
      "Implement: Use claude-haiku-4-5 with temperature=0 for determinism",
      "Implement: Save to tests/fixtures/llm_responses/claude-haiku-4-5/<scenario>/<name>.json",
      "Verify: Run 'python scripts/capture_fixtures.py --scenario template_parsing' creates fixture files",
      "Verify: Fixture JSON has metadata, request, response sections"
    ],
    "status": "pending",
    "additional notes": "Requires ANTHROPIC_API_KEY environment variable. Test with --dry-run first to verify call site detection works."
  },
  {
    "id": "task-007",
    "category": "Infrastructure",
    "description": "Create sample checkpoint fixtures for testing",
    "depends_on": ["task-001"],
    "steps": [
      "Implement: Analyze src/karenina/storage/ to understand checkpoint JSON-LD format",
      "Implement: Create tests/fixtures/checkpoints/minimal.jsonld with 1 simple question",
      "Implement: Create tests/fixtures/checkpoints/with_results.jsonld with existing verification results",
      "Implement: Create tests/fixtures/checkpoints/multi_question.jsonld with 5 diverse questions",
      "Implement: Ensure all use valid JSON-LD structure matching Benchmark.load() expectations",
      "Verify: Run python to load each checkpoint: 'from karenina import Benchmark; Benchmark.load(path)'",
      "Verify: Each checkpoint has expected question count and structure"
    ],
    "status": "completed",
    "additional notes": "Study src/karenina/storage/checkpoint.py and existing .jsonld files in the repo to understand exact format requirements."
  },
  {
    "id": "task-008",
    "category": "Infrastructure",
    "description": "Create sample answer template fixtures for testing",
    "depends_on": ["task-001"],
    "steps": [
      "Implement: Analyze src/karenina/schemas/domain.py to understand BaseAnswer class",
      "Implement: Create tests/fixtures/templates/simple_extraction.py with single-field Answer",
      "Implement: Create tests/fixtures/templates/multi_field.py with nested/complex fields",
      "Implement: Create tests/fixtures/templates/with_correct_dict.py using model_post_init for ground truth",
      "Implement: Each template must have working verify() method",
      "Verify: Import each template and instantiate Answer class successfully",
      "Verify: Call verify() with test data and confirm correct boolean return"
    ],
    "status": "completed",
    "additional notes": "Templates MUST use class name 'Answer' exactly. Study existing templates in the codebase to understand patterns."
  },
  {
    "id": "task-009",
    "category": "Unit Tests",
    "description": "Write unit tests for Benchmark class core functionality",
    "depends_on": ["task-004"],
    "steps": [
      "Implement: Analyze src/karenina/benchmark/__init__.py and benchmark.py thoroughly",
      "Implement: Create tests/unit/benchmark/test_benchmark_core.py",
      "Implement: Write tests for Benchmark initialization, name, metadata",
      "Implement: Write tests for add_question() with various inputs (minimal, full metadata, invalid)",
      "Implement: Write tests for question retrieval (by ID, by index, iteration)",
      "Implement: Write tests for question count, empty benchmark edge cases",
      "Implement: Add @pytest.mark.unit to all tests, include docstrings",
      "Verify: Run 'uv run pytest tests/unit/benchmark/test_benchmark_core.py -v' with 0 failures"
    ],
    "status": "completed",
    "additional notes": "Focus on pure logic - no file I/O, no LLM calls. Test edge cases: empty inputs, None values, duplicate IDs, invalid types. Each test function must have a docstring explaining what it tests."
  },
  {
    "id": "task-010",
    "category": "Unit Tests",
    "description": "Write unit tests for Benchmark filtering and querying",
    "depends_on": ["task-009"],
    "steps": [
      "Implement: Analyze filtering methods in Benchmark class",
      "Implement: Create tests/unit/benchmark/test_benchmark_filtering.py",
      "Implement: Write tests for filter_by_tag() with single/multiple tags",
      "Implement: Write tests for filter_by_status() (pending, completed, failed)",
      "Implement: Write tests for filter combinations and chaining",
      "Implement: Write tests for empty results, no matches, all matches",
      "Verify: Run 'uv run pytest tests/unit/benchmark/test_benchmark_filtering.py -v'",
      "Verify: All filter methods have test coverage"
    ],
    "status": "completed",
    "additional notes": "Created 31 tests covering filter_questions(), filter_by_metadata(), filter_by_custom_metadata(), search_questions(), get_questions_by_author(), get_questions_with_rubric(), get_finished/unfinished_questions(), count_by_field(). All tests passing."
  },
  {
    "id": "task-011",
    "category": "Unit Tests",
    "description": "Write unit tests for Benchmark result aggregation and DataFrame export",
    "depends_on": ["task-009"],
    "steps": [
      "Implement: Analyze result aggregation and to_dataframe methods in Benchmark class",
      "Implement: Create tests/unit/benchmark/test_benchmark_aggregation.py",
      "Implement: Write tests for pass_rate calculation (0%, 50%, 100%)",
      "Implement: Write tests for score averaging and weighted metrics",
      "Implement: Write tests for edge cases (no results, partial results)",
      "Implement: Write tests for DataFrame export with expected columns",
      "Verify: Run 'uv run pytest tests/unit/benchmark/test_benchmark_aggregation.py -v'",
      "Verify: Aggregation calculations are mathematically correct"
    ],
    "status": "completed",
    "additional notes": "Created 28 tests covering to_dict(), to_csv(), to_markdown(), get_summary(), get_statistics(), check_readiness(), get_health_report(), and property accessors. All tests passing."
  },
  {
    "id": "task-012",
    "category": "Unit Tests",
    "description": "Write unit tests for Pydantic schemas (BaseAnswer, answer templates)",
    "depends_on": ["task-004"],
    "steps": [
      "Implement: Analyze src/karenina/schemas/domain.py and related files",
      "Implement: Create tests/unit/schemas/test_answer_schemas.py",
      "Implement: Write tests for BaseAnswer validation, required fields, type coercion",
      "Implement: Write tests for answer template field constraints and defaults",
      "Implement: Write tests for serialization/deserialization roundtrip",
      "Implement: Write tests for invalid inputs triggering ValidationError",
      "Verify: Run 'uv run pytest tests/unit/schemas/test_answer_schemas.py -v' with 0 failures"
    ],
    "status": "completed",
    "additional notes": "Created 38 tests covering Question, LLMRubricTrait, RegexTrait, CallableTrait, BaseAnswer, TraitKind. All tests passing. Pydantic v2 validation working correctly."
  },
  {
    "id": "task-013",
    "category": "Unit Tests",
    "description": "Write unit tests for rubric trait schemas",
    "depends_on": ["task-012"],
    "steps": [
      "Implement: Analyze rubric trait schemas in src/karenina/schemas/",
      "Implement: Create tests/unit/schemas/test_rubric_schemas.py",
      "Implement: Write tests for LLMRubricTrait schema validation",
      "Implement: Write tests for RegexTrait schema validation",
      "Implement: Write tests for CallableTrait schema validation",
      "Implement: Write tests for MetricRubricTrait schema validation",
      "Verify: Run 'uv run pytest tests/unit/schemas/test_rubric_schemas.py -v'",
      "Verify: All 4 trait types have schema validation tests"
    ],
    "status": "completed",
    "additional notes": "Created 34 tests covering MetricRubricTrait (tp_only/full_matrix modes, validation), Rubric class (trait management, validation), RubricEvaluation, and merge_rubrics function. All tests passing."
  },
  {
    "id": "task-014",
    "category": "Unit Tests",
    "description": "Write unit tests for checkpoint schemas and JSON-LD validation",
    "depends_on": ["task-012"],
    "steps": [
      "Implement: Analyze src/karenina/schemas/checkpoint.py",
      "Implement: Create tests/unit/schemas/test_checkpoint_schemas.py",
      "Implement: Write tests for checkpoint structure validation",
      "Implement: Write tests for required fields, optional fields, defaults",
      "Implement: Write tests for JSON-LD @context and @type handling",
      "Implement: Write tests for invalid checkpoint structures",
      "Verify: Run 'uv run pytest tests/unit/schemas/test_checkpoint_schemas.py -v'",
      "Verify: Both valid and invalid schemas are tested"
    ],
    "status": "pending",
    "additional notes": "JSON-LD has specific structure requirements. Ensure @context and @type are validated correctly."
  },
  {
    "id": "task-015",
    "category": "Unit Tests",
    "description": "Write unit tests for domain verification logic (non-LLM parts)",
    "depends_on": ["task-004"],
    "steps": [
      "Implement: Analyze src/karenina/domain/verification/ for pure logic components",
      "Implement: Create tests/unit/domain/test_verification_logic.py",
      "Implement: Write tests for VerificationConfig validation and defaults",
      "Implement: Write tests for VerificationResult construction and pass/fail determination",
      "Implement: Write tests for verification status transitions",
      "Implement: Write tests for error state handling",
      "Verify: Run 'uv run pytest tests/unit/domain/ -v' with 0 failures",
      "Verify: Pure logic is isolated from LLM-dependent code"
    ],
    "status": "pending",
    "additional notes": "Only test logic that doesn't require LLM calls. LLM-dependent verification goes in integration tests."
  },
  {
    "id": "task-016",
    "category": "Unit Tests",
    "description": "Write unit tests for rubric trait evaluation rules (RegexTrait, CallableTrait)",
    "depends_on": ["task-013"],
    "steps": [
      "Implement: Analyze trait evaluation code in src/karenina/domain/",
      "Implement: Create tests/unit/domain/test_trait_rules.py",
      "Implement: Write tests for RegexTrait pattern matching (match found, not found, multiple matches)",
      "Implement: Write tests for CallableTrait invocation and return handling",
      "Implement: Write tests for score normalization and clamping to range",
      "Implement: Write tests for trait result aggregation",
      "Verify: Run 'uv run pytest tests/unit/domain/test_trait_rules.py -v'",
      "Verify: Edge cases covered (empty input, boundary scores, exceptions)"
    ],
    "status": "pending",
    "additional notes": "RegexTrait and CallableTrait are pure logic - no LLM needed. LLMRubricTrait and MetricRubricTrait evaluation needs integration tests."
  },
  {
    "id": "task-017",
    "category": "Unit Tests",
    "description": "Write unit tests for infrastructure module (LLM client utilities)",
    "depends_on": ["task-004"],
    "steps": [
      "Implement: Analyze src/karenina/infrastructure/llm/ module",
      "Implement: Create tests/unit/infrastructure/test_llm_client.py",
      "Implement: Write tests for client initialization and configuration",
      "Implement: Write tests for message formatting utilities",
      "Implement: Write tests for retry logic parameters",
      "Implement: Write tests for error handling utilities",
      "Verify: Run 'uv run pytest tests/unit/infrastructure/ -v' with 0 failures",
      "Verify: Client configuration logic is tested without making actual API calls"
    ],
    "status": "pending",
    "additional notes": "Test configuration and utility functions, not actual API calls. Mock external dependencies where needed."
  },
  {
    "id": "task-018",
    "category": "Unit Tests",
    "description": "Write unit tests for integrations module",
    "depends_on": ["task-004"],
    "steps": [
      "Implement: Analyze src/karenina/integrations/gepa/ module",
      "Implement: Create tests/unit/integrations/test_gepa.py",
      "Implement: Write tests for integration configuration and initialization",
      "Implement: Write tests for data format conversion utilities",
      "Implement: Write tests for error handling in integration code",
      "Verify: Run 'uv run pytest tests/unit/integrations/ -v' with 0 failures",
      "Verify: Integration utilities tested without external dependencies"
    ],
    "status": "pending",
    "additional notes": "The gepa integration may have specific data formats. Test converters and utilities without calling external services."
  },
  {
    "id": "task-019",
    "category": "Unit Tests",
    "description": "Write unit tests for storage serialization (JSON-LD)",
    "depends_on": ["task-004", "task-007"],
    "steps": [
      "Implement: Analyze src/karenina/storage/ serialization code",
      "Implement: Create tests/unit/storage/test_jsonld_serialization.py",
      "Implement: Write tests for Benchmark to JSON-LD conversion",
      "Implement: Write tests for JSON-LD to Benchmark parsing",
      "Implement: Write tests for roundtrip consistency (serialize → deserialize → identical)",
      "Implement: Write tests for malformed JSON-LD handling",
      "Verify: Run 'uv run pytest tests/unit/storage/ -v' with 0 failures",
      "Verify: Serialization preserves all data fields"
    ],
    "status": "pending",
    "additional notes": "Focus on in-memory serialization. File I/O goes in integration tests."
  },
  {
    "id": "task-020",
    "category": "Unit Tests",
    "description": "Write unit tests for utility functions",
    "depends_on": ["task-004"],
    "steps": [
      "Implement: Analyze src/karenina/utils/ directory - list all utility modules",
      "Implement: Create tests/unit/utils/test_fuzzy_match.py for fuzzy string matching",
      "Implement: Write tests for exact match (1.0), partial match, no match (0.0)",
      "Implement: Create tests/unit/utils/test_text_processing.py for normalization if present",
      "Implement: Test edge cases: empty strings, unicode, special characters, long strings",
      "Verify: Run 'uv run pytest tests/unit/utils/ -v' with 0 failures",
      "Verify: Each utility function has comprehensive tests"
    ],
    "status": "pending",
    "additional notes": "Utilities are used throughout the codebase. Test thoroughly. Discover all utility modules by analyzing the directory."
  },
  {
    "id": "task-021",
    "category": "Unit Tests",
    "description": "Write unit tests for CLI utilities and argument parsing",
    "depends_on": ["task-004"],
    "steps": [
      "Implement: Analyze src/karenina/cli/ directory structure and utilities",
      "Implement: Create tests/unit/cli/test_cli_utils.py",
      "Implement: Write tests for config file loading and validation",
      "Implement: Write tests for preset parsing and merging",
      "Implement: Write tests for output formatting utilities",
      "Implement: Write tests for error message generation",
      "Verify: Run 'uv run pytest tests/unit/cli/ -v' with 0 failures",
      "Verify: CLI utilities work independently of Click commands"
    ],
    "status": "pending",
    "additional notes": "Test utility functions, not Click commands directly. Command integration goes in e2e tests."
  },
  {
    "id": "task-022",
    "category": "LLM Fixtures",
    "description": "Capture LLM fixtures for template parsing scenarios",
    "depends_on": ["task-006"],
    "steps": [
      "Implement: Run capture script for template_parsing scenario",
      "Implement: Capture basic_extraction.json (single field, clean extraction)",
      "Implement: Capture complex_schema.json (nested fields, multiple types)",
      "Implement: Capture malformed_json.json (LLM returns invalid JSON)",
      "Implement: Capture partial_extraction.json (some fields null)",
      "Implement: Capture refusal.json (LLM refuses to answer)",
      "Verify: All fixtures exist in tests/fixtures/llm_responses/claude-haiku-4-5/template_parsing/",
      "Verify: Each fixture has valid JSON with metadata, request, response sections"
    ],
    "status": "pending",
    "additional notes": "Requires ANTHROPIC_API_KEY. Use temperature=0 for determinism. May need to craft specific prompts to trigger edge cases."
  },
  {
    "id": "task-023",
    "category": "LLM Fixtures",
    "description": "Capture LLM fixtures for rubric evaluation scenarios",
    "depends_on": ["task-006"],
    "steps": [
      "Implement: Run capture script for rubric_evaluation scenario",
      "Implement: Capture excerpt/found.json (relevant excerpts extracted)",
      "Implement: Capture reasoning/clear.json (clear reasoning with conclusion)",
      "Implement: Capture score/valid.json (score within expected range)",
      "Implement: Capture score/bool_true.json and score/bool_false.json",
      "Implement: Capture score/out_of_range.json (score outside defined range)",
      "Verify: All fixtures exist in tests/fixtures/llm_responses/claude-haiku-4-5/rubric_evaluation/",
      "Verify: Fixtures cover happy paths and edge cases"
    ],
    "status": "pending",
    "additional notes": "Rubric evaluation has multiple sub-stages. Analyze rubric_evaluator.py to identify all LLM call points."
  },
  {
    "id": "task-024",
    "category": "LLM Fixtures",
    "description": "Capture LLM fixtures for abstention detection and error cases",
    "depends_on": ["task-006"],
    "steps": [
      "Implement: Capture abstention/detected.json (model says 'I don't know')",
      "Implement: Capture abstention/not_detected.json (definitive answer)",
      "Implement: Capture abstention/hedging.json (hedges but answers)",
      "Implement: Capture generation/success.json (valid answer generated)",
      "Implement: Capture generation/refusal.json (model refuses)",
      "Implement: Create synthesized error fixtures (rate_limit.json, timeout.json, auth_error.json)",
      "Verify: All fixtures exist in appropriate directories",
      "Verify: Error fixtures have realistic Anthropic API error structures"
    ],
    "status": "pending",
    "additional notes": "Abstention requires specific prompts. Error cases are synthesized based on Anthropic API error formats."
  },
  {
    "id": "task-025",
    "category": "Integration Tests",
    "description": "Create integration test conftest.py with pipeline fixtures",
    "depends_on": ["task-004", "task-008", "task-022"],
    "steps": [
      "Implement: Create tests/integration/conftest.py",
      "Implement: Add pipeline fixture creating VerificationPipeline with FixtureBackedLLMClient",
      "Implement: Add evaluator fixture for RubricEvaluator with FixtureBackedLLMClient",
      "Implement: Add trace_with_citations fixture with citation patterns [1], [2]",
      "Implement: Add trace_without_citations fixture",
      "Implement: Add template fixtures loading from tests/fixtures/templates/",
      "Verify: Import all fixtures in test file without errors",
      "Verify: pipeline fixture can instantiate successfully"
    ],
    "status": "pending",
    "additional notes": "Analyze actual class constructors in src/karenina/domain/verification/ to ensure correct fixture setup."
  },
  {
    "id": "task-026",
    "category": "Integration Tests",
    "description": "Write integration tests for verification pipeline orchestration",
    "depends_on": ["task-025"],
    "steps": [
      "Implement: Analyze src/karenina/domain/verification/pipeline.py or equivalent",
      "Implement: Create tests/integration/verification/test_pipeline_orchestration.py",
      "Implement: Write test for successful full pipeline run",
      "Implement: Write test for stage failure propagation (early stage fails → pipeline stops)",
      "Implement: Write test for context passing between stages",
      "Implement: Write test for partial completion and result collection",
      "Implement: Add @pytest.mark.integration and @pytest.mark.pipeline markers",
      "Verify: Run 'uv run pytest tests/integration/verification/test_pipeline_orchestration.py -v'",
      "Verify: All tests use fixture-backed LLM client"
    ],
    "status": "pending",
    "additional notes": "The pipeline has 12 stages per docs. Cover stage transitions systematically."
  },
  {
    "id": "task-027",
    "category": "Integration Tests",
    "description": "Write integration tests for template parsing and verification",
    "depends_on": ["task-025", "task-022"],
    "steps": [
      "Implement: Analyze template_evaluator.py for parsing flow",
      "Implement: Create tests/integration/templates/test_template_parsing.py",
      "Implement: Write test_successful_extraction (LLM parses correctly, verify() returns True)",
      "Implement: Write test_malformed_json_response (graceful error handling)",
      "Implement: Write test_verify_throws_exception (exception captured, not propagated)",
      "Implement: Write test_null_fields_retry (retry logic for null fields)",
      "Implement: Add docstrings referencing specific fixtures used",
      "Verify: Run 'uv run pytest tests/integration/templates/ -v' passes",
      "Verify: Tests use actual fixture files"
    ],
    "status": "pending",
    "additional notes": "Template parsing is LLM-dependent. Each test should reference which fixture it uses in its docstring."
  },
  {
    "id": "task-028",
    "category": "Integration Tests",
    "description": "Write integration tests for LLMRubricTrait evaluation",
    "depends_on": ["task-025", "task-023"],
    "steps": [
      "Implement: Analyze LLMRubricTrait class and evaluation flow",
      "Implement: Create tests/integration/rubrics/test_llm_rubric_trait.py",
      "Implement: Write test_boolean_trait_pass using bool_true fixture",
      "Implement: Write test_boolean_trait_fail using bool_false fixture",
      "Implement: Write test_scored_trait_valid (score within range)",
      "Implement: Write test_scored_trait_out_of_range (clamping behavior)",
      "Implement: Add @pytest.mark.rubric marker",
      "Verify: Run 'uv run pytest tests/integration/rubrics/test_llm_rubric_trait.py -v'",
      "Verify: All trait behaviors are covered"
    ],
    "status": "pending",
    "additional notes": "LLMRubricTrait requires LLM for evaluation. Test the full flow from trait definition to score extraction."
  },
  {
    "id": "task-029",
    "category": "Integration Tests",
    "description": "Write integration tests for RegexTrait evaluation",
    "depends_on": ["task-025"],
    "steps": [
      "Implement: Create tests/integration/rubrics/test_regex_trait.py",
      "Implement: Write test_pattern_found using trace_with_citations fixture",
      "Implement: Write test_pattern_not_found using trace_without_citations fixture",
      "Implement: Write test_multiple_matches for traces with many matches",
      "Implement: Write test_empty_trace edge case",
      "Implement: Write test_case_sensitivity options",
      "Verify: Run 'uv run pytest tests/integration/rubrics/test_regex_trait.py -v'",
      "Verify: No LLM fixtures needed - regex is deterministic"
    ],
    "status": "pending",
    "additional notes": "RegexTrait doesn't need LLM but tests trait + evaluator integration. Cover edge cases thoroughly."
  },
  {
    "id": "task-030",
    "category": "Integration Tests",
    "description": "Write integration tests for CallableTrait evaluation",
    "depends_on": ["task-025"],
    "steps": [
      "Implement: Create tests/integration/rubrics/test_callable_trait.py",
      "Implement: Write test_callable_returns_true with various true conditions",
      "Implement: Write test_callable_returns_false with various false conditions",
      "Implement: Write test_callable_raises_exception (graceful handling)",
      "Implement: Write test_callable_with_different_signatures",
      "Implement: Write test_callable_with_context_access",
      "Verify: Run 'uv run pytest tests/integration/rubrics/test_callable_trait.py -v'",
      "Verify: Exception handling test confirms no propagation"
    ],
    "status": "pending",
    "additional notes": "CallableTrait is deterministic but tests trait + evaluator integration. Cover callable behavior edge cases."
  },
  {
    "id": "task-031",
    "category": "Integration Tests",
    "description": "Write integration tests for MetricRubricTrait evaluation",
    "depends_on": ["task-025", "task-023"],
    "steps": [
      "Implement: Analyze MetricRubricTrait for precision/recall/F1 calculation",
      "Implement: Create tests/integration/rubrics/test_metric_rubric_trait.py",
      "Implement: Write test_perfect_extraction (P=1, R=1, F1=1)",
      "Implement: Write test_partial_extraction (recall < 1)",
      "Implement: Write test_false_positives (precision < 1)",
      "Implement: Write test_no_matches (edge case, handle divide by zero)",
      "Implement: Test confusion matrix calculation",
      "Verify: Run 'uv run pytest tests/integration/rubrics/test_metric_rubric_trait.py -v'",
      "Verify: Metric calculations are mathematically verified"
    ],
    "status": "pending",
    "additional notes": "MetricRubricTrait uses LLM for extraction, then calculates metrics. Verify formulas match sklearn or standard definitions."
  },
  {
    "id": "task-032",
    "category": "Integration Tests",
    "description": "Write integration tests for checkpoint I/O and storage",
    "depends_on": ["task-025", "task-007"],
    "steps": [
      "Implement: Analyze src/karenina/storage/ for I/O operations",
      "Implement: Create tests/integration/storage/test_checkpoint_io.py",
      "Implement: Write test_save_and_load_roundtrip (data integrity)",
      "Implement: Write test_progressive_save (results saved during verification)",
      "Implement: Write test_concurrent_writes (race condition handling with threading)",
      "Implement: Write test_corrupted_file_handling",
      "Implement: Add @pytest.mark.storage marker",
      "Verify: Run 'uv run pytest tests/integration/storage/ -v' passes",
      "Verify: Use tmp_path fixture for all file operations"
    ],
    "status": "pending",
    "additional notes": "Storage integration tests file I/O. Concurrent write test should use threading to simulate race conditions."
  },
  {
    "id": "task-033",
    "category": "Integration Tests",
    "description": "Write integration tests for CLI commands",
    "depends_on": ["task-025", "task-007"],
    "steps": [
      "Implement: Analyze src/karenina/cli/ for command structure",
      "Implement: Create tests/integration/cli/test_verify_command.py",
      "Implement: Write tests for verify command with various options",
      "Implement: Create tests/integration/cli/test_preset_command.py",
      "Implement: Write tests for preset list, show, create commands",
      "Implement: Use Click's CliRunner for command invocation",
      "Implement: Add @pytest.mark.cli marker",
      "Verify: Run 'uv run pytest tests/integration/cli/ -v' passes",
      "Verify: Commands use fixture-backed LLM where needed"
    ],
    "status": "pending",
    "additional notes": "CLI integration tests the command structure, not full workflows. Full workflows go in e2e tests."
  },
  {
    "id": "task-034",
    "category": "Integration Tests",
    "description": "Write cross-component integration tests",
    "depends_on": ["task-026", "task-027", "task-028", "task-032"],
    "steps": [
      "Implement: Create tests/integration/test_cross_component.py",
      "Implement: Write test_template_passes_rubric_fails (correct answer but poor quality)",
      "Implement: Write test_pipeline_checkpoint_recovery (interrupt and resume)",
      "Implement: Write test_batch_verification_with_mixed_results",
      "Implement: Write test combining template, rubric, and storage",
      "Verify: Run 'uv run pytest tests/integration/test_cross_component.py -v'",
      "Verify: Tests span multiple integration boundaries"
    ],
    "status": "pending",
    "additional notes": "Cross-component tests verify that independently tested components work together correctly."
  },
  {
    "id": "task-035",
    "category": "E2E Tests",
    "description": "Create E2E test conftest.py with CLI runner fixtures",
    "depends_on": ["task-004", "task-007"],
    "steps": [
      "Implement: Create tests/e2e/conftest.py",
      "Implement: Add runner fixture returning Click's CliRunner",
      "Implement: Add minimal_checkpoint fixture (copy from fixtures/checkpoints/)",
      "Implement: Add large_benchmark fixture (5+ questions)",
      "Implement: Add preset_file fixture creating temporary preset JSON",
      "Implement: Add env_with_api_key fixture for monkeypatching",
      "Verify: Import fixtures in test file without errors",
      "Verify: Fixtures provide expected paths and objects"
    ],
    "status": "pending",
    "additional notes": "E2E tests call actual CLI entry points. Fixtures must set up complete environments."
  },
  {
    "id": "task-036",
    "category": "E2E Tests",
    "description": "Write E2E tests for full verification workflow",
    "depends_on": ["task-035"],
    "steps": [
      "Implement: Analyze CLI entry point in src/karenina/cli/",
      "Implement: Create tests/e2e/test_full_verification.py",
      "Implement: Write test_verify_minimal_benchmark (single question, success)",
      "Implement: Write test_verify_with_preset (preset options applied)",
      "Implement: Write test_verify_mixed_results (some pass, some fail)",
      "Implement: Write test_verify_with_output_file (results written correctly)",
      "Implement: Add @pytest.mark.e2e marker",
      "Verify: Run 'uv run pytest tests/e2e/test_full_verification.py -v'",
      "Verify: Tests invoke main() directly, not subprocess"
    ],
    "status": "pending",
    "additional notes": "E2E tests use fixture-backed LLM for speed. They test complete CLI workflows."
  },
  {
    "id": "task-037",
    "category": "E2E Tests",
    "description": "Write E2E tests for checkpoint resume functionality",
    "depends_on": ["task-035"],
    "steps": [
      "Implement: Create tests/e2e/test_checkpoint_resume.py",
      "Implement: Write test_resume_from_partial_checkpoint",
      "Implement: Write test_no_duplicate_work_on_resume",
      "Implement: Write test_resume_preserves_existing_results",
      "Implement: Simulate interruption by using --max-questions flag",
      "Verify: Run 'uv run pytest tests/e2e/test_checkpoint_resume.py -v'",
      "Verify: Question counts verify no duplicate processing"
    ],
    "status": "pending",
    "additional notes": "Resume is critical for long-running verifications. Test that state is correctly preserved."
  },
  {
    "id": "task-038",
    "category": "E2E Tests",
    "description": "Write E2E tests for error handling and edge cases",
    "depends_on": ["task-035"],
    "steps": [
      "Implement: Create tests/e2e/test_error_handling.py",
      "Implement: Write test_invalid_checkpoint_file (file not found)",
      "Implement: Write test_malformed_checkpoint (invalid JSON)",
      "Implement: Write test_invalid_preset_file",
      "Implement: Write test_missing_api_key (use monkeypatch)",
      "Implement: Verify all return non-zero exit codes with helpful messages",
      "Verify: Run 'uv run pytest tests/e2e/test_error_handling.py -v'",
      "Verify: Error messages are user-friendly, not stack traces"
    ],
    "status": "pending",
    "additional notes": "Error handling tests ensure graceful failures. Users should see helpful messages, not Python tracebacks."
  },
  {
    "id": "task-039",
    "category": "E2E Tests",
    "description": "Write E2E tests for preset and configuration commands",
    "depends_on": ["task-035"],
    "steps": [
      "Implement: Create tests/e2e/test_preset_commands.py",
      "Implement: Write test_preset_list (shows available presets)",
      "Implement: Write test_preset_show (displays preset details)",
      "Implement: Write test_preset_create (creates new preset file)",
      "Implement: Verify output format matches expected structure",
      "Verify: Run 'uv run pytest tests/e2e/test_preset_commands.py -v'",
      "Verify: Commands produce expected output"
    ],
    "status": "pending",
    "additional notes": "Preset commands are non-LLM functionality. Test CLI UX thoroughly."
  },
  {
    "id": "task-040",
    "category": "Documentation",
    "description": "Create tests/README.md with testing philosophy and quick start",
    "depends_on": ["task-001"],
    "steps": [
      "Implement: Create tests/README.md",
      "Implement: Document the 4 key testing principles from docs/testing/README.md",
      "Implement: Add quick-start commands for each test category (unit, integration, e2e)",
      "Implement: Document fixture philosophy (captured from real APIs, not mocks)",
      "Implement: Add links to docs/testing/*.md for detailed documentation",
      "Verify: README has working, copy-pasteable pytest commands",
      "Verify: Philosophy matches docs/testing/README.md"
    ],
    "status": "pending",
    "additional notes": "This is a quick reference for developers. Keep it concise - point to docs for details."
  },
  {
    "id": "task-041",
    "category": "Documentation",
    "description": "Create tests/fixtures/MANIFEST.md documenting all fixtures",
    "depends_on": ["task-022", "task-023", "task-024"],
    "steps": [
      "Implement: Create tests/fixtures/MANIFEST.md",
      "Implement: List all LLM fixture files with scenario, prompt_hash, captured_at",
      "Implement: List all checkpoint fixtures with description, question count",
      "Implement: List all template fixtures with description",
      "Implement: Mark synthesized fixtures (errors) vs captured",
      "Verify: Every file in fixtures/ is documented",
      "Verify: MANIFEST is kept in sync with actual files"
    ],
    "status": "pending",
    "additional notes": "This manifest helps developers understand what fixtures exist and when to regenerate them."
  },
  {
    "id": "task-042",
    "category": "Polish",
    "description": "Verify test coverage meets targets and fill gaps",
    "depends_on": ["task-009", "task-010", "task-011", "task-012", "task-013", "task-014", "task-015", "task-016", "task-017", "task-018", "task-019", "task-020", "task-021"],
    "steps": [
      "Implement: Run 'uv run pytest tests/unit/ --cov=src/karenina --cov-report=html'",
      "Implement: Identify modules with < 80% line coverage",
      "Implement: Add missing tests to reach coverage targets",
      "Implement: Run 'uv run pytest tests/ --cov=src/karenina --cov-report=term'",
      "Verify: Unit tests achieve ≥ 80% line coverage",
      "Verify: All major code paths are exercised"
    ],
    "status": "pending",
    "additional notes": "Coverage is a lagging indicator. Focus on meaningful tests that find bugs, not just hitting numbers. Target 80% as minimum, not 90%."
  },
  {
    "id": "task-043",
    "category": "Polish",
    "description": "Update testing documentation with final implementation",
    "depends_on": ["task-042"],
    "steps": [
      "Implement: Update docs/testing/README.md with actual final directory structure",
      "Implement: Update docs/testing/ROADMAP.md marking all phases complete",
      "Implement: Update docs/testing/FIXTURES.md with final call site inventory",
      "Implement: Add lessons learned or deviations from original plan",
      "Verify: Documentation matches actual test structure",
      "Verify: All roadmap items checked off"
    ],
    "status": "pending",
    "additional notes": "Final documentation pass ensures docs reflect reality."
  },
  {
    "id": "task-044",
    "category": "Polish",
    "description": "Run full test suite and generate final report",
    "depends_on": ["task-043"],
    "steps": [
      "Implement: Run 'uv run pytest tests/ -v --tb=short'",
      "Implement: Capture final test count by category (unit, integration, e2e)",
      "Implement: Document any skipped or xfailed tests with reasons",
      "Implement: Generate HTML coverage report",
      "Verify: 0 failures in full test run",
      "Verify: Test counts meet expectations (unit > integration > e2e)"
    ],
    "status": "pending",
    "additional notes": "This is the final verification that testing strategy is fully implemented."
  }
]
